[
["scope-of-the-thesis.html", "Chapter 1 Scope of the Thesis 1.1 Sources of Information 1.2 Methods and Focus of the Analysis", " Chapter 1 Scope of the Thesis 1.1 Sources of Information Documents written in natural language contains knowledge by design. Their goal in fact is the explication of implicit knowledge (Masters 1992, Graesser and Clark (1985)). With the growth of information of the last years also this kind of documents has grown in number and there exist a huge opportunity for business to manage this knowledge. The problem to address in order to take advantage of this opportunity, is understanding which sources of information are relevant to a certain company (D. T. Larose and Larose 2014; Chemchem and Drias 2015; Kasemsap 2015). It is considered relevant all the information that helps the company building knowledge as a tool to achieve their goals (both vision and mission). Finding these sources requires a deep understanding of digital technologies and of business acumen , skills that not every company can have (Hecklau et al. 2016; Davenport and Patil 2012; Provost and Fawcett 2013; Van der Aalst 2014). To understand the practical implications of this issue, it has been estimated that just a small portion of the digital universe has been explored with the purpose of extracting competitive gain (IDC Analyze the future 2012). The estimated percentage of untapped data is 25% and it is going to rise to 33% within 2020: it is likely that many of the potential sources of valuable knowledge are not taken in to consideration by companies and that a huge value is still hidden. This untapped value can be potentially found everywhere in the info-verse: users behaviors in social media, correlations among scientific studies, overlaps between medical and sociological studies, beneath complex analysis in legal documents and so on. For these reasons an analytically process gains great value thanks to a correct decision of which documents mine to extract knowledge. To maximize this value the problem space has to be searched in term of business relevant knowledge content of documents. Since this problem space is huge (all the documents of which a company is stakeholder) in the present thesis this decision has been taken a-priori ensuring a wide set of document types in terms of their characteristics. These documents are: Patents Scientific Papers Wikipedia Social Media Patents has their core in technical information. Despite this, some researchers (Bonino, Ciaramella, and Corno 2010) affirm that there is an increasing variety of readers of patents: not only technician and researchers but also marketers and designers who have grown an interest in patent analysis. To our knowledge this thesis represent a first step the direction of these potential readers. Papers are similar to patents in terms of technical content but are not affected by the bias of being legal documents. If this makes technical information easier to find, it also brings a problem when there is the need of synthesizing the huge information coming from a corpus of papers. Wikipedia is distant from patents and papers. It (by design) contains encyclopedic and less objective information, which comes in a semi-structured form. Due to the bottom-up approach in which Wikipedia is developed, it has some reliability shortages to take in to account. Finally social media documents are far from the previous ones since they make use of a short, subjective and colloquial lexicon. It is another of the challenges addressed by this thesis to demonstrate that this sources contains exploitable technical information too. 1.2 Methods and Focus of the Analysis Thanks to the improvement in the Artificial Intelligence (AI) field, Natural Language Processing (NLP) has dramatically increased its effectiveness in the last years (Russell and Norvig 2016). Now powerful tools for the analysis of documents are available to every one that has a little understating of programming (Arnold 2017, Benoit and Matsuo (2018), Wijffels (2018)). For this reason, the processes carried out in this thesis uses state of the art NLP techniques that only in few cases has been deeply modified in the design process. In other words the value of the output is not in the modified NLP algorithm but on the whole process of knowledge extraction that goes from the decision of which documents to analyze to the communication of the results. Furthermore, despite the golden ages of AI and NLP we are living in, some problems this research area is trying to address are still having a huge impact on companies (Hand 2007; Mining 2006). The first one is that the information overload is not manageable by machines alone and thus there is the need of understating which are the best practices in experts-machine interaction. In this context, it is interesting to notice that the way companies incorporate knowledge in their own artificial systems is drastically changed in the last 30 years. In fact, in the 1990s the main approach was knowledge engineering: machine learned how to perform analysis tasks thanks to the rules experts gave them. This idea has failed. In some contexts, it is impossible representing knowledge with explicit expert-driven rules. For this reason on the 2010s inductive revolution developed; machines are used to extract knowledge from huge amounts of data in an inductive way. This has led to a second major problem observed by literature: black boxes or systems whose working mechanism are unknown, except input and output. A recent paper (Pedreschi et al. 2018) tries to clarify the issue with black boxes and offers a few solutions. The authors point at a way similar to reversing, in which a program is stimulated with various inputs to study the outputs and understand its logic. In the same way, to study the mechanisms of a machine-learning model, it is possible to modify slightly the input only, and observe the result; or, foresee the right input for the needed output. It is still cutting edge, but it is already discussed and it is opening up to specific research, such as explainable AI. Considering these problems and considering the general goal of the present thesis (i.e. to extract and synthesize technical knowledge from unstructured documents) we can state that we can not take aside the design of processes, which can provide correct knowledge exchange between human and machine, leading to: incorporate knowledge of the experts inside AI systems experts’ ability to use in their process of decision making, inductively generated knowledge of machines References "],
["approach.html", "Chapter 2 Approach 2.1 Design Science Paradigm 2.2 Data Science Workflow", " Chapter 2 Approach 2.1 Design Science Paradigm Knowledge extraction systems are implemented within an organization for the purpose of improving the probability for that organisation to reach its goals. A wide literature exists on the design of these kind of systems and the main focus comes from Information Systems (IS) discipline. It is incumbent upon researchers in the IS to further knowledge that aids in the productive application of information technology to human organizations and their management (Editors 2012) and to develop and communicate knowledge concerning both the management of information technology and the use of information technology for managerial and organizational purposes (Zmud 1997). Hevner et al. (Bichler 2006) argues that acquiring such knowledge involves two complementary but distinct paradigms: behavioral science and design science (March and Smith 1995). The behavioral-science paradigm has its roots in the research method of natural science (Makadok, Burton, and Barney 2018; Colquitt and Zapata-Phelan 2007). Its goal is to develop and justify theories that explain (or predict) organizational phenomena (Zahra and Newey 2009) about the analysis, design, implementation, management, and use of information systems. Such theories inform researchers and companies of the interactions among humans, machines, and organizations that must be managed if an information system has to be effective. These theories interact with design decisions made with respect to the system development methodology used (Fini et al. 2018). The design-science paradigm is a problem solving paradigm and has its roots in engineering and the sciences of the artificial (Simon 1996). Its goal is to create innovations: define the ideas, practices, technical capabilities, and products through which the analysis, design, implementation, management, and use of information systems can be effectively and efficiently accomplished (Denning 1997; Tsichritzis 1997). In this context the researchers switches his goal: the artifacts that are created in the research process are not exempt from natural laws or behavioral theories but relies on existing kernel theories that are applied, tested, modified, and extended through the experience, creativity, intuition, and problem solving capabilities of the researcher (George et al. 2016; Madhavji, Miranskyy, and Kontogiannis 2015; Markus, Majchrzak, and Gasser 2002; Walls, Widmeyer, and El Sawy 1992). Since Design science creates and evaluates IT artifacts intended to solve identified organizational problems, the present thesis has as main approach this second paradigm. In Design Science, artifacts are represented in a structured form that may vary from software, formal logic, and rigorous mathematics to informal natural language descriptions. We will use all these representations at different level, for the different applications described. Furthermore the thesis has been developed following the guidelines of design-science research. Hevner et al. (Bichler 2006) provide a set of seven guidelines which help information systems researchers conduct, evaluate and present design-science research. The guidelines are summarised in table ??. As previously discussed, design science is a problem solving paradigm. The fundamental principle of design-science research is that is the building and application of an artifact that permits to the researcher to gain new knowledge and understanding of a design problem and its solution. From this fundamental principle the seven guidelines are derived. That is, design-science research must end and must have as goal the creation of an innovative, purposeful artifact (Guideline 1) for a specified problem domain (Guideline 2). Because the artifact has to be purposeful, researcher has to measure its utility for the specified problem: this imply that evaluation of the artifact is crucial (Guideline 3). Novelty is similarly crucial since the artifact must solve an unsolved problem or solving a known problem in a more effective or efficient manner (Guideline 4). In this way, design-science research is differentiated from the practice of design since artifact itself must be rigorously defined, formally represented, coherent, and internally consistent (Guideline 5). The process of design of the artificial and also artifact itself, incorporates a search process whereby a problem space is constructed and a mechanism posed or enacted to find an effective (not necessarely otpimal) solution (Guideline 6). Finally, the results of the design-science research must be communicated effectively (Guideline 7). The communication phase is crucial, and the results of a design science based research has to be comprehensible and usable to: a technical audience (researchers who will extend them and practitioners who will implement them) a managerial audience (researchers who will study them in context and practitioners who will decide if they should be implemented within their organizations). 2.2 Data Science Workflow Many frameworks and models exists that describes the process of knowledge extraction from data (Kanehisa et al. 2013, Bellinger, Castro, and Mills (2004), Ackoff (1989), Liew (2007)). All this models brings contribution to the literature but with a focus on the behavioral-science paradigm, thus with the goal to develop theories about the analysis, design, implementation, management, and use of data analysis systems. For the present thesis we adopted a more actionable framework popular in the field of data science: the Data Science Workflow (Wickham and Grolemund 2016). We adopted this model because is a generic workflow that can be used as guideline to design any knowledge extraction system. The workflow is represented in figure 2.1. The workflow is a model of the tools needed in a typical data science project. Since data science is a huge field, and there’s no way that a model can master it, the goal of the model is to give a solid foundation in the most important tools. These tools are used in every data science project, but for most projects they are not enough. The tools represented in the model has been chosen following a rough Pareto 80-20 rule (Pareto and others 1971). The workflow is divided in the subsequent tasks: Import: take data stored in a file, database, or web API, and load it into a computer. Tidy: Tidying data means storing it in a consistent form that matches the semantics of the data-set with the way it is stored. In brief, when the data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets focus on questions about the data and not in its shape. Transform: Transformation includes narrowing in on observations of interest, creating new variables that are functions of existing variables, and calculating a set of summary statistics. Visualize: Visualisation is a fundamentally human activity. A good visualisation will show unexpected information, or raise new questions about the data. A good visualisation might also hint that the hypothesis is wrong, or there is the need to collect different data. Visualisations can surprise, but do not scale particularly well because they require a human to interpret them. Model: Model is complementary to visualize. Once the hypothesis are sufficiently precise, a model can answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains. But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise. Communicate: The last step is communication, an absolutely critical part of any project. As stated before communication phase is crucial, and the results of a design science based research has to be comprehensible and usable to a technical audience a managerial audience. Program: To design knowledge extraction tool programming is nowadays essential. There is no need to be an expert programmer to design effective knowledge extraction tool, but learning more about programming allows to automate common tasks, and solve new problems with greater ease. Understand: The greatest feature of a knowledge extraction system is not to detect the large trends- they are visible anyway; it is, rather, to detect weak signals, or information that initially appears with low frequency (Apreda et al. 2016). These signals escape from traditional statistical detection techniques, exactly because it is difficult to distinguish them from pure statistical noise. It is hard to distinguish weak-signals from noise only using algorithms: understanding the problem to be solved and having domain expertise is essential. For this reason understand is not only part of the process of analysis, but is maybe the most important task with respect to the present thesis. A full state of the art on all of these tasks as been conducted in section 3. Figure 2.1: A general workflow for the process of data analysis. Readapted from Wickham (2016) References "],
["structure-and-rationale.html", "Structure and Rationale", " Structure and Rationale The selected documents to analyze, the exploitation of Natural Language Processing Techniques , the Design Science Approach and the Data Science Workflow helps to explain the structure of this thesis, since the decision of using these paradigm and tools had a direct impact on its form and content. The thesis is in fact structured as follow: Part Number 2 “State of the Art” makes a review of tools and techniques for Natural Language Processing and shows the documents used for knowledge extraction. In particular, the analysis of technical documents require the design of processes that rely both on Natural Language Processing techniques and on technical field experts. While the first techniques are codified and explicit, the second are sometimes implicit and always hard to systematize. In the State of the Art these two groups of techniques are treated in the same way to give to the reader a systematic literature review on these topics. Furthermore it is important to define the value in terms of new knowledge that documents can bring to companies. The goal of this state of the art is to demonstrate that a large literature on Techniques able to solve the problems described in section ?? exists, but weak or no effort exist in the literature in the systemic integration of these tools and in the design of holistic methods to solve these problems. Part Number 3 “Methods and Results” is the core Part of the thesis. This part describes the methods and processes designed for the analysis of technical documents following the guidelines of design research described in section 2.1. Since design research is a problem solving paradigm, each chapter (focalised on a different kind of document) starts with a brief description of the field of application of the method and with the framing of the problem to be solved. Then the methodology to solve the problem is described with the goal of being clear to a technical audience and a managerial audience. Each chapter closes with the results description that proves the utility, quality, and efficacy of the design artifact. Part Number 4 “Applications of the Results” is a collection of projects that describes the applications and results of the methods designed in Part 3 “Methods and Results”. The goal, again following the the guidelines of design research (Bichler 2006), is to provide clear and verifiable contributions in the areas of the design artifact, design foundations, and/or design methodologies. Part Number 5 “Conclusions” summarises the thesis, and its possible future developments. The thesis ends giving a more descriptive-science view of the results. In this last part we focus on a general method that has proven to be effective in the major part of the designs of the present thesis: lexicons design. References "]
]
