[
["index.html", "Knowledge Mining Engineering Management Methods &amp; Natural Language Processing Techniques Preface", " Knowledge Mining Engineering Management Methods &amp; Natural Language Processing Techniques Filippo Chiarello 2018-10-04 Preface Athore Titles Text Mining Techniques for Knowledge Extraction from Technical Documents Mining Technical Knowledge from Texts "],
["problem.html", "Chapter 1 Problem 1.1 Big Data, Many Information, Less Knowledge? 1.2 A New Challenge for Management Engineers", " Chapter 1 Problem 1.1 Big Data, Many Information, Less Knowledge? Se l’ambiente informativo in cui viviamo è radicalmente cambianto negli ultimi anni, con evidenti impatti su cultura, società, politica e salute, l’impatto che ha avuto sulle aziende è ancora più forte. Una persona infatti può raggiungere i suoi obiettivi anche senza il bisogno di gestire questa informazione, fatto non vero per le aziende: a causa anche dei recenti cambiamenti di mercato, per sopravviere ed essere competitive le aziende hanno la necessità di avere dei chiari metodi e strumenti per risolvere il problema della information overload. Per capire cosa devono affrontare le aziende oggi, dobbiamo inanzitutto svolgere un esercizio di immedesimazione nei loro confronti. Immaginiamo che le la quantità di informazione che potenzialmente ci interessa e che proviene dall’ambiente nel quale viviamo (le informazioni tattili, spaziali e sonore, cio che dicono le persone con le quali interagiamo, il contenuto dei documenti importanti per il lavoro che svogliamo ecc..) inizi velocemente ad aumentare. Per sopravvivere in un ambiente di questo genere, avremmo bisogno di sensi più potenti o forse di nuovi; potremmo inoltre aver necessità sistemi esterni che ci aiutino nel compito della processazione delle informazioni. Misurando la quantità di informazione digitale prodotta negli ultimi 10 anni, notiamo come la situazione ipotetica precedentemente descritta è simile a quella che le aziende stanno vivendo: si ritrovano in un universo digitale caotico ed in continua espansione. Questo universo digitale crescerà di un fattore di 300 dal 2005 al 2020, da 130 exabytes a 40,000 exabytes (40 zettabytes) (Gantz and Reinsel 2012), come mostrato in figura ??. Le nuove tecnologie digitali inoltre non stanno avendo un impatto solo fuori le aziende, ma anche dentro di esse. Grazie ad industria 4.0 è infatti diventato possibile creare a poco prezzo un duale digitale dell’azienda, o in altri termini è economicamente fattibile oggi estrarre informazione da qualsiasi processo aziendale. Questo che pare essere un enorme vantaggio per le aziende, crea il problema di una information overload anche interna, che necessità di essere gestita proprio come quella esterna. Un primo problema in tale contesto di pressione informativa crescente, è la difficoltà nel capire quali informazioni hanno valore per una azienda. Per valore qui si intende tutte le informazioni che possono aiutare l’azienda a costruire conoscenza che possa aiutarla a perseguire la propia mission. Questo tipo di analisi richiede infatti sia una profonda comprensione delle tecnologie digitali, che di business acumen: non tutte le aziende sono in grado di avere al proprio interno o comunque di procurarsi queste capacità, considerando inoltre che il business acumen è fortemente dipendere dal settore nel quale si lavora. Come conseguenza è stato stimato che solo una piccola frazione dell’unvierso digitale è stato esplorato con l’obiettivo di estrarre vantaggio competitivo (IDC Analyze the future 2012). La percenutale di dati ancora untapped è stimata essere del 25% ed è destinata a cresce al 33% entro il 2020. Questo valore untapped può essere trovato in pattern nascosti nei social media, correlazioni fra studi scientifici, studi medici intersecati con studi sociologici, dall’analisi massima di documenti legislativi e così via. In seconda istanza, note le potenziali fonti di informazione che possono portare valore, è necessario capire quali strumenti è possibile utilizzare per estrarre questo valore sotto forma di conoscenza. Come mostrato nell’esempio precedente, una azienda si trova in una posizione di pericolo se la crescita di informazione sorpassa la propria capacità di processarla. In altri termini, l’aumento di informazione mostrato in figura ?? non implica un aumento di conoscenza, ma al contrario è probabile che contribuisca ad impedirne la crescita. Un celebre esempio di tale fenomeno è il computer boom dei decenni 1970 e 1980, il quale implicò un declinio temporaneo nella produttività generale sia economica che scentifica. Tale fenomeno viene identificato con il nome di productivity paradox. Dal punto di vista economico, evidenza di ciò è che i computer in quel periodo ebbero impatti su molti indicatori ma non su quelli di produttività economica (Robert Solow in 1987), e che gli stati uniti furono vittima di quattro recessioni tra il 1969 ed il 1982. Il progresso scientifico è più difficile da misurare di quello economico. Non è infatti chiaro che misure possano essere utilizzate per misurare quanto uno stato sia efficace nel creare conoscenza, nonostante la letteratura sterminata su tale tema. Una proxy largamente utilizzata per effettuare tale misura, è il numero di brevetti prodotti, letto in proporzione rispetto a l’investimento in ricerca e sviluppo: se diventa meno costoso per le aziende innovare, questo suggerisce che le aziende stanno usando le informazioni che hanno a disposizione in maniera più efficiente e che sono in grado di trasformarle in conoscenza utilie all’innovazione. Per misurare tale costo, si può vedere quanto una nazione spende ogni anno per produrre in media un brevetto. Nel 1960 ad esempio gli stati uniti hanno speso $1000 (aggiustati per l’infalizione) per ogni patent application. Tale spesa inizialmente scende, poi sale anzichè scendere con l’arrivo dei computers con un picco di 735$ nel 1986 (Signal Noise). La produttività ha nuovamente un crescita negli anni ’90, quando ormai i computer erano diventati di largo utilizzo non solo per il business ma anche per applicazioni di tutti i giorni. Oggi, nell’era dei big data, le aziende (ma anche le università) hanno dunque bisogno di nuovi “sensi” ed “aiutanti” che le assistano nella estrazione e consolidazione della conoscenza. La presente tesi ha come obiettivo comprendere quali fonti di informazioni ad oggi contengono maggiore valore ancora non dischiuso e quali metodologie e strumenti posso essere utilizzati a tale scopo. 1.2 A New Challenge for Management Engineers Tipicamente ci occupaimo di attività ad altà ripetitività. Ti porti dietro metodologie ingegneristiche applicate a sistemi inernti, andnano a operare in sistemi socio-tecnici. Hai fatto il tuo mestieri (ricerca operativa ecc..). Negli ultimi anni però le aziende le attività a maggior valore aggiunto sono non ripetitive. R&amp;S, Design, marketing, HR ecc.. e quindi gestione della conoscena. Su situazione che sembrano uniche il gestionale rischia di perdere rispetto al creativo. Come disciplina voglio presidiare queste aree: non ci occupaimo di casi unici, ma costruire modelli in grado di incorporare conoscenza per essere usati in questi. La tesi ha l’obbiettivo di esploration and exploitation queste direzioni. References "],
["solutions.html", "Chapter 2 Solutions 2.1 A Process for Knowledge Creation 2.2 Text Mining 2.3 Human &amp; Machines", " Chapter 2 Solutions 2.1 A Process for Knowledge Creation Modello generico di come un sistema (uomo, macchina, azienda…) genera conoscenza. Immagine + spiegazione Un modello più actionable: data science. Immagine + spiegazione Figure 2.1: A general workflow for the process of data analysis. Readapted from Wickham (2016) 2.2 Text Mining Istanza di data science e zona a maggior valore per estrazione conoscenza. Importanza di definire documenti dai quali fare mining. Come si decide dove sta la conoscenza per l’azienda? Di quali documenti l’azienda è stakeholder? 2.3 Human &amp; Machines Datascience e text mining non fattibile da sole macchine… Il problema non è sostituire domain knowledge. Idea vecchia ha fallito. E’ insostibuibile perchè: Technology, interessa gli ingegneri Social Science, decision making PErchè fallita: da una parte è andata avanti la knowledge rappresentation. E’ impossibile rappresentare la conoscenza con regole, ma con altri strumenti si può rappresentare (bottom-up). Inoltre ho text mining, capaità di processare testi. Parte di intelligenza artificiale. Questi fenomeni non sostituioscono l’esperto ma ne cambiano il modo di operare. Si ha vantaggio su mitigazione bias se sistemi disegnati bene. Il pericolo delle black boxes. Oggi si integra. Vogliamo un esperto di dominio che faccia meglio il suo mestiere. Abbiamo oggi più potenza e correzione errori. Oltre ad efficienza e potenza nel correggere gli errori. Ora c’è anche la possibilità dio maggiore specificità. L’obiettivo è qiuindi poratre domain knowledge sia su technology sia ai decisori sociali. "],
["scope-and-stakeholders.html", "Chapter 3 Scope and Stakeholders", " Chapter 3 Scope and Stakeholders Alcuni documenti in analisi, alcuni stakeholders Research and Development, Design, Marketing, Human Resources. Policy makers. "],
["structure-and-rationale.html", "Chapter 4 Structure and rationale", " Chapter 4 Structure and rationale At a first level there are two sections 5 and 6, reviewing respectively the processes of programming and Natural Language Processing and of undestanding and knowldege of field experts application; Section 5 has a subsection for each of the phases showed in figure 2.1. These subsections goes from 5.1 to 5.7; Each subsection from 5.1 to 5.7 contains the relative Natural Language Processing task that are relevant for the analysis of technical documents, for example Document Retrieval 5.2.1, Part-Of-Speech-Tagging; 5.4.6 or Named Entity Recognition 5.5.5. Each task subsection describes the relevant techniques to perform that task. I use the word techniques to include mainly algorithms and procedures but also more generic methods or frameworks; Since the second section 6 describes less systematic phases, task and techniques this section opens with a first subsection 5.8 that focuses on the studies of the problems of using expert knowledge in an analytic process and which are the techniques to convert this knowledge in a format that is usable in a Natural Language Processing workflow. Finally, always section 6 has a subsection for each of the anlyzed technical documents. These subsections goes from 6.1 to 6.4. "],
["sotatools.html", "Chapter 5 Phases, Tasks, and Techniques 5.1 Program 5.2 Import 5.3 Tidy 5.4 Transform 5.5 Model 5.6 Visualize 5.7 Comunicate 5.8 Understand", " Chapter 5 Phases, Tasks, and Techniques In this section I make a review of the most important techniques for Natural Language Processing in the context of technical documents analysis. The techniques (mainly algorithms) are grouped in phases (Import, Tidy, Transform, Model, Visualize, Communicate) showed in figure 2.1 and each phases is dived in the NLP tasks that are the most important for the analysis of technical documents. This standard process has been disclosed in the framework of the tidyverse (Wickham and Grolemund 2016). The algorithms i reviewed in this section are summmarised in table tot, where the reader can see the relationship between tasks and techniques. 5.1 Program Programming is a key activity to perform in order to effectively and efficiently perform text mining. It is not a phases per se because each phase is implemented trough programming. It is critical that an analysts has in mind the need of maximizing the probability that their analysis is reproducible, accurate, and collaborative. This goals can be reached only trough programming. The most used programming languages for text mining and natural language processing are R (R Development Core Team 2008) and Python (Rossum 1995). R and Python are both open-source programming languages with a large community of developers, and new libraries or tools are added continuously to their respective catalog. R is mainly used for statistical analysis and data science while Python is a more general purpose programming language. R has been developed by Academics and statisticians over two decades. R has now one of the richest ecosystems to perform data analysis and there are around 12000 packages available in CRAN (open-source repository of R). The rich variety of libraries makes R the first choice for statistical analysis. Another cutting-edge difference between R and the other statistical products is R-studio. RStudio is a free and open-source integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. Finally, it is widely recognized the great performances that R has for data visualisation and communication. Python can pretty much do the same tasks as R: data wrangling, engineering, feature selection web scrapping, app and so on. Anyway, Python has great performances in the deployment and implementation of machine learning at a large-scale. Furthermore, Python codes are easier to maintain and more robust than R. 5.2 Import The first activities to perform in a text mining pipeline is to find all the documents that contains useful information for the analysis and then import the corpus (the set of documents) in to the computer program. The present section is thus focused on techniques for document retrieval 5.2.1 and on the most popular documents digital formats 5.2.2. 5.2.1 Document Retrieval Document retrieval is the process of matching a user query against a set of documents. A document retrieval system has two main tasks: 1- Find the documents that are relevant with respect to the user queries 2- Measure the relevance of the matching results Building a query means to use field specific knowledge and logical rules to write a text string that is the composition of keywords and Boolean operators. The set of keywords (single words or phrases) is chosen in such a way that these are likely to be contained in the searched documents. Boolean operators can also be used to increment the performance of the query. The AND operator, for example is used to retrieve all the document that contains both of the terms at the left and the right of it, OR for document that contains at least one of the two words. Another important tool for making a good query are regular expressions. Regular expression (regexp) is a language for specifying text search strings, an algebraic notation for characterizing a set of strings. This language widely used in modern word processor and text processing tools. A regular expression search function will search through the corpus, returning all texts that match the pattern. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. To evaluate the performance of a query is useful to understand the concepts of precision and recall. Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned. Due to the ambiguities of natural language, full-text-search systems typically includes options like stop words to increase precision. Stop-words are words that filter all the document which contains them. On the other side, stemming to increase recall 5.4.3. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision (Yuwono and Lee 1996). Usually when a user performs a query, the main problem are false positives (the results that are returned by the systems but are not relevant to the user). False positives has a negative impact on the precision of the query. The retrieval of irrelevant documents is particularly strong for technical documents due to the inherent ambiguity of technical language. For this reason to understand and to use the rules of query building are fundamental to the technical document analysis, since without a good query is rare to have a good set of documents to analyze. 5.2.2 Documents Format For the purpose of the present thesis documents are considered in a digital format, and there is no need to read it from a analogical source. From the computer science point of view, text is a human-readable sequence of characters and the words they form that can be encoded into computer-readable formats. There is no standard definition of a text file, though there are several common formats. The most common types of encoding are: ASCII, UTF-8 — plain text formats .doc for Microsoft Word — Structural binary format developed by Microsoft (specifications available since 2008 under the Open Specification Promise) HTML (.html, .htm), (open standard, ISO from 2000) Office Open XML — .docx (XML-based standard for office documents) OpenDocument — .odt (XML-based standard for office documents) PDF — Open standard for document exchange. ISO standards include PDF/X (eXchange), PDF/A (Archive), PDF/E (Engineering), ISO 32000 (PDF), PDF/UA (Accessibility) and PDF/VT (Variable data and transactional printing). PDF is readable on almost every platform with free or open source readers. Open source PDF creators are also available. Scalable Vector Graphics (SVG) - Graphics format primarily for vector-based images. TeX — Popular open-source typesetting program and format. First successful mathematical notation language. For the R software there exist many packages that helps to import documents in several formats (Wickham, Hester, and Francois 2017). 5.3 Tidy After that data are imported they have to be processed in such a way that it would be possible to perform the main task of data analysis (transformation, modelling and visualisation). This task of tidying data (usually referred to as data pre-processing) can be very time expensive, so it is important to have clear methods and techniques to perform this task. Tidy data sets have structure and working with them is easy; they’re easy to manipulate, model and visualize (Wickham and others 2014). Tidy data sets main concept is to arrange data in a way that each variable is a column and each observation (or case) is a row. The characteristics of tidy data can be thus summarised as the points (Leek 2015): Each variable you measure should be in one column Each different observation of that variable should be in a different row If you have multiple tables, they should include a column in the table that allows them to be linked There main advantages of structuring the data in this way is that a consistent data structure make it easier to use the tools (programs) that work with it because they have an underlying uniformity. This lead to an advantage in reproducibility of code. As stated before tidying data is not a trivial task, and applying this process to text is even harder for documents with respect to structured data (Silge and Robinson 2016). On the other side, is clear that using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use . Treating text as data frames of individual words allows us to manipulate, summarize, and visualize the characteristics of text easily and integrate natural language processing into effective workflows already used. Tidy text format is designed as being a table with one-token-per-row. A token unit of text that is meaningful for the analysis to be performed (for example letters, words, n-gram, sentences, or paragraphs ). Tokenization is the process of splitting text into tokens. This one-token-per-row structure is in different from the ways documents are often stored in current analyses, mainly strings or document-term matrix. The term document matrix has each corpus word represented as a row with documents as columns. The document term matrix is the transposition of the TDM so each document is a row and each word is a column. The term document matrix or document term matrix is the foundation of bag of words text mining. The bag-of-words model is a simplifying representation of documents: a text is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity (McTear, Callejas, and Griol 2016). 5.4 Transform Transforming in the context of Natural Language Processing is what in computational linguistic is called text normalization. Normalizing text means converting it to a more convenient, standard form. Most of the task of technical document analysis in fact relies on first separating out or tokenizing sentences and words, strip suffixes from the end of the word, determining the root of a word or transform the text using regular expressions. 5.4.1 Sentence Splitting The analysis of technical documents require as first process, that the input text is segmented in sentences. Since documents do not encode this information in a non ambiguous manner (using dots) due to common abbreviations (e.g.: “Mr., Dr.”), a sentence splitting process that does not rely only on a trivial dot based rule is required. This issue in the technical documents domain is even more problematic due to the presence of formulas, numbers, chemical entity names and bibliographic references. Furthermore, since sentence splitting is one of the first processes of an NLP pipeline, errors in this early stage are propagated in the following steps causing a strong decrease for what concerns their accuracy. One of the most advanced techniques are machine learning techniques: given a training corpus of properly segmented sentences and a learning algorithm, a statistical model is built. By reusing the statistical model, the sentence splitter is able to split sentences on texts not used in the training phase. ItalianNLP lab systems uses this approach (Dell’Orletta 2009, Attardi and Dell’Orletta (2009), Attardi et al. (2009)). For this reason this algorithm is used for the most of the application presented in this Thesis. 5.4.2 Tokenization Since documents are unstructured information, these has to be divided into linguistic units. The definition of linguistic units is non-trivial, and more advanced techniques can be used (such as n-gram extraction) but most of the times these are words, punctuation and numbers. English words are often separated from each other by white space, but white space is not always sufficient. Solving this problems and splitting words in well-defined tokens defined as tokenization. In most of the application described in the present Thesis, the tokenizer developed by the ItalianNLP lab was integrated (Dell’Orletta 2009; Attardi and Dell’Orletta 2009; Attardi et al. 2009). This tokenizer is regular expression based: each token must match one of the regular expression defined in a configuration file. Among the others, rules are defined to tokenize words, acronyms, numbers, dates and equations. 5.4.3 Stemming Stemming is a simpler but cruder methodology for chopping off of affixes. The goal of stemming is reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem of a word and its morphological root do not need to be identical; it is sufficient that related words map to the same stem, even if this stem is not a valid root. One of the most widely used stemming is the simple and efficient Porter algorithm (Porter 1980). 5.4.4 Lemmatisation Lemmatization is the task of determining the root of a words. The output allow to find that two words have the same root, despite their surface differences. For example, the verbs am, are, and is have the shared lemma be; the nouns cat and cats both have the lemma cat. Representing a word by its lemma is important for many natural language processing tasks. Lemmatisation in fact diminish the problem of sparsity of document-word matrix. Furthermore lemmatisaion is important for document retrieval 5.2.1 web search, since the goal is to find documents mentioning motors if the search is for motor. The most recent methods for lemmatization involve complete morphological parsing of the word (Hankamer 1989). 5.4.5 Words importance metrics Once that a document has been tokenized and the tokens has been transformed, an analyst usually wants to measure how important a word is to a document in a collection or corpus. Some of the metrics adopted are: Term Frequency: the number of times that a term occurs in document. Boolean frequency: 1 if the term occurs in the document and 0 otherwise; Term frequency adjusted for document length: is raw count normalized for the number of words contained in the document Logarithmically scaled frequency: is raw count normalized for the natural logarithm of one plus the number of words contained in the document Inverse document frequency: is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. It is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. Term frequency–Inverse document frequency: the product between term frequency and inverse document frequency. A high weight in tf–idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. 5.4.6 Part-of-Speech Tagging The part of speech plays an central role in technical document analysis since it provides very useful information concerning the morphological role of a word and its morphosyntactic context: for example, if a token is a determiner, the next token is a noun or an adjective with very high confidence. Part of speech tags are used for many information extraction tools such as named entity taggers (see section 5.5.5) in order to identify named entities. In typical named entity task these are people and locations since tokens representing named entities follow common morphological patterns (e.g. they start with a capital letter). For the application to technical documents, technical entities (like the possible failures of a manufact) becomes more relevant. In this context a correct part-of-speech tagger becomes even more important since morphosyntactical rules can not be used. In addition part of speech tags can be used to mitigate problems related to polysemy since words often have different meaning with respect to their part of speech (e.g. “track”, “guide”). This information is extremely valuable in patent analysis, and some patent tailored part-of-speech tagger has been designed (see section 6.1). The literature on pos-tagger is huge, and goes behind the scope of the present thesis to make a complete review. In most of the application presented in this work, was employed the ILC postagger (Attardi 2006). This postagger uses a supervised training algorithm: given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus. 5.5 Model The goal of a model is to provide a simple low-dimensional summary of a dataset (Wickham and Grolemund 2016). Ideally, the model will capture patterns generated by the phenomenon of interest (true signals), and ignore random variations (noise). A good model at the same time is able to capture the weak signals that cab be easily confounded with noise. These information is particularly valuable in the context of technical document analysis, where great technical insight could come weak quasi-invisible signals. (James et al. 2013) Probabilistic models are widely used in text mining nowadays, and applications range from topic modeling, language modeling, document classification and clustering to information extraction. The present section contains a review of the most used methods used to model textual information. 5.5.1 N-Grams An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like “credit card”, “3d printing”, or ”printing machine”, and a 3-gram (or trigram) is a three-word sequence of words like “3d printing machine”. Statistical model can be used to extract the n-grams contained in a document. A first approach has the of predicting the next item in a sequence in the form of a (n − 1)–order Markov model(Lafferty and Zhai 2001). The algorithm begin with the task of computing P(w|h), the probability of a word w given a word h.The way to estimate this probability is using relative frequency counts. To do that the algorithms count the number of times h is followed by the w. With a large enough corpus it is possible to build valuable models, able to extract n-grams (Bellegarda 2004). While this method of estimating probabilities directly from counts works for many natural language applications, in many cases a huge dimension of the corpus does make the model useful, and this is particularly true for technical documents (Brants et al. 2012). This is because technical language has a strong ratio of evolution; as new artifact are invented, new chunks are created all the time, and has no sense to continuously count every word co-occurrence to update our model(Gibson, Gibson, and Ingold 1994). A more useful method for chunk extraction fro technical document uses part-of-speech-tagging and regular expression. Once a document is pos-taggerd each word is associated whit a particular part of speech: each sentence is represented as a sequence of part-of-spech. Once this representation is ready, it is possible to extract only certain sequences of part-of-speeches, the ones that whit an high level of confidence are n-grams. 5.5.2 Document Classification Classification is a general process that has the goal of taking an object, extract features, and assign to the observation one of a set of discrete classes. This process is largely used for documents (Borko and Bernick 1963) and there exist many methods for document classification (Aggarwal and Zhai 2012). Regardless of technological sector, most organizations today are facing the problem of overload of information. When it comes to classify huge amount of documents or to separate the useful documents from the irrelevant, document classification techniques can reduce the process cost and time. The simplest method for classifying text is to use expert defined rules. These systems are called expert systems or knowledge engineering approach. Expert rule-based systems are programs that consist of rules in the IF form condition THEN action (if condition, then action). Given a series of facts, expert systems, thanks to the rules they are made of, manage to deduce new facts. The expert systems therefore differ from other similar programs, since, by referring to technologies developed according to artificial intelligence, they are always able to exhibit the logical steps that underlie their decisions: a purpose that, for example, is not feasible from the human mind or black box-systems. There are many type of documents for which expert based classifiers constitute a state-of-the-art system, or at least part of it. Anyway, rules can be useless in situations such as: - data change over time - the rules are too many and interrelated Most systems of documents classification are instead done via supervised learning: a data set of input observations is available and each observation is associated with some correct output (training set). The goal of the algorithm is to build a static model able to learn how to map from a new observation (test set) to a correct output. The advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. In the supervised document classification process, is used a training set of N documents that have each been typically hand-labeled with a class: (d1, c1),….,(dN, cN). I say typically, because other less expensive methods could be designed, as it will be shown for the task of Named Entity Recognition (another supervised learning task, that classifies words instead of documents 5.5.5). The goal of the supervised document classification task is to learn a statistical model capable of assign a new document d to its correct class c ∈ C. There exist a class of these classifier, probabilistic classifiers, that additionally will tell us the probability of the observation being in the class. Many kinds of machine learning algorithms are used to build classifiers (Aggarwal and Zhai 2012), such as: Decision Tree Classifiers: Decision tree documents classifier are systems that has as output a classification tree (Sebastiani 2002). In this tree internal nodes are terms contained in the corpus under analysis, branches departing are labeled by the weight (see section 5.3) that the term has in the test document, and leafs are labeled by categories. There exists many methods to automatically learn trees from data. A tree can be build by splitting the data source into subsets based on an test feature. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. Rule Based Classifiers: Rule-based classifiers are systems in which the patterns which are most likely to be related to the different classes are extracted from a set of test documents. The set of rules corresponds to the left hand side to a word pattern, and the right-hand side to a class label. These rules are used for the purposes of classification. In its most general form, the left hand side of the rule is a Boolean condition, which is expressed in Disjunctive Normal Form (DNF). However, in most cases, the condition on the left hand side is much simpler and represents a set of terms, all of which must be present in the document for the condition to be satisfied (Yang, Li, and Wang 2004). Support Vector Machines (SVM) Classifiers: SVM Classifiers attempt to partition the data space with the use of linear or non-linear delineations between the different classes. The main principle of SVM algorithm is to determine separators in the feature space which can best separate the different classes (Joachims 1998,Manevitz and Yousef (2001)). Baeysian Classifiers: Bayesian classifiers build a probabilistic classifier based on modeling the underlying word features in different classes. The idea is then to classify documents using the posterior probability of the documents belonging to the different classes on the basis of the word presence in the documents (Pop 2006). Neural Netword Classifiers: The basic unit in a neural network is a neuron. Each neuron receives a set of inputs, which are denoted by the vector Xi, which are the values of the feature vector for a certain instance. Each neuron is also associated with a set of weights, which are used in order to compute a function of its inputs. Neural Networks Classifier are able, thank to a process called learning phase, to adjust their weights in such a way that the function is able to effectively classify new instances. Neural networks are nowadays one of the best method for documents classification, and are used in a wide variety of applications (Manevitz and Yousef 2007). Great performances has also been reached by deep neural networks, which are neural networks whit a large number o neurons arranged in multiple layers (Lai et al. 2015. Kim (2014)). 5.5.3 Sentiment Analsysis Sentiment analysis techniques are algorithms able to measure from text, people’s opinions and emotions toward events, topics, products and their attributes (Pang, Lee, and others 2008). For example, businesses (particularly marketeers) are interested in finding costumers opinions about their products and services. Thanks to the growth of social media (forums, blogs and social networks), individuals and organizations are producing a huge quantity of their written opinion. This has make it possible to scholars to study this phenomena and to develop many different and effective sentiment analysis techniques (Liu and Zhang 2012). In the past decade, a considerable amount of research has been done by scholars and there are also numerous commercial companies that provide opinion mining services. However, measuring sentiment in documents and distilling the information contained in them remains a challenging task because of the diversity of documents from which is possible to extract sentiment. The approaches to perform sentiment analysis are many. Among all, the most interesting for technical documents analysis are: Dictionary Base Approaches : This approach has the aim of collecting words that are clues for positive or negative sentiment. In literature these words are called opinion words, opinion-bearing words or sentiment words. Examples of positive opinion words are: good, nice and amazing. Examples of negative opinion words are bad, poor, and terrible. Collectively, they are called the opinion lexicon. The most simple and widely used techniques to produce a dictionary of opinion words is based on bootstrapping using a small set of seed opinion words and an online dictionary such as WordNet (Miller 1995). The works that used this approach (Hu and Liu 2004, Kim and Hovy (2004)), adopts a process that consist in two phases: first collect set of opinion words manually, then grow this set by searching in the WordNet for their synonyms and antonyms. The process stops when no more new words are found. After that a manual inspection can be carried out to remove and/or correct errors. Scholars has developed several opinion lexicons (Ding, Liu, and Yu 2008, Baccianella, Esuli, and Sebastiani (2010), Hu and Liu (2004), Philip et al. (1966), Wiebe, Bruce, and O’Hara (1999)) The lexicon based approach has the characteristic of being strongly context specific. This is an advantage when the goal is to design a method able to extract sentiment in a specific context (Chiarello et al. 2017), but is a major shortcoming if the goal is to design a general purpose method. Supervised Learning Approaches: Sentiment analysis can be formulated as a document classification problem with three classes: positive, negative and neutral(Mullen and Collier 2004). Training and test sets of documents are typically collected from product reviews, movies reviews or are created by scratch using manual annotation. Any learning algorithm can be applied to sentiment classification (naive Bayesian classification, and support vector machines (Prabowo and Thelwall 2009)). The crucial phase for Supervised Learning sentiment analysis is the features presentation of the data. It was shown (Pang, Lee, and Vaithyanathan 2002) that using uni-grams (a bag of individual words) as features in classification performed well with either naive Bayesian or SVM. Subsequent research used many more features and techniques in learning (Pang, Lee, and others 2008). 5.5.4 Text Clustering The goal of clustering methods is to find groups of similar objects in the data thanks to the measure of a similarity function (Jain and Dubes 1988, Kaufman and Rousseeuw (2009)). Clustering techniques has been widely applied in the text domain, where the objects of the clustering can be documents (at different level of granularity) or terms. In the context of technical documents analysis Clustering is especially useful documents retrieval (Anick and Vaithyanathan 1997, Cutting, Karger, and Pedersen (1993)). Clustering problems has been and are studied widely outside the text domain. Methods for clustering have been developed focusing on quantitative/non-textual data (Guha, Rastogi, and Shim 1998, Han, Kamber, and Tung (2001), Zhang, Ramakrishnan, and Livny (1996)). In the context of text analysis, the problem of clustering finds applicability for a number of tasks, such as Document Organization and Browsing (Cutting et al. 2017), Corpus Stigmatization using documents maps (Schütze and Silverstein 1997) or word clusters (Baker and McCallum 1998, Bekkerman et al. (2001)). It is useful also to use a Soft clustering approach, that associates each document with multiple clusters with a given probability. However, standard techniques for cluster analysis (k-means or hierarchical clustering) do not typically work well for clustering textual data in general or more specific technical documents. This is because of the unique characteristics of textual data which implies the design of specialized algorithms for the task. The distinguishing characteristics of the text representation are the following (Aggarwal and Zhai 2012): There is a problem of course of dimensionality. The dimensionality of the bug-of-words representation is very large and the underlying data is sparse. In other words, the lexicon from which the documents are drawn may be of the order of millions, but a given document may contain only a few hundred words.This problem is even more serious for technical documents in which the lexicon is even more large. The words are correlated with one another and thus the number of concepts (or principal components) in the data is much smaller than the feature space. This necessitates the careful design of algorithms which can account for word correlations in the clustering process. The number of words (or non-zero entries) in the different documents may vary widely. Therefore, it is important to normalize the document representations appropriately during the clustering task. The problems of sparsity and high dimensionality necessitate the design of specific algorithms text processing. The topic has been heavily studied in the information retrieval literature where many techniques have been proposed (Ricardo and Berthier 2011). 5.5.5 Named Entity Recognition Named Entity Recognition is the task of identifying entity names like people, organizations, places, temporal expressions or numerical expressions. An example of an annotated sentence for a NER extraction system tailored for user entity extraction from patents, is the following: Traditionally, &lt; user &gt; guitar players &lt; user/ &gt; or &lt; user &gt; players &lt; user/ &gt; of other stringed instruments may perform in any of a number of various positions, from seated, with the stringed instru- ment supported on the leg of theperformer, to standing or walking, with the stringed instrument suspended from a strap. Methods and algorithms to deal with the entity extraction task are different, but the most effective are the ones based on supervised methods. Supervised methods tackle this task by extracting relevant statistics from an annotated corpus. These statistics are collected from the computation of features values, which are strong indicators for the identification of entities in the analyzed text. Features used in NLP for NER purposes are divided in two main categories: - Linguistically motivated features, such as n-gram of words (sequences of n words), lemma and part of speech - External resources features as, for example, external lists of entities that are candidates to be classified in the extraction process. The annotation methods of a training corpus can be of two different kinds: human based, which is time expensive, but usually effective in the classification phase; automatically based, which can lead to annotation errors due to language ambiguity. For instance driver can be classified both as a user (the operator of a motor vehicle), or not a user (a program that determines how a computer will communicate with a peripheral device). Different training algorithms, such as Hidden Markov Models (Eddy 1996a), Conditional Random Fields (CRF) (Lafferty, McCallum, and Pereira 2001a) Support Vector Machines (SVM) (Hearst, Dumais, Osuna, et al. 1998), or Bidirectional Long Short Term Mermory-CRF Neural Networks (Lample et al. 2016, Misawa et al. (2017)) are used to build a statistical model based on features that are extracted from the analyzed documents in the training phase. 5.5.6 Topic Modelling Topic modeling is a form of dimension reduction that uses probabilistic models to find the co-occurrence patterns of terms that correspond to semantic topics in a collection of documents (Crain et al. 2012). To understand topic modelling it is useful to understand its differences with clustering 5.5.4 and the problem they both solves: the course of dimensionality. Both these techniques has in fact the goal of representing documents in such a way that they reveals their internal structure and interrelations. Clustering measures the similarity (or dissimilarity) between documents to place documents into groups. Representing each document by considering the belonging to a group, clustering induces a low-dimensional representation for documents. However, it is often difficult to characterize a cluster in terms of meaningful features because the clustering is independent of the document representation, given the computed similarity. Topic modeling integrates soft clustering (assigning each element to a cluster with a given probability and not with a Boolean variable) with dimension reduction. Each document is associated with a number of latent topics: a topic can be seed as both document clusters and compact group of words identified from a corpus. Each document is assigned to the topics with different weights: this feature can be seen both as the degree of membership in the clusters, as well as the coordinates of the document in the reduced dimension space. The result is an understandable representation of documents that is useful for analyzing the themes in documents. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model (Blei, Ng, and Jordan 2003). It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language. 5.6 Visualize Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests: the process of developing the outputs (the paper, the report, the dashboard, or other deliverable) is less frequently analyzed. This problem influences also text mining (Parker 2017). One of the most studied problems of output production is data visualisation. Data visualisation involves the creation and study of the visual representation of data (Friendly and Denis 2001). Data visualization uses statistical graphics, plots, information graphics and other tools to communicate information in a clearl and efficient way. The main process of data visualisation is the visual encoding of numbers. Numerical data may be encoded in many ways, using a wide range of shapes: the main used are dots, lines, and bars (Wickham 2016). The main goal of visualizations is to help users (students, researchers, companies and many others) analyze and reason about evidences hidden in data. It is possible thanks to the ability of visualisation to make complex data more accessible, understandable and usable. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables. Da visualisation has become in the last year a well enstablished discipline thanks to the increased amounts of data created by Internet activity and an expanding number of sensors in the environment are referred to as “big data” or Internet of things. It is important to undeline how the way this data is comunicated present ethical and analytical challenges for data visualization practitioner (Bikakis 2018). The field of data science and practitioners called data scientists help address this challenge (Loukides 2011). Users of information displays are executing (consciously or not) particular analytical tasks such as making comparisons or determining causality (Tufte, Goeler, and Benson 1990). The design principle of the information graphic should thus support the analytical task, showing the comparison or causality (Tufte 2006). Graphical displays and principles for effective graphical display is defined as the ability to communicate complex statistical and quantitative ideas with clarity, precision and efficiency (Mulrow 2002). For this reason graphical displays should: show the data induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production or something else avoid distorting what the data has to say present many numbers in a small space make large data sets coherent encourage the eye to compare different pieces of data reveal the data at several levels of detail, from a broad overview to the fine structure serve a reasonably clear purpose: description, exploration, tabulation or decoration be closely integrated with the statistical and verbal descriptions of a data set In litterature are identified eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message (Few 2012): Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend. Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons. Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market. Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount. Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis. A boxplot helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc. Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message. Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison. Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used. Data visualisation practionires has to consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis. 5.6.1 The Grammar of Graphics Even if trial and error is and will remain an important part of data visualisation, some works as tried to give to data visualisation practitioners a well structured framework able to guide the process of data visualisation. Among the many frameworks, the most used is the Grammar of Graphics (Wilkinson 2006) and its implementation (Wickham, Chang, and others 2008). The grammar of graphics is a coherent system for describing and building graphs. Like other kind of grammars, it describes to basic rules to use the element of data visualization with the goal of comunicating some content. The main concept in the grammar of graphics is that graphs are made by multiple layers. Layers are responsible for creating the objects that we perceive on the plot. A layer is composed of four parts: Data and aesthetic mapping: Data are independent from the other components: we can construct a graphic that can be applied to multiple datasets. Along with the data, we need a specification of which variables are mapped to which aesthetics. Statistical transformation: A statistical transformation transforms the data, typically by summarizing them in some manner. Geometric object: Geometric objects control the type of plot that is created. For example, using a point geom will create a scatterplot, whereas using a line geom will create a line plot. Geometric objects can be classified by their dimensionality. Position adjustment: Sometimes there exist th need to tweak the position of the geometric elements on the plot, when otherwise they would obscure each other. This is most common in bar plots, where we stack or dodge (place side-by-side) the bars to avoid overlaps. Multyple layers togheter are used to create complex plots. Togheter with the layer the designer can control the scale. A scale controls the mapping from data to aesthetic attributes, and one scale for each aesthetic property used in a layer is needed. Scales are common across layers to ensure a consistent mapping from data to aesthetics. After the decision of the scale, the designer has ti decid the coordinate system for the layer. A coordinate system maps the position of objects onto the plane of the plot. Position is often specified by two coordinates (x, y), but could be any number of coordinates. The Cartesian coordinate system is the most common coordinate system for two dimensions, whereas polar coordinates and various map projections are used less frequently. For higher dimensions, we have parallel coordinates (a projective geometry), mosaic plots (a hierarchical coordinate system), and linear projections onto the plane. Coordinate systems affect all position variables simultaneously and differ from scales in that they also change the appearance of the geometric objects. Finally, the last element of the grammar are facets. Faceting makes it easy to create small multiples of different subsets of an entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions. The faceting specification describes which variables should be used to split up the data, and how they should be arranged. 5.7 Comunicate The last task to perform in the process of knowledge extraction from technical documents is communications. If it means to comunicate the results of an anlysis inside a team or to the world, it doesn not matter how great an analysis is unless it is impossibile to explain it to others (Wickham and Grolemund 2016). For the purposes of the present thesis, the focus is on the review of technical mechanics of communication especialli in the R (R Core Team 2018) enviroment. One of the most important innovation for the task of communication in data science is R Markdown (Allaire et al. 2018). R Markdown provides an unified authoring framework for data science, combining code, results, and comments. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more. R Markdown files are designed to be used in three ways: For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis. For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them ( i.e. the code). As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking. Togheter with reports (and usually contained in them) there are visualisation. Making graphics for communication follow all the rules and framework previously revised in section 5.6, but when a graph has to be used to comunicate to a wide audience there are some more rules to follow. The reason why this happen is that the audience likely do not share the background knowledge of the anlysit and do not be deeply invested in the data. To help others quickly build up a good mental model of the data, the analyst need to invest considerable effort in making plots as self-explanatory as possible. For this reason has been developed many tools to help data scientist to make effective comunication graphs(Wickham 2016, Chang et al. (2017), Sievert et al. (2017), Pedersen (2018), Bastian, Heymann, and Jacomy (2009)) . 5.8 Understand The most difficult challenge in technology intelligence is not how to detect the large trends- they are visible anyway. It is, rather, how to detect weak signals, or information that initially appears with low frequency, in unrelated or unexpected regions of the technology landscape, and associated with large noise (Apreda et al. 2016). These signals escape from traditional statistical detection techniques, exactly because it is difficult to distinguish them from pure statistical noise. Metadata are not the appropriate source of data for detecting weak signals. As a matter of fact, they can be detected only by using a fine-grained domain knowledge structure, or using the full text of documents. As an example, classification-based clustering has been shown to be flawed because the patent class used is usually only the first one listed in patents, generating loss of granularity (Benner and Waldfogel, 2008; Aharonson and Schilling, 2016). Hypothesis postulation 5.8.1 Domain Expertise (collins) Sheela Jasanow Taleb? 5.8.2 The problem of byases Each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs. The average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them. Moreover, it is also known that human analysis of text information is subject to considerable biases, e.g., people often pay greater attention to opinions that are consistent with their own preferences. People also have difficulty, owing to their mental and physical limitations, producing consistent results when the amount of information to be processed is large. Automated opinion mining and summarization systems are thus needed, as subjective biases and mental limitations can be overcome with an objective sentiment analysis system. 5.8.3 The Importance of Lexicons for Technical Documents Analysis References "],
["sotadocuments.html", "Chapter 6 Documents 6.1 Patents 6.2 Papers 6.3 Wikipedia 6.4 Social Media", " Chapter 6 Documents In this section contains a review of the main classes of technical documents analyzed in the present work. The documents are Patents, Papers, Wikipedia and Social Media. 6.1 Patents Nowadays patent data can be used for planning technological strategy (Ernst 2003). The focus on the technological usefulness of patent data is certainly a great advantage, but this huge research area could hide other useful application for patents. For example, in (Jin, Jeong, and Yoon 2015) the authors consider on one side patents as a source to collect information about technologies and products, and on the other side manuals, handbooks and market reports to collect market information. Since patents are only technological documents many potential patent reader (e.g. designers, marketers) could be taken aside. Despite this problem, some researchers (Bonino, Ciaramella, and Corno 2010) affirm that there is an increasing variety of readers: not only technician and researchers but also marketers and designers who have grown an interest in patent analysis. Nevertheless, to our knowledge there are no researches that aim at facilitating information extraction for non-technological focused patent readers. The bias that patent are only tech-oriented documents is due to two main reasons: Patents are produced to disclose and protect an invention, their content is mainly technical and legal. 80% of technical information is not available elsewhere (Terragno 1979), so patents are one of the most comprehensive resources for technical analysis. Focusing on the second point, our hypothesis is that also a fraction of all the other kinds of information (e.g. marketing and sociological information ) is not contained elsewhere and it will appear in public documents (e.g. manual handbooks and market reports) in 6-18 months (Golzio 2012). Unfortunately there are four aspects reducing the non-tech readers’ ability to analyze patents efficiently. First of all, an increasingly high number of patent filings generates a massive information overflow [Bergmann et al. (2008); secondly, analyzing patents takes a long time and requires skilled personnel (Liang and Tan 2007); the quality of patent assessment process is decreasing (Burke and Reitzig 2007, Philipp (2006)) because of the reduced assessment time available for patent examiners; finally, activities like patent hiding, proliferation and bombing, contribute to the generation of confusion and to the loss of time in research and analysis phases (Fantoni et al. 2013). These problems affect non-tech oriented patent readers as well as typical readers, even though the impact may be stronger on the firsts. The main difference between typical and non-tech patent readers is the information they focus on. Patent attorneys and Intellectual Property (IP) managers are interested in reading patents for legal reasons to orient the IP direction. Analyzing patents is the core of their work, so they are experts in finding the information they need. Furthermore, they can spend most of their work-time on the activity. On the other hand, usually marketers and designers (taken as example of non-tech oriented readers) search users’ behavioral changes and needs, market trends, designers’ vision, R&amp;D trends and competitors’ strategies. In addition, they rarely work with patents, so they do not know what and how to search. Lastly, they have short time to spend on the activity, and they waste most of this time understanding the legal and technical jargon used in patents. Due to the large amount of information contained in patents and the growing interest to exploit this information, huge efforts have been devoted to the development of systems source to automatically extract different kind of information from such an enormous and valuable data. Many techniques introduced in order to extract textual information from patents come from extensive research advances in the Natural Language Processing field (NLP). NLP is an area of research and artificial intelligence which aims at teaching computers to understand and manipulate natural language text in order to performs different tasks such as information extraction, machine translation and sentiment analysis. The field of technology intelligence has become so large in recent years that several efforts to review and summarize the various approaches have been undertaken (Abbas, Zhang, and Khan 2014). There are several possible ways to classify the approaches. For example a used classification distinguish between Visualization techniques (Patent networks, Clustering) and Text mining (NLP-based, Property-function, Rule-based, Semantic analysis, Neural networks). Another possible classification is: Metadata approaches: methods that uses sources of information embedded in patents, such as claims structure or bibliographic information Keyword approaches: methods able to produce vector representations of the analyzed documents. Computed vectors can be used for many applications such as patent retrieval by keyword, or patent similarity matching. Even though this approach can be used for several tasks, it is not suitable to catch semantic relationships between entities in sentences. Furthermore, these methods use a blacklist to remove noisy words (Blanchard 2007) or use predefined lexicons (Chiarello et al. 2017). The right design of such list dramatically impacts the final output of the analysis (Sungjoo Lee, Yoon, and Park 2009a, C. Lee, Kang, and Shin (2015a), Montecchi, Russo, and Liu (2013)) Natural Language Processing approaches: methods based on grammatical and syntactical structures extracted by natural language processing tools, such as Part-of-Speech taggers and syntactical parsers. Unlike the keyword based approach, these methods are able to capture the relationships between the entities mentioned in sentences (J. Yoon, Park, and Kim 2013a,Park, Yoon, and Kim (2011a), Park et al. (2013)). Each approach allows to capture different types of information from patents and build a knowledge base which can be exploited by patent analysis tools. For this reason the right approach to be chosen to develop a patent analysis system depends on the task to be solved, on the information to be analyzed and on the computational resources involved to solve the task. Choosing a good trade-off between these factors is a strict requirement in particular when analyzing big patent sets. 6.1.1 Metadata Approaches Patent documents has the following metadata: Patent office Inventors Affiliation of the inventors Filling date Publication date Address of the affiliation of the inventor Patent classifications References Assignee Affiliation of the assignee Address of the affiliation of the assignee Furthermore they contain text content: Title Abstract Keyword Summary page Drawing set Background of the invention Brief summary of the invention Brief description of the drawings Detailed description of the invention Claim set This does not mean that metadata allow a unique identification. Disambiguation of metadata remains a challenge in most cases. Only recently documents have started to include a unique identifier, following the cooperation among the main producers and users. The unique identifiers refer to the publication (DOI, Digital Object Identifier) or the author (author ID). However, metadata are written and stored in a standardized way, so that it is possible to categorize them. Issues of disambiguation refer mainly to the identity of individual entities (e.g. distinguish between two authors with exactly the same name and surname) but not of categories (e.g. distinguish between the name of an author and the name of a university). Metadata approaches for patent analysis exploit three types of information: bibliometric information patent structure information patent review process information. In this approach are usually considered both patent and non-patent literature: for example patents with a high number of citations in papers usually indicate a strong correlation with the foundation of a technology. On of the main problems addressed using metadata approaches is measure of the technology life cycle stage. The information about at which stage of maturity a technology is, is an important aspect taken into account by who decides to invest. Since the life cycle of a product is clearly related by patent grants evolution (Andersen 1999), this lead research to investigate on patent indices that can be considered as appropriate life cycle stage indicators. The main effort has been directed in the identification of the three different technology life cycle stages: introduction, growth and maturity (Haupt, Kloyer, and Lange 2007). In this work, the author took into account that several studies have shown that a S-shape evolution of the number of patent applications or even a double-S-shape is typical. Consequently, the author defined the concept of patent activity index as an appropriate life cycle indicator only if its mean value differs significantly between the life cycle stages. The results of the work can be summarised as follow: Backward literature citations increase significantly only at the transition from introduction to growth; Backward patent citations increase significantly at both stage transitions; The number of forward citations decreases significantly at the transition from introduction to growth; The number of dependent claims is significantly higher at later technology life cycle stages than in earlier ones; The number of priorities referred to in a patent application is significantly higher at later technology life cycle stages than in earlier ones; Examination processes take longer in the phases of introduction and maturity than at the growth stage. The main limit of these methods is the need of assumptions for what concerns the shapes of the stages curves. For this reason furthuter works introduced an unsupervised method able to automatically detect the number of life cycle stages and the transition times of the technology of interest (Lee et al. 2016). Here, seven time series patent indicator were taken into account: patent activity which allows to model the evolution of a pattern. In particular increasing and decreasing patterns are considered a change for what concerns the research and development activity; the number of technology developers in the analyzed temporal series. It has been shown that a great number of competitor enters in the initial stages of a technology’s life cycle, but this number lowers in the matu- rity stage the number of different patent application areas in the considered temporal series. This is an important indicator since it has been shown that the number of technology application areas are small in the first stages of their life cycles and in- creases in the later life cycle stages the number of backward citations. It has been shown that patents with an high number of backward citations have less relevance with respect to the patents with a lower number of citations the number of forward citations which expresses the technological value of a patent in the analyzed temporal period the duration of examination processes as the average time between the filing and granting dates. the number of claims belonging to the patent. The more the number of claims reported by the patents, the higher the correlation with novelty and the financial value is. Another widely adressed problem using metadata is citation analysis. Publications, patents, technical standards or clinical guidelines include a section in which other documents are cited. Citation analysis argues that including the reference to another document is the result of an intentional act, whose meaning may differ according to the type of document, but is nevertheless always worth of consideration (Moed 2006). The analysis of citations, initially developed in scientometrics and bibliometrics, has migrated to technology intelligence, following the initial concept of patent bibliometrics (Narin 1994). Patent (or firms, or inventors) that cite the same prior art are clustered together. Patent citation networks are then generated (Karki 1997, Érdi et al. (2013)). In fact, citations form a network structure, whose graph-theoretic properties can be interpreted in technology intelligence exercises (Lee and Kim 2017). Patent citation networks have properties of small world (Cowan and Jonard 2004) and their degree follows a power law distribution (Chen and Hicks 2004). Patent citation analysis can be used to identify trajectory patterns and technology structure and paths, that is, knowledge flows among firms and among subsectors of an industry. In standard citation analysis all citations are considered equal. This counting approach can be criticized because “it relies on the assumption that patents are equally significant” (Gerken and Moehrle 2012), which is in contrast with the empirical evidence on the large differences in patent value. This assumption is therefore removed in more advanced techniques in which the structure of citations from patents gets a qualification. It may be possible, however, that citations to other patents are strategically made by applicants, for example by citing their own patents or hiding other relevant citations). Backward citations may not be associated to technological novelty if they deliberately point only to the state of the art (Rost 2011). Citations introduced by examiners are also another potential source of bias (Alcacer and Gittelman 2006). Finally, it has been reported that the total number of citations increased over time, leading to “citation inflation” and the loss of value (Hall, Jaffe, and Trajtenberg 2001). Derived from citation analysis, co-citation analysis argues that documents that are cited by the same documents should be considered part of the same cluster (Small 2006, Small and Sweeney (1985)). A variant of this technique, called author co-citation analysis (White and Griffith 1981), cluster documents that are cited by the same authors. Co-citation analysis can also be used to create classification systems of patents (Lai and Wu 2005). 6.1.2 Keywords Approaches In the keyword based approach each patent is represented as a vector where each component measure the importance of a specific keyword, like explained in section 5.4.5. The keywords to be taken into account depend on the patent set under analysis and on the goal of the task. Keywords can be extracted automatically using a text mining module, manually by experts or with hybrid methods where domain experts judge the relevance and the quality of the extracted keywords in order to limit the results to the most important keywords. Once keyword vectors are obtained, tasks such as patent similarity can be easily computed by using standard distance measures like cosine similarity. In addition, the keyword extraction allows to define more complex patent similarities measures (Moehrle 2010) that can be exploited for the development of patent analysis tools (Sungjoo Lee, Yoon, and Park 2009b,C. Lee, Kang, and Shin (2015b)) such as mappers or patent search engines. The main goal of these works is to developed systems for building keyword-based patent maps to be used for technology innovation activities. The system is composed of a text mining module, a patent mapping module and a patent vacancies identification module. Once a specific technology field is taken into account for analysis and a related patent set is extracted, the modules of the system are sequentially executed. The text mining module automatically identifies relevant keywords in each patent of the considered patent set. Once all the keywords are extracted, only the ones with the highest relevance are selected for a further screening by domain experts. The final set of keywords resulting from the screening process is then considered for building the patent keyword vectors on the considered patent set. Specifically each component of the patent vector holds the frequency the corresponding keyword in the considered patent. Once all the keyword vectors are computed, the patent mapping module is executed to generate the patent map. The mapping is calculated by executing the Principal Component Analysis (PCA) algorithm on all the vectors. The PCA method allows to map n-dimensional vectors on a rectangular planar surface in order to generate the patent map. Intuitively this method allows to find the most meaningful 2 dimensional projection that filters out the correlated components of a n-dimensional vectors. The result of applying this method over the patent keyword vectors is a meaningful patent mapping, in which each patent is mapped over a 2-dimensional surface. Once the patent map is computed, a vacancy detection module is executed on the patent map. The vacancy detection module identifies sparse areas which can be considered good candidates for a research investigation. For each interesting vacancy, a list of related patents is obtained by selecting the ones which are located on the region boundaries. On the calculated list, a set of information for each patent is computed. This information is used to capture the importance of a patent in this patent list. Features considered strong indicators of the relevance of each patent are the number of citations [38] and the number of average citations by patents in the patent list. Finally, emerging and declining keywords are computed by taking into account the time series analysis of the considered keywords in the patent list. This allows to identify promising technology trends that can considered for further investigation. The metadata and keyword approaches has a long tradition but suffers from several limitations. First, the initial query based on keywords is usually produced by human experts, either on an individual basis or organized as a panel. In practice, one of the best skills of research centers or consultancies specialised in technology intelligence has been, in the past, the ability to mobilize high level experts on an international basis in order to produce well crafted query lists. Unfortunately these lists, even if they are produced following elicitation procedures that respect state of the art recommendations in social sciences, are inevitably biased. Experts are extremely good in their field, but are not better that others if they have to evaluate matters that are outside their domain (Burgman 2015). To the extent that emerging technologies are complex and fast evolving technologies, it is likely that experts have a narrow, or biased, perception of the dynamics. Experts tend to keep their existing R&amp;D areas in mind, have personal and organizational inclinations, are subject to halo effects in favor of well known institutions or solutions, and may follow different criteria for selecting promising technologies (Kim and Bae 2017). It has been shown that little differences in the wording of queries, or on the time window, may end up in completely different sets of documents, leading the analysis in different directions (Bassecoulard, Lelu, and Zitt 2007). In addition to these authors, several studies in recent years have called the attention to the risk that initial differences in the delineation process generate non-comparable descriptions of technologies (Mogoutov and Kahane 2007,Youtie, Shapira, and Porter (2008), Ghazinoory, Ameri, and Farnoodi (2013)). Following this line of concern,methodologies to update the keyword structure in an iterative, or evolutionary way has been proposed (Mogoutov and Kahane 2007). Second, it has been shown that when experts are asked to decide on relatedness measures (e.g. synonyms, hypernims or hyponims), they do not apply systematic rules (Tseng, Lin, and Lin 2007,Noh, Jo, and Lee (2015)). Third, the query list is static. Once defined, it is used to extract documents from large corpora, which are then processed. In dynamic technologies, it is likely that the pace of technological changes exceeds the speed of updating of the query lists. It is difficult to convene panels of experts repeatedly, also because of the large costs incurred in expert selection and management (Tseng, Lin, and Lin 2007). As an example, with the advent of nanotechnology it was felt the need to introduce a new patent sub-class. The sub-class B82B was introduced in year 2000, but it did not incorporate the previous patents, so that a comparison across time is not feasible. A new sub-class, B82Y, was introduced in 2011 (Kreuchauff and Korzinov 2017). 6.1.3 Natural Language Processing approaches The impressive advancements of computational linguistics in the last two decades have made it possible to carry out analysis on the full content, not only the metadata, of large collections of texts. In text mining patterns are extracted from unstructured collection of documents, while in the metadata approach the patterns are extracted from structured documents or databases. This has opened the way to the “full text based scientometrics” (Boyack, Small, and Klavans 2013) and has created the conditions for the convergence between the citationist approach illustrated above, and the lexical approach. Text mining techniques have then been applied to the corpus of patent texts, with a number of extremely powerful results (Tseng, Lin, and Lin 2007, Joung and Kim (2017), Kreuchauff and Korzinov (2017), Ozcan and Islam (2017); Yoon and Kim 2012). In turn, text mining can be applied for the search of specific words (or combination thereof) or in the search for patterns that are not defined ex ante. In the former case the most used techniques are combination of keywords, correspondence analysis or category specific terms. These approaches expand the search over the full text of patents but preserve the limitations of keyword-based search. On the contrary, the search for patterns is the object of the most largely used technique, namely topic modelling. Pattern recognition in patent texts is “still in its infancy” (Madani and Weber 2016) but its applications are growing rapidly. A useful review of NLP techniques in patent analysis (Madani and Weber 2016) identifies: the statistical approach that uses the Term Frequency-Inverted Document Frequency (TF-IDF) method to detect regularities the semantic approach uses SAO (Subject-Action-Object) and property-function structures in order to attribute meaning to the texts the corpus approach adopts ontology-based techniques. In turn, all these three information retrieval approaches can be extended by using pattern recognition techniques, that are keyword-, patent- or concept-based. Text mining has several limitations : it cannot consider synonyms and the co-occurrence of keywords, while the inclusion of compound words and n-gram expressions requires large computational power. In addition, in the case of patents, claims are written in “arcane legalese” in order to hide critical elements and confound potential competitors. The challenge here is how to maximize the substantive knowledge that can be generated by automatic processing of the full text. It has been remarked since long time that a promising direction for research into technology intelligence and foresight lies in the combination of methods. This recommendation requires the combination between domain-knowledge and powerful computational approaches. It is this combination that holds the best promise to generate methods for the identification of emerging technologies, and more generally, for technology intelligence, that are able to identify high-granularity information producing weak signals, that is, to distinguish accurately the signal from the noise in turbulent and dynamic technological landscapes. By exploiting the information obtained by these steps, several information extraction tasks can be solved by other NLP tools such as: - Term extraction: the task of automatically extract relevant terms from a given corpus. Part of Speech tags are typically used by term extractors to narrow the terms search to a predefined term structure; - Named entity recognition: the task of automatically identify and classify named entities in text such as persons, organizations and locations. Named entity recognizers usually use Part of Speech tags in order to disambiguate the morphosyntactic role of tokens in a phrase, improving the performance of the extraction; - Relation extraction: the task of automatically build relations among entities in the analyzed text. In this context entities can be named entities or extracted terms. In addition, the syntactic role of the entities can be exploited to better categorize the relation type (e.g.: subject, object). Technical domain language, as other linguistic domains, suffers from linguistic ambiguities. For instance the word “support” can have two totally different meanings when used as a noun or as a verb. By using part of speech taggers which are able to disambiguate the morphological role of each word in a sentence, more precise information extractions are possible and can be used in several applications (e.g. patent search engines). In addition part of speech taggers allow to perform textual lemmatization, which can further improve the performances of automatic patent analysis tools. Another key NLP tool used by several automatic patent analysis systems are syntactic parsers: by identifying the syntactic role of each word in document sentences, several patent analysis applications are possible. The most well established system for patent analysis using NLP techniques is the extraction of the Subject-Action-Object (SAO) structures, which is also a common use of syntactic parsers in automatic patent analysis tools. Each SAO structure represents the subject (S), the action (A) and the object (O) in a patent sentence (Yoon and Kim 2011). By automatically extracting SAO structures from patents, relationships between key technological components can be easily represented (J. Yoon, Park, and Kim 2013b, Choi et al. (2011), Park, Yoon, and Kim (2011b)). Another techniques that is growing in patent literature analysis is Named Entity Recognition (for further details see section 5.5.5). The Named Entity Recognition (NER) is the task of identifying entity names like people, organizations, places, temporal expressions or numerical expressions. Entity extraction tools used in patent analysis are largely based on NLP tools which can be applied to the analyzed text to extract entities that are important for the extraction purpose. For example, in the chemical field relevant entities are chemical components, proteins or product names. For the latter cases, adaptations of Named Entity Recognizers (NER) are commonly used for this task. Methods and algorithms to deal with the entity extraction task are different, but the most effective are based on supervised methods. Supervised methods tackle this task by extracting relevant statistics from an annotated corpus. These statistics are collected from the computation of features values, which are strong indicators of the identification of entities in the analyzed text. Features used in NLP based entity recognition systems, are divided in two main categories: linguistically motivated features, such as n-grams of words, lemma and part of speech; external resources features as, for example, external lists of entities that are candidates to be classified in the extraction process. The annotation methods of a training corpus can be of two different kinds: (a) human based, which is time expensive, but usually effective in the classification phase; (b) automatically based, which can lead to annotation errors due to language ambiguity. As an example crack can be classified both as a drawback (a fracture), or not drawback (short for crack cocaine). Different training algorithms, such as Hidden Markov Models (Eddy 1996b), Neural Networks (Haykin and Network 2004), Conditional Random Fields (Lafferty, McCallum, and Pereira 2001b) or Support Vector Machines (Hearst, Dumais, Osman, et al. 1998), are used to build a statistical model based on the features that are extracted from the analyzed documents in the training phase. The same statistical model is later used in classification of unseen documents. For what concerns the extraction of specific entities in patents, a major interest both in academia and commercial organizations has raised in the latest years, with the main aim of improving the accuracy of domain specific patent retrieval systems (Krallinger et al. 2015). In (Lee and Kang 2014) the authors proposed a machine learning based patent NER system that identifies key terms in patent documents and recognizes products, services and technology names in patent summaries and claims. In this work a study was conducted to identify the most relevant features for this classification task and by using lexical features like word uni-grams, word bi-grams and word trig-rams, their NER system reached an F1 score (the harmonic mean of precision and recall) of 65.4%. The authors compared their NER tagging system resulting from the optimal feature selection method, with the human tagged corpus, showing that the kappa coefficient was 0.67. This result was better than the kappa coefficient between two human taggers (0.60). Other entity extraction systems for the patent domain were proposed for the CHEMDNER (chemical compounds and drug names recognition) community challenge (Krallinger et al. 2015). The main aim of the organizers was to promote the development of novel, competitive and accessible chemical text mining systems. The best results were obtained by the tmChem system (Leaman, Wei, and Lu 2015), achieving a 0.8739 f-measure score. The authors proposed an ensemble system composed of two Conditional Random Fields based classifiers, each one using hard feature engineering such as lemmatization, stemming, lexical and morphological features. In addition, external lists of entities were exploited to recognize whether a token matched the name of a chemical symbol or element, each one used to compute features to be added in the final statistical model. The described entity tagging systems have very good performances mainly for two reasons: firstly, chemical entity names (such as molecular formulas) have very common orthographic patterns; secondly, these entities surrounding contexts are very similar. In more generic cases, these two features can not be exploited for entity extraction from patents, since different words have totally different surrounding contexts. Another important key factor concerning the high performances of the described systems is that many external resources, such as lists of chemicals or product names, are available: this external knowledge can not be fully exploited in generic system. 6.2 Papers 6.3 Wikipedia The use of Wikipedia as source of knowledge started more than a decade ago and has been validated repeatedly in a variety of text mining applications (text annotation, categorization, indexing, clustering, searching (Milne and Witten 2008)). In addition to the large and growing size in terms of number of articles, the structure of Wikipedia has a number of useful features that make it a good candidate for text mining applications. First, Wikipedia pages are considered reliable in many knowledge fields, including the ones more interesting for technical analysis, i.e. engineering and computer science (Xu et al. 2015). The pages are regularly and systematically updated by a large global community of contributors, which includes many scientific and industrial authorities in the field. The use of Wikipedia as knowledge source for computerized text mining tools is established in the literature (Ferragina and Scaiella 2012). In addition, it is powerful in disambiguation of terms, particularly through the use of redirect pages and disambiguation pages. This means that it can be used for detection and disambiguation of named entities (Bunescu and Paşca 2006). Second, the pages include links to other pages motivated by clear reasons on content. There are many links between Wikipedia pages, which are clues for semantic relations. This makes Wikipedia a densely connected structure, creating a classical small world effect: according to an often cited estimate, it takes on average 4.5 clicks to reach an article from any other article (Dolan 2008). Unfortunately it is not possible to disentangle the kind of semantic relation, introducing a distinction between equivalent relations (synonymy), hierarchical relations (hyponymy/ hyperonimy) and associative relations, but this limitation is not relevant for our applications. Third, it makes use of categories which do not have a hierarchical structure, but a tree-like structure. Fourth, it has the ability to evolve quickly (Lih 2004), particularly after the development of systems such as Wikify (Mihalcea and Csomai 2007,Cheng and Roth (2013)). Wikipedia has by design a dynamic structure, since it is constantly growing in the number of entries and changing in their content, when this is needed due to the advancements of knowledge (Ponzetto and Strube 2007). Furthermore the new terms that appear on Wikipedia thanks to comprehensive contributions by volunteers around the world, cannot be found in other linguistic corpora, such as WordNet Miller, 1995. Indeed, Wikipedia is the expression of a large international community, that is, of a “real community agreement” (Bizer et al. 2009) or “community consensus” (Hepp, Siorpaes, and Bachlechner 2007), guaranteed by permanent collective monitoring of the quality and rigor of the entries (Bryant, Forte, and Bruckman 2005). Finally, Wikipedia is free-content and multilingual. This make it possible to freely collect the information contained in the web pages and allows the possibility for future developments of the dictionary in other languages. In our opinion multilanguage is an interesting feature for the dictionary, due to the fact that Industry 4.0 is a worldwide phenomena. These properties make Wikipedia the ideal candidate for the goal of extracting technical knowledge from texts. Technical fields are in fact comprehensive, dynamically updated, and, as far as possible, expert-independent. In particular, Wikipedia entries allow an endogenous measurement of semantic relatedness. This is an exceedingly important property for technical analysis: technologies can be mapped and can be defined as included in the perimeter of a knowledge field if and only it exhibits relatedness with other technologies already included in the perimeter. The inclusion of new technologies is therefore not dependent on experts’ subjective views, but is endogenously generated by the technological community that writes the articles for the encyclopedia and includes hyperlinks in the text of newly added pages. 6.4 Social Media Nowadays, more than ever before, companies, governments, and researchers can gather and access data about people on a massive scale. Monitoring public opinion is increasingly made possible thanks to the rise of Social Media. These ones are computer-mediated technologies that facilitate the creation and sharing of information, ideas, career interests and other forms of expression with friends, families, co-workers, and other users, via virtual communities and networks. There are many different Social Media platforms, each of which targets a different aspect of what users want or need: e.g., LinkedIn targets professional networking activities, Facebook provides a mean of connecting friends and family, and Twitter provides a platform from which to quickly broadcast thoughts and ideas. These platforms are incredibly popular: as of February 2017, Facebook sees an average of 1,871 billion active users, with 76% of them that logging in every day (Tuten and Solomon 2017). Being so widely used, Social Media platforms generate huge amount of data. In 2013 users were posting an average of over 500 million tweets every day (Krikorian 2013). Social Media are not constrained by national, cultural, and linguistic boundaries differently from traditional data sources and records of human activities, such as newspapers and broadcast media. Moreover, traditional media requires time to compile relevant information for publication, while Social Media data is generated in real-time as events take place. Virtually anyone who wishes to use all this information could collect and mine it. In 2009, the United States Geological Survey (USGS) began investigating the possibility of using SM data to detect earthquakes in real time (Ellis 2015). Information about an earthquake spreads faster on Social Media than the earthquake itself can spread through the crust of the Earth (Konkel 2013). Similarly, interesting work in Social Media forecasting also exists: EMBERS is a currently deployed system for monitoring civil unrests and forecasting events such as riots and protests (Ramakrishnan et al. 2014). Using a combination of Social Media and publicly-available, non-SM, researchers are able to predict not just when and where a protest will take place, but also why a protest may occur. These encouraging results have stocked the interest of researchers toward the possibilities opened by Social Media data, although some unanswered questions remain. If Social Media is useful for detecting real-time events, can it be used to make predictions about the future? What limitations does forecasting with Social Media data face? What methods lead researchers to positive results with Social Media data? However, some researchers are pessimist about Social Media analysis. According to (Ruths and Pfeffer 2014, Weller (2015)), Social Media is noisy, and the data derived from it are of mixed quality: for every relevant post there may be millions that should be ignored. Learning with Social Media data sometimes requires robust statistical models. Nevertheless, researchers continue to investigate how best to make use of Social Media data. First studies show positive findings. Social Media users not only react to and talk about events in real time, but also talk about and react to events that will happen in the future. This fact fuels the interesting possibility that Social Media data might be useful for forecasting events: making predictions about future events. Not only have researchers begun to investigate this line of questioning, earlier review articles on Social Media forecasting showcase early positive examples of predictive success (Kalampokis, Tambouris, and Tarabanis 2013, O’Leary (2015), Schoen et al. (2013)). A lot of studies show that Social Media could be used to predict the future. At the same time, some works have been controversial (Schoen et al. 2013). It’s clear that this domain of research is in its infancy, methodologies are different, common best practices are difficult to determine, and true replication of studies is near-impossible due to data sharing concerns (@ Weller 2015). The use of data from Social Media for modelling real-world events and behavior has seen a growing interest since his first appearance in academic world around 2008. This increasing popularity is proportional to the leaps ahead made in computational social science. In the past, many sociological theories were hard to prove for the difficulties encountered in gathering indispensable data. Today, Social Media can record so many sides of human relationships on the web from millions of people all around the world. On the other hand, Social Media data cannot always provide a complete picture of what researchers might hope to see. The use of Social Media varies depending on age, culture, social background, gender and ethnicity. However, positive findings and the interest in fundamental dynamics of Social Media platforms explain the exponential growth in popularity of this field of research. Social Media data has a huge potential but understanding if its application can be useful is not a trivial task. Forecasting models (data- or theory-driven) are important in many fields but Social Media data challenges researchers to find new ways to apply them. In natural sciences, aggregating techniques of data coming from network of sensors are important, but Social Media data challenges researchers to find new ways to increase their forecasting power. Researchers should first identify the methods through which Social Media challenges may be addressed to be able to make valid and reliable predictions. Among these difficulties, there are: noisy data, possible biases, a rapidly shifting Social Media landscape that prevents generalization and a need for domain-specific theory that brings all together. Furthermore it is important to chose the best text source for Social Media analysis, among the many available. Previous studies found that researchers focused mainly on Twitter data (Giacomo 2017). While Facebook is trying to compete, and Snapchat offers a unique perspective on the theme, Twitter remains the best indicator of the wider pulse of the world and what is happening in it. According to Hamad (Ahmed 2017), there are at least six reasons that explain the importance of Twitter for Social Media analysis: 1. Twitter is a popular platform in terms of the media attention it receives, and it therefore attracts more research due to its cultural status; 2. Twitter makes it easier to find and follow conversations (i.e., by both its search feature and by tweets appearing in Google search results); 3. Twitter has hashtag norms which make it easier gathering, sorting, and expanding searches when collecting data; 4. Twitter data is easy to retrieve as major incidents, news stories and events on Twitter are tending to be centered around a hashtag; 5. The Twitter API is more open and accessible compared to other Social Media platforms, which makes it more favorable to developers creating tools to access data. This consequently increases the availability of tools to researchers; 6. Many researchers themselves are using Twitter and because of their favorable personal experiences, they feel more comfortable with researching a familiar platform. It is probable that a combination of the response from 1 to 6 led to more research on Twitter. However, this raises another distinct but closely related question: when research is focused so heavily on Twitter, what (if any) are the implications of this on methods? As for the methods that are currently used in analysing Twitter data i.e., sentiment analysis, time series analysis (examining peaks in tweets), network analysis etc., can these be applied to other platforms or are different tools, methods and techniques required? Below has to be considered whether these methods would work for other Social Media platforms (Ahmed 2017): Sentiment analysis works well with Twitter data, as tweets are consistent in length (i.e., ) would sentiment analysis work well with, for example Facebook data where posts may be longer? Time series analysis is normally used when examining tweets overtime to see when a peak of tweets may occur, would examining time stamps in Facebook posts, or Instagram posts, for example, produce the same results? Or is this only a viable method because of the real-time nature of Twitter data? Network analysis is used to visualize the connections between people and to better understand the structure of the conversation. Would this work as well on other platforms whereby users may not be connected to each other i.e., public Facebook pages? Machine learning methods may work well with Twitter data due to the length of tweets (i.e., ) but would these work for longer posts and for platforms that are not text based, i.e., Instagram? Maybe at least some of these methods can be applied to other platforms, however they may not be the best methods, and may require the formulation of new methods and tools. In conclusion, Twitter is the best for Social Media analysis for now. Despite its smaller user base compared with Facebook, its responsiveness and openness to researchers’ tool make possible gathering useful data. Since the usage of social media has a wide impact on a great number of disciplines, here is exposed the main literature in the most technical related fields that are strongly related to social media analysis: economics and marketing. 6.4.1 Economics This domain has raised the great interest of researchers. The first studies focused especially on market fluctuation and on aggregated measure, such as Dow Jones Industrial Average (DJIA). Most recent researches have gone further predicting single stock price and yield. Great interest in Social Media analysis for economics has been on Stock market analysis. Stock price forecasting is an important and thriving topic in financial engineering and is considered a very difficult task, even outside Social Media. Many articles in this context present models based on sentiment analysis to make forecasts (Xu and Keelj 2014, Kordonis, Symeonidis, and Arampatzis (2016), Cakra and Trisedya (2015), Cakra and Trisedya (2015), Wang and Wang (2016), Shen, Dong, and He (2016), Brown (2012), Rao and Srivastava (2012)), although some researchers realised more detailed models: Crone et al. (Crone and Koeppel 2014) implemented neural networks and incorporated non-SM sources, and Shen et al. (Shen, Dong, and He 2016) developed a model that studies the connection between consumers’ emotion and commodity prices. The simplest task for stock market forecasting is predicting whether the following day will see rise or fall in stock prices. Comparison between researches is complicated by the fact that stock market volatility, and so the difficulty of prediction, may vary over time periods. High accuracy on this task was reported by Bollen et al. (Bollen, Mao, and Zeng 2011), using sentiment analysis to achieve an accuracy of 87,6%. They investigated whether measurements of collective mood states derived from large-scale Twitter feeds are correlated to the value of the Dow Jones Industrial Average (DJIA) over time. They analysed the text content of daily Twitter feeds by two mood tracking tools, namely OpinionFinder, that measures positive vs negative mood, and Google-Profile of Mood States (GPOMS) that measures mood in terms of 6 dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). They find that measures of “calm” on Twitter along with DJIA numbers from the previous three days provide the best up/down predictions. Further adding the emotion “happy” reduces rise/fall accuracy to 80% but does reduce error in terms of forecasting absolute DJIA values. Importantly, they find that positive/negative sentiment analysis through the popular OpinionFinder’s tool leads to no improvement over just using previous DJIA values. In conclusion, researchers obtained good results forecasting up/down movements in the stock market. Furthermore the topic of Sales and revenues is of great interest for people working on economics. For example, boosting movie ticket sales is an important task for producers and publishers, and this has been studied specifically on Social Media platforms like Twitter. Asur et al. (Asur and Huberman 2010) showed how social media content can be used to predict movie success. In particular, they used the chatter from Twitter.com to forecast box-office revenues for movies. Specifically, using the rate of chatter from almost 3 million tweets, they constructed a linear regression model for predicting box-office revenues of movies in advance of their release. Then, they showed that the results outperformed in accuracy those of the Hollywood Stock Exchange and that there is a strong correlation between the amount of attention a given topic has (in this case a forthcoming movie) and its ranking in the future. They also analysed the sentiments present in tweets and demonstrated their efficacy at improving predictions after a movie has released. Cheng et al. (Cheng et al. 2013) obtained mixed results developing a model for predicting TV audience rating. They accumulated the broadcasted TV programs’ word-of-mouse on Facebook and apply the Back-propagation Network to predict the latest program audience rating. They also presented the audience rating trend analysis on demo system which is used to describe the relation between predictive audience rating and Nielsen TV rating. Kim et al. (Kim, Suh, and Lee 2014) investigated the relationship between music listening behavior in Twitter and the Billboard rankings. They found that the play-counts extracted from tweets have strong relationships with the Billboard rank, whereas, interestingly, the artist popularity extracted from tweets has a weak correlation with future chart rankings. In addition, the number of weeks on chart information alone was insufficient to predict rank alone. With the features extracted from tweets, They built three regression models to predict the ranking. Among the proposed models, SVR (Support Vector Machine) showed the highest squared correlation coefficient (0.75). Although the combined model with the number of weeks on chart performed the best in rank prediction, the music listening behavior available in Twitter can generate an outstanding predictive model. They also built a hit prediction classifier with the features acquired in tweets and the number of weeks on chart. They classified the hit and non-hit songs in the Billboard Hot 100 and obtained a value of 83.9% accuracy, 83% precision, and 85.3% recall for classifying a hit song over the whole data set. The proposed feature showed a high performance both for rank prediction and hit classification. The previous week’s twitter features and the number of weeks on chart are effective for predicting the Billboard rank of a song. Ahn et al. (Ahn and Spangler 2014) focused on periodic forecasting problems of product sales based on social media analysis and time-series analysis. In particular, they presented a predictive model of monthly automobile sales using sentiment and topical keyword frequencies related to the target brand over time on social media. Their predictive model illustrates how different time scale-based predictors derived from sentiment and topical keyword frequencies can improve the prediction of the future sales. Tuarob et al. (Tuarob and Tucker 2013) proposed a Knowledge Discovery in Databases (KDD) model for predicting product market adoption and longevity using large scale, social media data. In particular, the authors analysed the sentiment in tweets and use the results to predict product sales. The authors presented a mathematical model that can quantify the correlations between social media sentiment and product market adoption in an effort to compute the ability to stay in the market of individual products. The proposed technique involves computing the Subjectivity, Polarity, and Favorability of the product. Finally, the authors utilised Information Retrieval techniques to mine users’ opinions about strong, weak, and controversial features of a given product model. The authors evaluated their approaches using the real-world smartphone data, which are obtained from www.statista.com and www.gsmarena.com. The findings show that tweets can be used to predict product sales for up to at least 3 months in advance for well-known products such as Apple iPhone 4, Samsung Galaxy S 4G, and Samsung Galaxy S II, thus the predictive ability varies across products. 6.4.2 Marketing Scholars had a great focus in the last years on using Social Media Information for marketing. Chen et al. (Chen et al. 2015) conducted a survey study and a field study to explore the feasibility of using predicted personality traits derived from social media text for the purpose of ad targeting. In the survey study, they measured people’s personalities and their responses to an advertisement tweet. They found that people with high openness and low neuroticism responded more favorably to a targeted advertisement, thus demonstrating the effects of the personality traits themselves. In the field study, they sent the advertisement tweets to real-world Twitter users, and found the same effects on users’ responses using personality traits derived from users’ tweet text. They demonstrate that aiming advertisements at users with particular personality traits improves click and follow rates by 66% and 87% respectively, representing a large increase in value for companies. These results suggest that the derived personality traits had the same effects as the personality traits measured by traditional personality questionnaires and can indeed improve ad targeting in real-world settings. Li et al. (Li, Rakesh, and Reddy 2016) present a solution to the problem of predicting project success in a crowd-funding environment combined with innovative introduction of survival analysis based approaches. They used comprehensive data of 18 thousand Kick-starter (a popular crowd-funding platform) projects and 116 thousand corresponding tweets collected from Twitter. While the day of success is considered to be the time to reach an event, the failed projects are considered to be censored since the day of success is not known. They performed rigorous analysis of the Kick-starter crowd-funding domain to reveal unique insights about factors that impact the success of projects. Their experimental results show that incorporation of failed projects (censored information) can significantly help in building a robust prediction model. Additionally, they also created several Twitter-based features to study the impact of social network on the crowd-funding domain. Their study shows that these social network-based features can help in improving the prediction performance. They found that the temporal features obtained at the beginning stage (first 3 days) of each project will significantly improve the prediction performance. Even when just using Social Media information from the first three days of the project, they achieve an AUC of 0.90, reflecting very high classification performance. This part describes the methods applied for the analysis of technical documents. The methods are ensamble of Natural Language Processing (NLP) and Text Mining techniques described in 5, re-designed depending on the analyzed document and the analysis goal. Not all the techniques have been applied to all the documents: table tot summarise the relations between the documents under analysis (introduced in section 6) and the NLP techniques. Table documents vs tools Each chapter starts with a brief description of the field of applicaton of the method and with the framing of the problem to be solved. Then the methodlogy to solve the prolbem is described. Each chapter closes with the results. References "],
["patents.html", "Chapter 7 Patents 7.1 Users 7.2 Advantages and Drawbacks 7.3 Trademakrs", " Chapter 7 Patents Patents contain a large quantity of information which is usually neglected. This information is hidden beneath technical and juridical jargon and therefore so many potential readers cannot take advantage of it. State of the art natural language processing tools and in particular named entity recognition tools, could be used to detect valuable concepts in patent documents. A deeper description of what patents are and how these documents are used to mine technical knowledge can be found in section 6.1 In this section we present three methodologies capable of automatically detecting and extracting threee of the multiple entities hidden in patents: the users of the invention, advantages and drawbacks of the invention and trademarks contained in patents. The results of the methodologies are described, togheter with example of applications of the extracted entities for intelligence tasks. 7.1 Users Patents are documents that must provide a detailed public disclosure of an invention (Idris 2008). An invention is a new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof.1 The notion of usefulness implies that the invention must have some value and not necessarily for a human entity. In fact, patents usually describe processes, machines or composition of matter which are useful for another process, machine or composition of matter. Therefore, we distinguish between stakeholders and users, considering the definitions given by the authors in (Bonaccorsi and D’amico 2017). Definition 1: Stakeholder : Stakeholders are entities on which the invention has or will have a positive or negative effect in order to show usefulness. This definition covers all possible entities that engage an active or passive relation with the invention.Given the logical condition of usefulness of patents, all patents must have stakeholder information. If a patent has not got any stakeholder information in it the patent application should be rejected. Definition 2: User: Users are animated or previously animated entities (human or animal, alive or dead), on which the invention has a positive or negative effect at an unspecified moment. Given definition 2, it is clear that every user is a stakeholder while non-users stakeholders include artifacts, machines, manufacturing or operational processes. Corollary 1: Multiple roles: Identities may have multiple roles as users. Our idea of users describes roles, not identities. Animated entities have an identity, as it happens for a specific person. A person has many roles as a user. For example, a working mother starts her day taking on the role of a mom, in which she is expected to feed her children and get them ready for school. At the office she shifts to the role of project manager, so she oversees projects in a timely and professional manner. Working mother, mom or project manager can be considered user roles attributed to the same person, or identity. From this definition it is clear that users are close to what social sciences define as “social roles”. Afterward, we can outline knowledge fields using the concept of user, with a twofold aim: help the reader to understand how the concept of users is interpreted in different knowledge fields; explain the background of the methodology we adopted. Social sciences: social roles as users In social sciences social roles are comparable with our definition of user. As defined in the psychological field (Dog 2015), “social roles refer to the expectations, responsibilities and behaviors we adopt in certain situations.”. The example of the working mother shown before, is the case of social roles. The field of social sciences is the only one in which an attempt of automatic extraction of users has been done. In (Beller et al. 2014) the authors extracted social roles from Twitter using heuristic methods. The authors looked for all the words preceded by constructions like “I’m a” and similar variations. This search resulted in 63.858 unique roles identified, 44.260 of which appeared only once. The result of the extraction process is noisy and only a low percentage of the extracted words are social roles. Despite of this noisy extraction, some entities are consistent with our definition of user, e.g. doctor, teacher, mother or christian. Another work (Beller, Harman, and Van Durme 2014) tries to identify social roles on Twitter exploiting a set of assumptions. The authors take into account roles, each one with a set of related verbs: if someone uses verbs from a set, that person may cover that particular social role. To sanitize the collection of positively identified users, the authors crowd-sourced a manual verification procedure, using the Mechanical Turk platform (Kittur, Chi, and Suh 2008). Also here some interesting extractions are performed, obtaining users like artist, athlete, blogger, cheerleader, christian, DJ, or filmmaker. These two works differ from the present study for what concerns the analyzed texts and the methods to extract the entities. Nevertheless, the extracted set of entities is consistent with our definition of user. Human Resources Management: workers as users In organizations, Human Resources Management is the function designed to maximize employees performance (Johnson 2009). Employees are key actors and they can be considered users according to our definition. Human Resources Management has tried to classify employees, especially in sub-fields like insurance, social security or work psychology. Usually, we refer to those as lists of jobs. Classifications were made with the goal of grouping similar jobs for educational requirements, job outlooks, salary ranges or work environments to facilitate social analysis and the placement of new workers. Such lists are relevant because, even if they represent just one subset of all the possible users, they contain valid information. Many institutions developed lists of jobs (“Suffix Codes for Jobs Defined in the Dictionary of Occupational Titles, Third Edition” 1967). Medicine: patients as users Another field of interest is medicine, since patients can be considered users. Also in this case there are many lists of patients, illnesses and diseases (Health and Services 2018), which are valuable in terms of information contained. Design and Marketing: between users and customers In the field of Design the concept of user plays a central role and it overlaps with our definition of user. Many tools and theories like “User Centered Design” are based on the concept of user (ISO 1999). As stated by the authors in (K. 2008), the quality of the design process is proportional to the user needs’ satisfaction. It implies that a designer has to understand the user needs; as a consequence he has to discover whom are potential users. 7.1.1 Method In this section we show the approach used to extract the users of the invention described in a patent. The proposed process is shown in figure 7.1 and its phases are: Generation of an input list of users: search all possible sources with the aim of creating an input list of users with the largest possible coverage (section ??); Patent set selection: select the set of documents from which extract the users (section ??); Patent text pre-processing: application of natural language processing tools on the documents with the aim of preparing them for the automatic user extraction; Automatic patent set annotation 1: projection of the input list of users on the text to generate the Automatically Annotated Patent Set 1; Relevant sentences extraction: selection of sentences containing at least one user to generate an informative training set; Automatic patent set annotation 2: generation of a statistical model by a machine learning algorithm based on the training set sentences and automatically tagging the patent set to generate the Automatically Annotated Patent Set 2; Difference computation: generation of the new list of users by computing the difference between the lists of users found in the automatically annotated patent set 1 and 2; Manual review: manual selection of the entities that, in the new list of users, are effectively users. This new list will enrich the original list of users. This phase is described in section ??. Figure 7.1: Process flow diagram of the proposed automatic user extraction system from patents. The diagram contains the representation of the documents and the operations performed on them. The process takes in input a patent set and a list of users and produces a list of new users as output. Before the description of each phases, in section ?? the concept of user of the invetion is explained by giving a definition of users and presenting the way that this concept is exploited in different knowledge fields. List of users generation To generate the input list of users, we used two different approaches: a bottom-up approach and a top-down approach. The bottom-up approach is based on the merge of lists from heterogeneous sources. In the present work we used the following lists of entities: Lists of jobs :(“Suffix Codes for Jobs Defined in the Dictionary of Occupational Titles, Third Edition” 1967), 11.142 entities Lists of sports and hobbies2: 9.660 entities; List of animals:3 600 entities; Lists of patients:4 14.609 users; List of generic words: manually generated. It contains users with a higher level of abstraction (such as person or human being), 56 items. Bottom-up approach produced a list of 35.767 entries. Afterwards, a top-down approach was applied. Starting from the list generated with the bottom-up approach, we looked for alternative methods to indicate a user, finding defined word patterns. The most relevant are: Patterns like “hobby_term + practitioner” for the hobbies; Patterns like “person who has + disease_term” or “suffering from + disease_term” for the diseases; Patterns like “practitioner of + sport_term” for sports. Top-down approach generated a total of 41.090 entries. The whole process generated a total of 76.857 users and gave us a reasonable number of terms to be used in the next step of the process. Obviously our lists have a limited coverage and, therefore, they do not contain all variations of a certain user. For instance, the lists miss some users belonging to the classes mentioned above (e.g. new jobs emerged in the last years) and all the alternative ways for referring to a user we do not spotted in the top-down approach. For example our lists miss jobs like data analyst, lap dancer, undertaker, mortician and thief or patients with emerging diseases like work-alcoholic and web-addicted. In addition, our lists miss a class of users related to religious groups, containing users like christians or jewish. Such terms have intentionally not been introduced in the input list because we considered these terms as candidates to be extracted by the process in our case study . Patent set selection Our choice of patent sets aimed at challenging our system to find new users missing in the input list. To reproduce a patent set selection, we took into consideration the International Patent Classification (IPC) (Organization 1971). IPC is a hierarchical system of patent classes representing different areas of technology. Then, we wondered which classes could contain new users according to our seed list. Furthermore, IPC class A, which is the first level in IPC differentiation, is based on human necessities. For this reason, we assumed that in this class we would have found likely users from patents texts. Patent text analysis Our Entity Extraction system is composed by a set of sequential phases. The first three phases are related to the linguistic annotation: sentence splitting and tokenization, part of speech tagging and lemmatization. Then, the patent set is analyzed by the entity extractor, specialized for users extraction. A more detailed description of each phase is: Sentence splitting and Tokenization: These processes split the text into sentences and then segment each sentence in orthographic units called tokens. In our system, sentence splitting plays a key role since thanks to a given word, it is possible to find sentences where the word is used. Finding correct boundaries for a specific word allows to dramatically reduce the space to retrieve its surrounding contexts. POS tagging and Lemmatization: The Part-Of-Speech tagging (or POS tagging) is the process of assigning unambiguous grammatical categories to words in context. It plays a key role in NLP and in many language technology systems. For the present application we used the most recent version of the Felice-POS-tagger described in (Dell’Orletta 2009). Once the computation of the POS-tagged text is completed, the text is lemmatized according to the result of this analysis. Semi-automatic Users Annotation: The Users Extraction tool is based on supervised methods. Such methods require an entity annotated corpus in order to extract new entities from unseen documents. A semi-automatic method has been used to generate an annotated corpus of users to avoid manual annotation of a patent set. The method is a projection of the list of users on the patent set defined in section @ref{patsel}. The list of users described in section @ref{sourc} is cleaned to avoid linguistic ambiguities when projecting these entities on the corpus. For example, the term “guide” has two different meanings when used as a verb or as a noun. Furthermore, as a noun it could indicate a component of a system (guide for mechanical parts) or a person (someone employed to conduct others) and therefore a user. Avoiding ambiguities is a crucial aspect to produce an informative training set, so ambiguous words were pruned. The entity annotation schema for a single token is defined using a widely accepted BIO annotation scheme : B-USE: the token is the beginning of an entity representing an User; I-USE: the token is the continuation of a sequence of tokens representing an User; O: for all the other cases. User Entity Extraction The Users extraction problem is tackled by the implementation of a supervised classifier that is trained on an annotated patent set. Thus, the patent set is linguistically-annotated, using the steps described above and entity-annotated, exploiting the semiautomatic annotation process executed in the previous steps. Given a set of features the classifier trains a statistical model using the feature statistics extracted from the corpus. For each new document the trained model assigns to each word the probability of belonging to one of the classes previously defined (B-USE, I-USE, O). In our experiments the classifier has been trained using two different learning algorithms: Support Vector Machines (SVM) using the LIBSVM library (Hearst, Dumais, Osman, et al. 1998) configured to use a linear kernel and Multi Layer Perceptron (MLP) implemented using the Keras library (Chollet 2015). It has been proven that LSTM methods are well suited for similar NER task. Anyway, we chose SVM and MLP method to study how two wheel established state of the art classifiers perform on the specific task of user extraction from patents and to evaluate their performance in terms of precision and computational effort. We also think that the popularity of these methods increment the reproducibility of the work. The classifier uses different kind of features extracted from the text: linguistic features, i.e. lemma, Part-Of-Speech, prefix and suffix of the analyzed token; contextual features, the linguistic characteristics of the context words of the analyzed token; in addition the entity category of the previous token is considered; compositional features, combinations of contextual features and linguistic features. i.e. Part-Of-Speech of the previous word and the lemma of the current word. These extra features allow to infer statistics on the interaction of the combined features that can not be captured by a linear SVM model. word2vec features: vector representations of words computed by the word2vec (Mikolov et al. 2013) tool. Word2vec is a NLP tool able to produce word representations exploiting big corpora. The main property of the vectors produced by word2vec is that words sharing similar contexts have similar vector representations. By using word vectors instead of the corresponding words we were able overcome the problem of the limited lexical knowledge in the training phase. Using these features and excluding all the others (delexicalized model) we expected that the resulting user extraction system had a lower precision and an higher recall in the classification phase. We presumed to find new users not contained in the input seed list. Manual Review of the new list of users It is still possible that the classification process creates false positive results (words labeled as users that do not match the definition in section @ref{theuse}). Thus, it is necessary to make a manual review of the extracted entities with the aim of evaluating the output. 7.1.2 Results The following section describes the performances of the automatic users extraction process on two different patent sets. To test the system four experiments were conducted}. Finally the performances and the outcomes of the system are shown and discussed. Following the guidelines for the patent set selection described in section , we examined two patent sets belonging to the IPC class A: A47G33. The IPC definition of the subclass is “religious or ritual equipment in dwelling or for general”. A61G1-A61G13. The IPC definition of the subclass A61G1 is “Stretchers” while the definition of the subclass A61G13 is “Operating tables; Auxiliary appliances therefor”. We extracted from the private Errequadro s.r.l.5 database a random sample of 2.000 patents from each IPC class. For each patent set we applied the semiautomatic set annotation process by projecting the input list of users on the morphosyntactically analyzed patent set. After this process, each semi-automatically annotated patent set was split in two parts: the first was used as training set for the user extractor, and the second one was used as test set. To build an informative training set, from the semi-automatically patent set we selected a subset of sentences containing at least one user. The size of the training set in both cases is approximately composed by 600.000 tokens. For each patent set table 7.1 shows the number of sentences of the training set, the number of sentences of the test set, and the number of distinct users in the training set (re-projected by the semi-automatic annotation process). ref —&gt; patent set-details Table 7.1: Statistics related to the patent set groups analyzed in the case study patent set group #Sentences - training #Sentences - test #Distinct users projected on training A47G33 13.364 214.029 126 A61G1-A61G13 15.108 2.520.350 121 We chose two orders of magnitude for the sentences test-set to test the efficiency of multiple configurations of the system. To test the performances of the implemented user extractor, we devised four different configurations. Each configuration uses a specific learning algorithm and a set of features to build the statistical model. The main purpose of this procedure is to find the configurations that better perform in the user extraction task. In addition, the different behaviour of the system in the classification phase is studied. In table 7.2 are reported the detailed configurations used in our experiments. Table 7.2: Context windows of the extracted features considering 0 as the current analyzed token. Feature group Context Window Lemma unigrams \\([-2, -1, 0, 1]\\) Lemma bigrams \\([(-1 ,0), (0, 1)]\\) Word bigrams \\([(-1 ,0), (-2, -1), (0, 1), (1, 2)]\\) Word trigrams \\([(1, 0, 1) (-2, 1, 0)]\\) Pos unigrams \\([-2, -1, 0, 1]\\) Pos bigrams \\(([(-2, -1) (-1, 0), (0,1)])\\) Compositional feature #1 \\((POS_{-1}, Lemma_{0})\\) Compositional feature #2 \\((Lemma_{-1}, Lemma_{0})\\) Compositional feature #3 \\((Lemma_{0}, Lemma_{1})\\) Compositional feature #4 \\((POS_{0}, Lemma_{1})\\) Compositional feature #5 \\((NER_{-1}, Lemma_{0})\\) Word2vec \\[-2, -1, 0, 1, 2\\] By using the first and the second configuration we expected to have a higher precision in the classification phase, since explicit lexical information is used in the training phase. For the same reason we expected to have low recall in classification phase. On the other hand, the third and fourth configurations are delexicalized: lexical information is provided by word vectors computed by word2vec_. In these two configurations we expected to have an higher recall and a lower precision, due to the characteristics of the computed vectors explained before. To limit errors when using the word2vec features, some linguistically motivated filtering rules were introduced. Specifically, sequences of tokens classified as users were constrained from the following categories: verbs, adjectives not preceded by articles, articles and adverbs. To evaluate the whole user extraction process in each experiment, we defined some evaluation measures. Each measure was introduced to evaluate the characteristics of the extraction system concerning the configuration applied. These measures are: Training time: time needed to create the statistical model using the training set; Test time: time needed to re-annotate the semi-automatically annotated patent set; Number of extracted users: number of unique entities classified as user in the automatically annotated patent set; Number of known users: number of distinct extracted users in the automatically annotated patent set and belonging to the list of user in input; Number of new users: number of distinct entities classified as user in the automatically annotated patent set and not belonging to the input list of users; Number of new correct users: number of distinct entities considered as user and as correct after a manual review; Precision: ratio between the number of new distinct correct users and the total number of new distinct users; Gain: ratio between the number of new distinct correct users and the number of re-projected distinct users on the training set. Table 7.3 reports the values of the defined metrics across all the experiments run on the two patent sets. Table 7.3: Comparison of the values of the defined metrics across all the experiments. The patent set annotation in the experiment (6) was not performed due to the computational costs. All the experiments were run on a machine provided with 10 AMD Opteron(tm) 6376 processors. Experiment Training time Test Time Extracted Known New New correct New wrong Prec. (%) Gain (%) 1 (SVM) 83m 321m 161 93 68 47 21 69.11 37.30 2 (MLP) 1911m 9091m 196 55 141 27 114 19.15 21.42 3 (MLP-W2V) 165m 246m 162 35 127 45 82 35.43 35.71 4 (SVM-W2V) 1265m 4310m 121 29 92 45 47 48.91 35.71 5 (SVM) 148m 3443m 302 120 182 88 108 48.35 72.72 6 (MLP) 1818m — — — — — — — — 7 (MLP-W2V) 333m 3530m 305 38 267 44 230 16.48 36.36 8 (SVM-W2V) 1268m 47020m 313 49 264 74 197 28.03 61.15 For what concerns training and test time of the automatic patent set annotation, it’s clear that the configuration based on the SVM learning algorithm without the word2vec features performs better in both the experiments (1, 5). When the features based on word2vec are introduced, the configuration based on the MLP learning algorithm is the fastest both in training and test time (3, 6): it is due to the fact that keras implementation of this algorithm exploits all the available CPU cores of the system. On the other side, the MLP algorithm does not scale properly with a higher number of features, as seen in training and annotation time in the experiment (2). In addition, we could not perform the patent set annotation in the experiment (6), since it would have required more than 60 machine days to complete the process. When word2vec features are introduced, the patent set annotation based on the SVM algorithm is 10 times slower than the MLP algorithm. For what concerns the precision in the automatic patent set annotation, the SVM configuration without word2vec features is clearly the more reliable: the precision values are from 1.5 to 2 times higher in the experiments (1, 5) in contrast to the other experiments. The higher precision is justified by the fact that the configurations based on word2vec features lack explicit lexical information: words with very similar contexts are represented by similar word2vec vectors, probably leading to errors in the classification phase. On the other hand, the use of word2vec vectors aims at extracting entities that would not be extracted by considering explicit lexical information only. Finally, for what concerns information gain, the same amount of new information (21-37%) is extracted in the experiments on the A47G33 patent set. The gain values drastically change in the experiments on the A61G1-A61G13 patent set: in the experiments (5, 8) a gain between 61% and 72% is obtained: it is due to the size of this patent set in comparison to the A47G33 one. In the experiment (7), despite the introduction of word2vec features, a gain of 36% is obtained. This fact, in conjunction with the non-feasibility of the experimental configuration 6, shows how MLP systems lack in efficacy and efficiency (in entity extraction in patent domain) when the test-set has an order of magnitude of millions of sentences. We think that this result is relevant, based on our experience with practical applications. Furthremore, a way to maximize the overall informative gain is to merge the results of all manually reviewed user extractions obtained by executing the patent set annotation process with all possible configurations. The overall informative gain of the merging process is related to intersections that occur among the results obtained by the patent set annotation process in each configuration: the less the intersections, the more the overall informative gain obtained. In table 7.4 is shown the overall gain obtained by merging results of the manually reviewed extractions in each patent set. Table 7.4: Gain obtained by merging correct entities extracted from each patent set annotation. Configuration A47G33 - Gain (%) A61G1+A61G11 - Gain (%) SVM 37.30 72.72 MLP 21.42 — MLP-W2V 35.71 36.36 SVM-W2V 35.71 61.15 SVM - MLP 52.38 — SVM - MLP-W2V 69.84 126.44 SVM - SVM-W2V 73.01 103.30 MLP - MLP-W2V 55.55 — MLP - SVM-W2V 57.14 — MLP-W2V - SVM-W2V 59.52 76.30 SVM - SVM-W2V - MLP-W2V 90.47 140.49 SVM - MLP - MLP-W2V 82.53 — SVM - MLP - SVM-W2V 85.71 — MLP - SVM-W2V - MLP-W2V 77.77 — SVM - MLP - SVM-W2V - MLP-W2V 103.17 — The table shows that the merging process of manually reviewed entities extracted from each patent set annotation run effectively contributes to increase the overall informative gain. For instance in the A47G33 patent set an overall gain of 103.17% is obtained, tripling the best result achieved by the extraction performed using the best single configuration. Good results are also achieved in the A47G33 patent set user extraction. In this case an overall gain of 140.49% is obtained, doubling the best result achieved by the extraction performed using the best single configuration. The results shown in section 5 prove that if the goal of the extraction is to reach the maximal recall, an ensemble method (combining the output of multiple classifier) over-performs every single classifier method. Anyway, the ensemble approach has clear efficiency issues, because the time of analysis will be the sum of every single approach time (in hypotheses of non-parallelization). This leads to a trade off between the speed of the system and the quality of the results, and whoever would use the presented system can decide to gain benefit in one or in another direction. Finally, tables 7.5 and 7.6 show an overview of extracted users randomly chosen from the A47G33 patent set (the only one in which were able to perform all experiments). Each table is divided in two blocks, representing the results of the extraction performed using a specific configuration. For each extracted user is shown the corresponding lemma (the root form), the frequency (how many times that user appears in the whole corpus) and the total number of patents containing the user. Users not contained in the starting user list, are highlighted in bold. The table shows that the system was able to extract characteristic users of the patent set. The results are in fact not unexpected for the IPC class under analysis: this is an evidence of the correct performances of the proposed system. In other words, the results presented in the table show that it is possible to train a NER systems able to extract sparse and valuable information. Such users are the ones that an expert would manually extract but the NER system does it with an enormous saving in terms of time and efforts. Other remarkable results are: many newly extracted entities have very low frequency in the patent set: it shows that the developed system is able to extract rare entities. table 7.6 shows that configurations using word2vec features are able to find new users with a higher frequency in the patent set: it was an expected result, since the word2vec configurations are not explicitly lexicalized and more able to generalize during extraction phase. The system is able to extract single words and multi-words. Taking into consideration the definition of user of an invention, the system extracts unusual and sometimes borderline users. Examples like saint, angel, god and ghost need discussion that is far beyond the purposes of the present paper. These results are a remarkable evidence of the human-like generalization ability of the described method. Table 7.5: Extracted users from the A47G33 patent set using the SVM and DL configurations. New users extracted by the system are reported in bold. Lemma Frequency # Patents Lemma Frequency # Patents female 801 109 child 402 102 child 426 108 cleregy member 128 5 guy 156 17 patient 113 11 patient 115 11 man 50 26 parent 70 31 young 48 32 man 51 26 angel 29 23 merchant 50 6 dog 20 7 soon 46 29 artisan 12 12 engineer 45 45 male/female 12 4 adult 39 23 hockey player 7 1 young 35 24 professional 7 7 society 32 21 tennis player 7 4 angel 29 23 football player 6 3 fund raiser 27 4 ghost 5 3 priest 22 4 children 5 5 cheerleader 15 4 manager 5 5 fund-raiser 11 4 spider 5 5 athlete 10 9 vandal 5 1 ghost 5 5 athlete 4 3 adulterant 3 3 mother 4 2 jew 3 3 soccer player 4 3 maid 3 1 squirrel 3 2 tourist 3 3 maid 3 1 indian 2 2 god 3 2 beginner 1 1 mariner 3 3 christians 1 1 male-female 2 2 datum entry operator 1 1 manufacturer 2 2 expert 1 1 jew 1 1 jewish 1 1 merchandizers 1 1 marinaro 1 1 parishioner 1 1 Table 7.6: Extracted users from the A47G33 patent set using the SVM-W2V and MLP-W2V configurations. New users extracted by the system are reported in bold. Lemma Frequency # Patents Lemma Frequency # Patents child 152 68 clergy member 124 5 clergy member 124 5 crowd 36 3 man 50 26 basketball player 20 5 engineer 45 45 him 17 8 young 29 24 woman 16 8 choir 17 1 saint 14 2 infirm 13 8 youth 14 2 bride 9 4 angel 8 4 volunteer 8 6 choir 8 1 musician 6 6 musician 6 6 boy 3 1 god 5 1 children 3 3 children 3 3 girl 3 2 guy 3 3 creature 2 1 infant 3 3 deceased 2 1 priest 3 3 jewish 2 2 bride 2 2 person 2 2 consumer 2 2 mother 2 2 everyone 2 2 audience 1 1 him/her 2 2 boyfriend 1 1 spectator 2 2 derby member 1 1 farmer 2 1 gift giver 1 1 youngster 2 2 handicapped 1 1 boyfriend 1 1 jesus 1 1 grandparent 1 1 saint 1 1 subject 1 1 husband 1 1 clown 1 1 lady 1 1 husband 1 1 runner 1 1 runner 1 1 society 1 1 society 1 1 teenager 1 1 tennis player 1 1 The total number of users is 109. 28,2% (564 on 2.000) of patents in analysis contains at least one user. This result is an evidence of the fact that patents actually contain users information, and, considering the approach we followed, this percentage is an accurate lower approximation of the actual percentage of patents containing at least one user. In figure 7.2 for each user on the x axes is shown the number of patents in which the user is contained. The distribution is skewed, with some occurrences showing large numbers and many others with just one or few occurrences. It is clear that there is a Pareto like distribution, with the first 20% of users covering 70% of total users in terms of occurrence. It means that some users are more likely to be cited in patents and many more users that rarely appear. Following this observations, we can divide users in three groups: Group A: users that appear in more than 100 patents (5% of the patent set). In our case these are male, child and female. Group B: users that appear in more than 20 patents (1% of the patent set). This group is composed by 13 different users. Some of these are engineer, person, player, adult, angel and _guy. Group C: users that appear in less then 20 patents. This group is composed by 93 different users. Some of these are mother, athlete, priest, adulterant, golfer and hockey player. Further research means to study how these users differ from patent set to patent set. We expect to see similar distribution but with different content of users. Frequent and non-specific users comprise Group A: in other patent set we could see differences in terms of entities contained in this class but its content will stay non-specific. These results seem to be generic social roles indicating the gender or the age of a person. Group B is composed of mainly non-specific users and some specific users that change from patent set to patent set. This class helps to identify the core users of the patent set. Lastly, Group C contains non-frequent users that are both specific and non-specific, making it the most interesting of the three for the purposes of our work. In this group we find users that are market niches, so the patent that contains these users is of great interest for marketers and designers. These are both samples of the more generic users (for example a mother is a female and a hockey player is a player) or specific users of the patent-set (like priest, fund-raiser, doll, spouse and clergy member. Figure 7.2: Process flow diagram of the proposed automatic user extraction system from patents. The diagram contains the representation of the documents and the operations performed on them. The process takes in input a patent set and a list of users and produces a list of new users as output. 7.2 Advantages and Drawbacks An effective development of new products or the redesign of an existing one require the analysis of its positive and negative properties. Due to that, advantages and drawbacks of products are extremely valuable information for companies. Unfortunately, this information is not easy to obtain and manage: a strong evidence of that is the effort in the development of tools able to manage this information (Pahl and Beitz 2013, Ulrich (2003)). Companies frequently make use of Quality Functional Deployment (QFD) and requisites lists, users’ needs, users’ requirements with the purpose of tracking advantages (Carnevalli and Miguel 2008). On the other hand, companies use Failure mode and effects analysis (FMEA) to gather and study drawbacks, failure modes and their effects and causes (Liu, Liu, and Liu 2013.) However, product developers can acquire QFD and FMEA data only from the users of the invention: this leads to the need of expensive processes of customer’s voice listening whose results are often unclear. Moreover, this information is not disclosed to researchers since this is part of the company know-how. The description of the brought advantages and solved drawbacks are critical requirements for the patentability of a product, as stated by the politics and the guidelines given by World Intellectual Property Organization (WIPO) on writing patents (Organization 2004). As stated by WIPO, an invention is a solution to a specific problem . The problem that an invention solves is a negative effect that state-of-the-art technologies can not fully overcome; on the other side, a solution is a way to solve this problem. A solution can lead to some advantages with respect to the known art. Thus, starting from the definition of invention, it is clear how it can be characterized by the advantages that it brings and by the problems that it solves. Also, it is reasonable to assume that having a clear picture of both advantages and drawbacks of a technology is important for an effective design process6 7.2.1 Methodology In this section we show the approach used to extract the advantages and the drawbacks of the invention described in a patent. More precisely, the system identifies textual elements which represent the advantages or the drawbacks described in a patent. Advantages and drawbacks information can strongly advantage designers in the phase of new product development or in the process of product improvement and marketers in the phase of customer understanding and product placing. Furthermore a patent based method has a strong advantage with respected to methods that extracts information using other type of documents (e.g. online reviews of products (Mirtalaie et al. 2018a)): patents anticipate availability of products on the market by a factor varying from 6 to 18 months (Golzio 2012). The proposed extraction process is shown in figure 7.3 and its macro-phases are: Advantage and Drawback Clues Collection: in this section we described the method to collect a reasonable number of generic advantage and drawback clues; Domain Clues Extraction: in this section is shown how the generic clues are used in exploiting machine learning algorithm to extract new domain specific advantages and drawback clues; Domain Clues Validation : since the new clues are automatically extracted, the output of the extraction phase surely contains a certain degree of noise. To clean this output a validation tool based on tweeter sentiment analysis is developed; Advantages and Drawbacks Extraction: here the extracted clues (generic and domain specific) are expanded according to specific regular expression pattern in order to obtain the advantages and the drawbacks of the analyzed patent set. Figure 7.3: Main overview of the advantages and drawbacks extraction process from patents. Clues of Advantages and drawbacks in patents First of all we have to define the concept of clue to an advantage or a drawback. To describe with a certain degree of precision an advantage or a drawback, patent writers need to use sequences of words of a certain length. Since NER systems do not perform well on long sequence of tokens, we split the problem of extracting advantages and drawbacks in two parts: first we extract entities that are clues in the sequence of words that describes the advantage or the drawback; then we extract the surrounding words to collect the whole sequence that describes the advantage or the drawback. To better understand these concepts some examples are: ease of access cook food quickly and economically benefits of keeping an outdoor cooker lid fixed For the present work, we refer to advantages and drawbacks as a sequence of words of minimal length that express the advantage or the drawback. The three phrases of the example are three advantages. On the other hand clues are words that are likely to be contained in advantages or drawbacks phrases. Advantages and Drawbacks Clue Collection Figure 7.4: Main overview of the patent set annotation process. The approaches to generate a knowledge base of clues were two. The first approach was based on a manual collection of clues of advantages and drawbacks directly from patent texts. This process was performed on 2,000 patents, randomly chosen from the freepatent database.7 With this approach we were able to collect 3,254 advantages and 5,142 drawback clues. Some examples of the extracted clues are shown in table 7.7. Table 7.7: Examples of the clues collected with the first approach. Advantages Clues Drawbacks Clues ability aggravated efficacy breakage ensure damage healthy defect innovative error optimum improper protect leak quick-release problem reinforce unavailable securely wrong The second approach consisted in looking for alternative methods to indicate advantages or drawbacks clues, looking defined word patterns. The most relevant are the negations of advantages to obtain drawbacks, and the negation of drawbacks to obtain advantages. Some examples of such constructions are shown in table 7.8. Table 7.8: Examples of the clues collected with the second approach. Advantages Clues Drawbacks Clues non damaged out of anti-corrosion in need of comfort loss reduction non user-friendly prevent diminish comfort defect free issue with reduce waste problem with reduction of lacks of cost less lacks with less severe loss of avoid disease unfacilitate At the end of this process, a total of 6.568 advantages and the 14.809 drawbacks formed the knowledge base for the system, and gave us a reasonable number of clues to be used in the next step of the process. The first approach was restricted by the lists being extracted from a random and limited sample of patents. On the other side, the rules used in the second approach are non exhaustive, and this can create non-sense clues, due to all of the possible combinations of words. Anyway, it is reasonable to assume that a large set of non-domain dependent clues are collected and will be used in the next steps of the process. It is important to underline that the list of advantages and drawbacks clues is designed to avoid linguistic ambiguities when projecting these entities on the corpus. For example guide has two different meanings8 when used as a verb or as a noun: as a verb it means “to assist something or someone to travel through, or reach a destination in, an unfamiliar area, as by accompanying or giving directions”, so it could be the clue to an advantage; at the same time as a noun it assumes the mean of “a book, pamphlet, etc., giving information, instructions, or advice; handbook” thus indicating a product and not an advantage. Avoiding such ambiguities is a crucial aspect to produce an informative training set, so ambiguous words were avoided to be projected on patents. Domain Clues Extraction The domain-specific clues collection process takes in input the automatically annotated patent set 1. The analyzed patent set is automatically annotated using the domain-independent clues, and is used to extract new domain-specific clues. We decided for the present methods to analyze the whole text of the patents and not to focus only on a specific section. Figure 7.5: Overview of the domain specific advantages and failures clues extraction process. Our system resorts to state-of-the-art NLP tools which are part of the linguistic analysis pipeline shown in figure 7.5. In addition we developed a specific advantages and drawbacks clues extraction tool, still based on Natural Language Processing techniques. The automatic patent set annotation 2 process, as shown in figure 7.5, is composed by a set of sequential steps. The first three steps are related to the linguistic annotation: sentence splitting and tokenization, part of speech tagging and lemmatization. Once these three steps are completed the entity extractor collects the advantages and drawbacks clues from the analyzed patents. Sentence splitting and Tokenization steps split the text into sentences and then segment each sentence in orthographic units called tokens. The Part-Of-Speech tagging (or POS tagging) step assigns unambiguous grammatical categories to the tokens. For the present application we use the most recent version of the Felice-POS-tagger described in (Dell’Orletta 2009). Once the computation of the POS-tagged text is completed, the text is automatically lemmatized in order to group inflected forms of a word in a single item. Some of the following steps of the entire extraction process exploit the lemmatized texts in order to achieve better extraction results. Successively the semi-automatic annotation of advantages and drawbacks clues is performed. The advantages and drawbacks clues extraction tool is the key ingredient of the present paper, and it is based on supervised methods. Such methods require an entity annotated corpus in order to extract new entities from unseen documents. Since the manual annotation of a patent set is too expensive both in terms of time and manual effort, we apply a semi-automatic method to generate an advantage and drawback annotated corpus. The entity annotation schema for a single token is defined using a widely accepted BIO annotation scheme (Ramshaw and Marcus 1999): B-ADV: the token is the start of an entity representing an advantage clue; I-ADV: the token is the continuation of a sequence of tokens representing an advantage clue; B-DRW: the token is the start of an entity representing a drawback clue; I-DRW: the token is the continuation of a sequence of tokens representing a drawback clue; O: for the remaining case. The Advantages and Drawbacks Clues Extractor is a supervised classifier that, given an annotated patent set, is trained on these examples. The patent set is: (a) linguistically-annotated, using the steps described above; (b) entity-annotated, exploiting the semiautomatic annotation process executed in the previous steps. Given a set of features the classifier trains a statistical model using the feature statistics extracted from the corpus. This trained model is then employed in the classification of unseen patents: it extracts new domain specific clues from patents and assigns them a probability score whether they are an advantage or a drawback. In our experiments the classifier has been trained using the Support Vector Machines (SVM) learning algorithm using the LIBSVM (Hearst, Dumais, Osman, et al. 1998) library configured to use a linear kernel. The classifier uses two different kinds of features that are extracted from the text: raw features: prefix and suffix of the analyzed token; it works particularly well with advantages ending with -full -ious and with drawbacks starting with un- dis- etc.. word2vec features: vector representations of words computed by the word2vec (Mikolov et al. 2013) tool. Table 7.9 reports the detailed features chosen for the proposed advantage and drawbacks clues extractor. Table 7.9: Context windows of the extracted features considering 0 as the current analyzed token. Feature group Context Window Prefixes up to 4 \\[0\\] Suffixes up to 4 \\[0\\] Word2vec \\[-2, -1, 0, 1, 2\\] TAG \\[-1\\] By introducing prefixes and suffixes of the analyzed token, the classifier is able to identify frequent orthographic patterns which allow to maximize the precision in classification phase. On the other hand, the word2vec features are introduced in order to maximize the recall, since semantically similar clues should have similar word2vec vectors. Finally, the tag of the previous token is added to the final feature vector in order to improve the accuracy classification of multi-word clues. Word2vec feature computation While contextual, linguistic and compositional features are commonly used for entity extraction task in patents, from a computational linguistic point of view the presented system introduces the novelty of using word2vec features for entity extraction in patents. Word2vec is a NLP tool able to produce word representations exploiting big corpora. The main property of the vectors produced by word2vec is that words that share similar contexts have similar vector representations. By using word vectors instead of the corresponding words we were able to overcome the problem of the limited lexical knowledge in the training phase. To build our word2vec vectors we used the Skipgram model with a context window of 5 tokens. As reported in table 7.10, we used a corpus consisting of 48,194 different patents, containing more than 400,000,000 tokens. The corpus was designed to contain patents belonging to different classes (12 in total) in order to acquire an extended knowledge of the contexts in which the words in general are surrounded. In addition, patents belonging to two of these classes are analyzed and, in the same section, detailed configurations of the entity extractor has been provided. Table 7.10: Statistics of the documents on which the word2vec vectors have been learned. The patent sets of the analyzed case study are reported in bold. Patent class # Patents # Tokens A47G33 2423 5.225.000 A61G13 2991 15.937.000 A61G1 5040 36.348.000 A61H 5199 41.831.000 A61P25/24 5297 103.098.000 A63F1 5461 75.900.000 A63F3 4923 40.909.000 A63F7 4747 13.807.000 E02B3 3796 14.434.000 E04H9 2221 12.500.000 G01V11 1345 11.166.000 G08B13 4831 40.904.000 Total 48194 412.065.000 Clue validation using tweets sentiment analysis Figure 7.6: Overview of the domain specific advantages and failures clue validation process. Figure 7.6 gives an overview of the activities performed to validate the collected clues using twtitter. Since the manual review of the new domain specific clues can be very time consuming, an innovative approach to automatic validation of these entities is proposed. The approach is based on the assumption that advantages of technological innovations can be considered positive factors by the users. Conversely, the drawbacks of the artifacts are considered negative factors impacting on the satisfaction of the users. Both advantages and drawbacks are common terms or chunks of terms commonly used in other contexts, too. Therefore, if we can identify a wide source of sentences tagged with a polarity score and containing advantages or drawbacks, the probability of assigning the proper polarity to advantages and drawbacks increases. Social media platforms provide powerful venues for consumers to interact not only with brands but also with other consumers as they engage in the processes of curation, creation, and collaboration (Evans, Bratton, and McKee 2010). Such virtual platforms are places where users discuss about products, about their features but also about problems and failures they experienced during the daily use. The way they discuss or describe products or services is often unambiguous and highly polarized. Our approach to the automatic validation of advantages and drawbacks exploits the information contained in the Twitter platform.9 More precisely, for each extracted advantage or drawback clue we collect a set of tweets in which the clue is mentioned. Once a significant number of tweets is collected (in our case 3,073,959 - around 2,738 per entity in average), they are analyzed by a sentiment classifier. The main idea behind this process is to assign to each clue a sentiment polarity score which should express the feeling of the user with respect to the considered clue on the social media. The tweet collection can be easily performed by using the Twitter streaming API,10 which is freely available. By assigning a polarity score to each clue, we expect to detect tagging anomalies: entities tagged as advantages by the classifier are expected to have a positive polarity in the extracted tweets. Vice versa, entities tagged as drawbacks by the classifier are expected to have a negative polarity in the extracted tweets. Sentiment Classifier: features, classification model and performance evaluation In our sentiment classifier we focused on a wide set of features ranging across different levels of linguistic description. The whole set of features we started with is described below, organized into four main categories: raw and lexical text features morpho-syntactic features syntactic features lexicon features. This proposed four-fold partition closely follows the different levels of linguistic analysis which is automatically carried out on the text being evaluated, (i.e. tokenization, lemmatization, morpho-syntactic tagging and dependency parsing) and the use of external lexical resources. Raw and lexical text features are extracted considering the text available in the tweet. For this work we considered a number of tokens, character n-grams, word n-grams, lemma n-grams, char repetition sequences, mentions number, hashtags number and punctuation. Morpho-syntactic and Syntactic Features consider the part of speech tags and the syntactic analysis of the text. Our sentiment analyzer extracts Part-Of-Speech n-grams (coarse and fine), coarse grained Part-Of-Speech distribution and syntactic dependency types n-grams. To extract features based on lexicons we exploited three freely available resources. The Bing Liu Lexicon (Hu and Liu 2004), which includes approximately 6,000 English words, the Multi–Perspective Question Answering Subjectivity Lexicon (Wilson, Wiebe, and Hoffmann 2005), which consists of approximately 8,200 English words and the SentiWordNet 3.0 Lexicon (Baccianella, Esuli, and Sebastiani 2010) which consists of more than 117,000 words. For each word in these lexicons the associated polarity is provided. In addition, we manually developed a lexicon of positive and negative emoticons, which usually is a strong indicator of tweets polarity. By exploiting the described resources, the following features were extracted: positive/negative emoticon distribution, sentiment polarity n-grams, sentiment polarity modifiers, the distribution of sentiment polarity, the most frequent sentiment polarity and changes of polarity in tweet sections. A more detailed description of these features is provided in [Cimino et al. (2014). In order to assign a sentiment polarity score to each tweet, we employed an adapted version of the ItaliaNLP Sentiment Polarity Classifier for the English language (Cimino et al. 2014). This classifier operates on morpho-syntactically tagged and dependency parsed texts and assigns to each document a score expressing its probability of belonging to a given polarity class. The highest score represents the most probable class. Given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus. This model is used in the classification of unseen documents. The set of features and the machine learning algorithm can be parametrized through a configuration file. For this work, we used a tandem Long Short Term Memory Recurrent Neural Network (LSTM) - Support Vector Machines (SVM) architecture. Validation of the extracted clues The sentiment classifier is employed to validate the advantage and drawback clues which were previously extracted from patents by the clue extractor. In order to do so, we exploited the output of the tweet classifier on the tweets we previously downloaded. Each tweet, as said before, contains one or more clues to be validated. The sentiment classifier assigns the likeliness to each tweet to positive, neutral or negative. Consequently by analyzing all the tweets we previously downloaded, we obtained for each clue the distribution of positive, neutral and negative tweets. Then to make a decision regarding each clue we used another SVM based classifier. This classifier is trained on a gold set of advantages and drawbacks clues: we manually labeled 344 words as advantages and 193 words as drawbacks, obtaining the gold set. The features used by this classifier are a number of positive, neutral and negative tweets extracted in which the entities are mentioned and the word2vec vector representing the considered entity. In table 7.11 the classification results of the proposed approach over a 5-fold cross validation are reported. The obtained results show that the proposed entity validation method is suitable for the automatic advantages/drawbacks clues validation process. Table 7.11: Classification results of the proposed validation method over a 5-fold validation. Method Global accuracy ADV Prec. ADV Rec. ADV F1 DRW Prec. DRW Rec. DRW F1 SVM-W2V 87.71 89.89 91.29 90.57 83.35 80.66 81.92 Advantages and Drawbacks sentences extraction Figure 7.7: Overview of the advantages and failures extraction process. The advantages and drawbacks extraction process is shown in figure 7.7. Once all the domain specific advantages and drawbacks clues are extracted, these are merged with the ones belonging to the original knowledge base, obtaining a final list which will be processed by the advantages and drawbacks sentences extractor. The advantages and drawbacks sentences extractor exploits predefined linguistic and clues filters which operate on the automatic pos–tagged patents. Specifically, for each advantage and drawback term identified in patents, we used a pos–clue-pattern constraining the start-token and the rest of the token pos. Since we were interested in phrases containing words belonging to specific morphological categories, we identified sequences of allowed pos–clue-pattern in order to cover most of the English morphosyntactic multi–words structures, using the following pattern: (ADVClue|textbar DISClue)+Noun.*Noun.*Noun. The pattern is applied to the previously lemmatized text in order to have less sparse and more informative extractions. This choice was made because the pattern: expresses an advantage or a drawback exhaustively; increases the precision and the recall of the final output list of advantages and drawbacks; allows to build a three-level named based tree over the final output list. In particular, the tree is built by grouping terms which share at the first level the same clue, at the second level the same noun and at the third level the same noun. This grouping procedure allows to easily represent the final output list in a tree structure which can be easily navigated by the end user of the system. 7.2.2 Results In this section we describe the experimental use of the proposed process by applying it on four different patent sets. To test the proposed methodology, we chose 4 patent sets composed of a sample of 3,000 patents each. The patent sets belong to 4 different IPC patent classes. The chosen classes and the definitions given by WIPO are reported in table 7.12. Table 7.12: The patent IPC classes from which samples of 3,000 patents were chosen for the experimental analysis. IPC name Definition A61G13 Operating tables and auxiliary appliances therefor A61H Physical therapy apparatus A61C15 Devices for cleaning between the teeth A47J37 Baking; Roasting; Grilling; Frying Our choice of patent sets aimed at challenging our system to find new domain specific advantages and drawback clues in different domains. Furthermore, we only selected patent sets from the IPC class A, which is based on human necessities, to maximize the probability of finding advantages and drawbacks that impacts on the users and not only on other products/components. Once the advantages and drawbacks are extracted, a manual review process was performed on the output of the system to compute the number of true positive clues. In this way we were able to compute the precision of the process for both the advantages and the drawbacks. The output of the clue extraction validated by the clue validator is analyzed in table 7.13. The table shows that the number of the extracted true positive advantage clues is higher than the number of the extracted true positive drawback clues. On the other side, the automatic evaluation process has a lower performance on advantage clues in terms of precision. A first hypotheses to explain these results is that our knowledge base contained more drawback clues than advantage clues. Another possible reason could be that the applicant is minded to describe the invention highlighting the positive effects of the invention. Table 7.13: Number of clues filtered with the clue validator and number of true positive clues. # Advantage clues # Drawbacks clues Tot extracted clues 3607 1244 Automatically Validated clues 1976 576 True Positive 984 448 Precision 49.8% 77.8% In order to assess the performance of the overall process, another important measure is the amount of new information that is obtained, which we call information gain. As shown in table 7.14, the percentage of new discovered clues decreases with the number of starting clues. Obviously, the more patent sets are analyzed, the less new generic and domain specific clues are extracted. The percentage of information gain (represented as delta in the table), stabilizes at a 5% value in the advantage clues case, and 1% in the drawback clues case. This trend could be an evidence that the clue extraction process has a natural saturation level. Table 7.14: Information gained by applying the extraction process on different patent sets. Each row reports the percentage of information gained by incrementally adding the extracted entities to the knowledge base and the overall number of entities belonging to the extended knowledge base. Patent set ∆ Adv. # Adv. ∆ Draw. # Draw. Knowledge Base N/A 6,568 N/A 14,809 A47J33 +23% 8,133 +3% 15,332 A61C15 +12% 9,178 +2% 15,644 A61G13 +5% 9,653 +1% 15,849 A61H +5% 10,175 +1% 16,053 Tables 7.15 and 7.16 show the frequencies of a randomly chosen set of the new extracted advantages and drawbacks domain specific clues for each of the four analyzed patent sets. The results show that the domain specific clues clearly characterize the different technical areas of the patent sets. It is thus interesting to notice how valuable information is contained in the context specific clues them self. Further research can decide to stop here the process, without extracting the whole sentence. Table 7.15: Extracted domain specific advantages clues with the measures of occurrences for each patent set. A47J37 (Baking) A61H (Therapy apparatus) A61C15 (Teeth cleaning) A61G13 (Operating tables) transport 295 regenerative 144 elasticity 495 rigidity 784 integrity 246 waterproof 101 rigidity 461 ventilation 177 rigidity 233 hygienic 85 disinfection 247 hygiene 135 insure 180 ergonomically 77 precision 199 versatility 121 adjusting 164 disinfection 48 ergonomically 108 reliably 113 unobstructed 73 prevent excessive 39 economically 105 disinfection 56 uniformity 64 hemodynamics 25 waterproof 81 humidification 48 sensitivity 44 prophylaxis 22 hygienically 33 ergonomically 39 hygienic 30 prevent slippage 21 quick-connect 26 sanitation 20 selectively 34 smoothly 15 sanitation 24 non-invasive 12 Table 7.16: Extracted domain specific drawbacks clues with the measures of occurrences for each patent set. A47J37 (Baking) A61H (Therapy apparatus) A61C15 (Teeth cleaning) A61G13 (Operating tables) accidental 61 infection 595 infection 446 syndrome 134 burnt 59 trauma 378 inconvenience 126 costly 72 malfunctioning 12 abrasion 106 irregularity 40 claustrophobia 38 time-consuming 8 fragmentation 37 pathogen 14 malfunctioning 36 non-compliant 6 paralysis 17 infected 8 unnecessarily 27 dirty 6 hematoma 15 unintentionally 6 discoloration 19 ignites 4 uncomfortably 8 abnormal 6 hyperglycemia 11 turbulence 4 undetectable 8 burn 4 unavoidable 10 cross-contamination 3 embarrassment 8 toxic 3 not linearly 8 violently 3 discoloration 8 erosive 3 catastrophic 6 Then, after the re-projection of the extracted clues on the text, the regular expression described in section is used to extract the sentences highlighted by the clues. The number of advantages and drawbacks sentences extracted from each patent are shown in table 7.17. The table shows that the occurrence of sentences describing an advantage is higher than the ones containing a drawback 7.14. This result may be due to the fact that the applicant is minded to describe the invention by highlighting the positive effects of the invention. Table 7.17: Number of sentences containing advantages and drawbacks for each analyzed patent set. Patent class # Advantage sentences # Drawbacks sentences A61G13 7,836 1,048 A61H 10,879 1,463 A61C15 9,551 1,572 A47J37 9,973 1,662 Total 38,239 5,745 Figure 7.8 and 7.9 show two subsets of the taxonomies obtained by the extraction of the advantages and drawbacks for each of the four analyzed patent sets. The two figures respectively refers to a subset of the leaves linked to the advantage clue Improve and a subset of the leaves linked to the drawback clue _Damage. In both cases an additional trimming action is performed by removing those branches or leaves containing terms belonging to the stop-word list typical of patent lexicon (e.g. claim, embodiment, invention, comprise, figure, etc..). The figure shows that our process can extract highly informative sentences also starting from generic and non-contextual clues like improve or damage. Moreover the words that follow the generic clues are specific of the technical field of the analyzed products. Both the results are promising for future applications, especially for the design fields. In particular figure 7.8 allows designers to focus on the positive side of the effects provided by the product and to better meet the explicit and implicit user needs. Similarly, figure 7.9 helps designers to redesign of the product in a proactive way, to keep attention to the critical issues identified by the drawbacks and to conceive possible corrective actions to solve such drawbacks. Figure 7.8: Sample of the tree based taxonomy extracted from the analyzed patent sets. The sample contains some of the leaves linked to the advantage clue Improve Figure 7.9: Sample of the tree based taxonomy extracted from the analyzed patent sets. The sample contains some of the leaves linked to the drawback clue Damage. 7.3 Trademakrs The market interest for both patents and trademarks has increased during the last decades, with a significant increase in filing for both types of Intellectual Property. The academia also took attention to patent data and trademark data, but with different and (almost) disconnected research approaches. Furthermore, the analysis of patents to study R&amp;D is predominant while less attention has been paid to trademarks (Griliches 1981). Indeed, very few are the works where patents and trademarks are investigated together and not generically cited together as parts of the intellectual property right framework. Moreover cultural differences exist between United States and Europe (at least) and practical consequences can be observed in daily life: for example, even if Europe had an earlier and more enduring interest in trademarking than the US the presence of the symbols ® and ™ in the product labels are more common in US than in Europe (MERCER, Lopes, and DUGUID 2010). The difference exists also from an industrial perspective, across different sectors (Baroncelli, Fink, and Smarzynska 2004). Indeed, trademarks have their larger use worldwide in the R&amp;D intensive scientific equipment, pharmaceuticals sector and advertising intensive manufacturing industries (clothing, footwear, detergents and food products). The present chapter studies the usage (if any) of trademarks and trade symbols within patent texts. The aim is to investigate possible indicators about IP strategies and innovation output utilizing information about the interlink between patents and trademarks. An additional goal is to understand if patent applicants makes a right use of trademarks and trade symbols when writing a patent document. The analysis clearly stands at the boundary between patent and trademark research areas, and since such intermediate territory has not been investigated in depth, there are several research questions we would like to answer. The starting point is a sheer numerical evidence: we have found out that, at the date of 01/09/2017, a total of 2.162.962 patents worldwide (for details about the database and its coverage see section 3) contain a trademark symbol. It is therefore a not negligible phenomenon, worth of investigation and that should provide interesting insights about the mechanisms and strategies that companies adopt at the frontier between technical innovation and marketing. The use of trademark symbols ™ and ® in patents from a legal perspective It is evident from our preliminary analysis that there exist an high number of patent documents containing a trademark indication. In the present section we investigate what is the legal perspective of this usage. In (Butler 1969) the authors published a series of rules to standardize the use of trade terms in patents. They also specifies different rules in the case of use of trade terms in specifications and the use of trade terms in patent claims. The rules are: 1- A trade term is properly used in a specification if those skilled in the art can make the product designated by the trade term at the time the application is filed, using the specification and/or published literature that is implicated by the specification. 2- A trade term is also properly used in a specification if the product is generally known to persons skilled in the art and is readily obtainable at the time the application is filed, provided the composition of the product is a trade secret and there is reason to believe that whenever the composition of the product is modified the trade term will also be changed. 3- A trade term is also properly used in a specification if it designates a component of the embodiment which is not essential to the invention. 4- A trade term can be used in a claim only if its meaning has been adequately defined in the specifications, whereby it imparts specific limitations to the claim.” The second rule is particularly interesting because it seems preventing early patenting or patenting before having produced and used a trade name. Rule number two is even more interesting since it contains a case of a product that is a trade secret used to build another product subject to patent application. Similar guidelines are contained in the European patent convention (Hall and Helmers 2018). Here is state that is not desirable to use trademarks or trade names if such words merely denote origin or where they may relate to a range of different products. Anyway trademarks could be used in patent application to satisfy art.83, that states that the application shall disclose the invention in a manner sufficiently clear and complete for it to be carried out by a person skilled in the art. In this case the product must be sufficiently identified, without reliance upon the word. A special case are such words that have become internationally accepted as standard descriptive terms and have acquired a precise meaning (e.g. “Bowden” cable, “Belleville” washer, “Panhard” rod, “caterpillar” belt). In this case they may be allowed without further identification of the product to which they relate. It is clear specifications afflicts the analysis that we want to carry out. Always in the same document are given also guidelines for the usage of trademarks in claims. These are, as expected, different from the specifications. For claims we have the problem that it may not be guaranteed that the product or feature referred to is not modified while maintaining its name during the term of the patent. They may be allowed exceptionally if their use is unavoidable and they are generally recognised as having a precise meaning. It is the applicant’s responsibility then to ensure that registered trademarks are acknowledged as such in the description. From that we can deduce that the presence of a trademark in patents and more specifically in claims decreases the reproducibility of the invention and thus the quality of the patent. Also the the US patent legislation (Jaffe 2000) focuses on the use of trademarks in patents claims. Here the presence of a trademark or trade name in a claim is not considered improper but the examiner should analyze the claim to determine how the mark or name is used. In fact, the trademark or trade name should identify a source of goods, and not the goods themselves. In this case the claim scope is uncertain since the trademark or trade name cannot be used properly to identify any particular material or product. In fact, the value of a trademark would be lost to the extent that it became descriptive of a product, rather than used as an identification of a source or origin of a product. Thus, the use of a trademark or trade name in a claim to identify or describe a material or product would not only render a claim indefinite, but would also constitute an improper use of the trademark or trade name. Finally, in (Pressman and Stim 2018) the authors gives some guidelines on how to introduce trademarks when it is unavoidable. First, the trademark should be capitalized and used as an adjective (not a noun), followed by the generic name of the product or service. Furthermore when referring to the trademark there should also be a reference to the trademark owner. 7.3.1 Methodology With respect to Users @ref(#usersresults) and to Advantages and Distadvantages @ref(#advdrwresults) the process to extract tradenames from patents is trivial. The main activity performed are (for details aobut the activities see section 5: Sentence splitting Tokenization Part-of-speech tagging Lemmatization After that is is possibile to identify tradenames searching for the symbols ™ and ®. 7.3.2 Results The first investigation has the aim of understanding if the ™ and ® symbols has different content depending on different IPC (Organization 1971) classes. The IPC classes are: The main evidence in figure 7.10 is that ® is used more than ™ in patent, with and average percent of presence of 7.4% and 5.6% respectively. The reason of this evidence could be that patents contains unregistered trademarks that are never converted to registered trademarks; another reason is the confusion between the ® and ™ symbol, usually considered as synonymous. Furthermore the writer will use the symbol only if he/she knows that the mark is protected, otherwise he/she will not indicate any symbols or will use ™ or ® without checking the right use. Another possible problem is that the word has become of common use and thus usually used without symbols. Furthemore, classes A (Human Necessities) and C (Chemistry; Metallurgy) clearly has more trade symbols than the other classes. Figure 7.10: Histogram of the percentage of ® and ™ in patentes for each IPC class (limited to US patents). A (Human Necessities); B (Performing Operations; Transporting); C (Chemistry; Metallurgy); D (Textiles; Paper); E (Fixed Constructions); F (Mechanical Engineering; Lighting; Heating; Weapons; Blasting); G (Physics); H (Electricity). Secondly, from table 7.18 it is evident that the average ratio is 6.4 for ® (the usage is 6.4 times higher in the year span 2007-2014 than the span 1995-2002) and 5.4 for ™ (the usage is 5.4 times higher in the year span 2007-2014 than the span 1995-2002) thus the usage of trade-symbols in patent is growing and the presence of ® is increasing faster than the presence of ™. From the present results is not possible to say if this effect is due to an higher quality of the process of patent application (and thus an higher awareness of applicant and examiner) or of a positive trend in trademark registration. In particular for the IPC class E (fields constructions) the number of ® and ™ has increased 10.5 and 9.0 times respectively. This could be an evidence of the fact that the number of trademark in the field of fixed constructions is increasing. Table 7.18: For each IPC class the ratio of the percentage of patents containing at least one ® or ™ character is computed for two patents sets 1995 to 2002 and from 2007 to 2014. IPC Classes Ratio ® 2007-2014 over ® 1995-2002 Ratio ™ 2007-2014 over ™ 1995-2002 A 5.9 5.0 B 6.2 5.4 C 5.9 5.2 D 5.6 5.7 E 10.5 9.0 F 7.5 6.2 G 4.6 3.2 H 5.1 3.2 Average 6.4 5.4 The Selected Tradenames We a set of popular tradenames referring to (Morris 2016). Then we filtered the ambigouus ones (the one that can refer to surnames), because this can create an onverstimation of the number of patent citing it witouth the tradenames. The total namber of tradenames is 38 and these are: amazon® blackberry® bose® budweiser® chiquita® chrome® coca-cola® ebay® facebook® fender® firefox® gibson® gillette® heineken® ibanez® intel® iphone® kellog® kodak® lego® marlboro® matlab® mcdonald’s® nitinol® nutella® nylon® photoshop® polaroid® post-it® powerpoint® prozac® sennheiser® spotify® teflon® velcro® whatsapp® xanax® In figure 7.11 are shown the number of patent containing the tradenames with and withouht ™ and ®; furthermore the information is divided for all the years, from 1995 to 2002 and from 2007 to 2014. In the previous analysis we noticed that ® is generally more used than ™. Now the goal is to understand if patent writers uses tradesymbols or not when citing tradenames, if there are any differences between different tradenames and if we can notice different behaviours in recent years. From figure 7.11 is evident that despite the fact that it is mandatory to use ™ o ®, these symbols are not always used. Figure 7.11: Number of patents citing one of the 38 selected tradenames. The data are divided for any years, from 1995 to 2002 and from 2007 to 2014. Tradenames Usage in Patents It is relevant to understand if there exists any difference between different tradenames in the correct usage of trade symbols (a trade name has been cited in the patent texts with the trade symbol). Figure 7.12 illustrates in a bi-logaritmic plot the distribution of trade names correctly cited in patents with ® and ™ (Y-axis) versus the same trade names cited without using the proper symbol. Most of trade names are located under the diagonal. Those around the diagonal are those we can considered as well known trademarks and not so subject to genericization phenomena (green area). Conversely on the right we can find a read area where the ratio is even more shifted toward the absence of any indication of protected trademark. In the middle the orange area where many of the most famous trademarks fall down and that could be in phase of generalisation or conversely, since the trademark of a product is totally entangled with the owner (Iphone-Apple; Intel), inventors do not feel the reason to cite them as a trademark. A remark on Figure 7.12 is necessary: the plot presents a correct measurement of trademarks with ® and ™ plotted in the y axis, while the search for trademark with missing trademark symbols in some case can introduce undesired errors and include other meaning of the word. Take for example the case of Blackberry®: the datum is referring to mobile phones and accessories or software applications. In all the cases where ® or ™ are correctly written, it is probably true that the inventor is referring to the Blackberry® mobile phones, but machines for jam or juice production extracted out of the blackberry introduce false positive examples. Similarly Fender is probably located too far on the right because it is also a part of a car, bike, motorbike, boat, etc.. and Gibson could introduce citations of works performed by someone called Gibson, therefore if cleaned results are necessary the searching strategies for the trademarks without symbols have to be refined. Figure 7.12: Plot of the trade names correctly cited in patents with ® and ™ on the y-axis and the same trade names cited without using the proper symbol on the x-axes. Both axes are on a logaritmic scale. The Popularity of Tradenames in Patens In figure 7.13 are plotted the trends of the numbers of patents that correctly cites a tradenames. We anlyzed 12 different tradenames, divided in couples: each couple belong to a similar sector. We make use of a generalized additive model fitting function to better analyze the results and to make more easily to understand and compare the two element of each pair. Each pair has been chosen to compare trademarks in homogeneous markets. Some of them shows: (d, e) similar trends but different incidence; (a, c) dissimilar behaviours (Amazon vs Ebay, Blackberry vs. Iphone: linear vs. exponential); similar results (e.g. Gibson vs Fender in the music market have almost the same behaviour and a reduced presence in patents) similar behaviour but shifted in time (e.g. Firefox vs Chrome). Figure 7.13: Plot of the trade names correctly cited in patents from 2000 to 2014. The Similarity of Tradenames in Patens Further informations can be derived by analysing the IPC classes where the patent have been chosen by the assignees-attorneys-examiners.The choice of a class is not only matter of market segment but rather of market use. Moreover it is necessary to remark that from a formal point of view the IPC classes form a feature vector, denoted y ∈ N639. IPC classes form a complete dataset of 639 elements of such feature vector. Each tradename is represented in such a feature vector where the number of patents in each class constitutes the length of the vector along that direction (feature). Our training data is therefore D = (x,y,z), where x∈[1, .., 12], y∈[1, .., 639], and z represents the patent numerosity z∈[0..15*106]. This dataset contains information about how z varies as a function of x and y. The matrix is quite empty since each product/brand addresses needs of specific market/s, after cleaning of the totally empty columns the matrix reduces to the dimensions of 12x479 and demonstrates a correct choice of the trademarks since they covered almost 75% of the IPC space. With such a dataset a series of computations can be performed: here we show the results of the correlation analysis among the chosen trademarks to analyse their relative positioning on the invention landscape (not only the market); Correlation analysis is shown in Figure 7.14 The matrix is triangular owing to the symmetry of the relationship. The main evidences are the following: Figure 7.14: Heat-map of the IPC-based similiraties between trademarks. References "],
["papers.html", "Chapter 8 Papers 8.1 Sustainable Manufacturing: an Analysis of the 6R Framework 8.2 Sustainable Manufacturing: An Extended Mapping 8.3 Blockchain 8.4 Precision Agriculture", " Chapter 8 Papers There are fields (mature technologies) where patents anticipate papers, while in others (basic research) the opposite happens. For this reason, scientific literature is the place where companies and scholars gather information about the problems that researchers are facing in the development of new technologies. Anyway, standard approaches of knowledge extraction from papers requires skilled personnel, they are time consuming and lacks in reproducibility. Furthemore, the volume of scientific literature has grown rapidly raising an imminent question about how to extract knowledge from this source. On the other hand, this tehcnical knowledge if managed, can be used to answer questions that 10 years ago would need the help of domain experts to be answered. In the present section is shown how text mining techniques can help to unswer some of this questions. In particular, it is an essential problem when different classification codes are used in order to organize scientific knowledge on a specific domain, becaues a specific categorization in a certain scientific field is missing. This leads to unnecessary complications in the researchers’ aims who want to quickly and easily find literature on a specific topic among the large amount of scientific publications, or want to effectivelly position a new research. Text mining techniques, and in particular topic modelling, can help scholars in solving this problems, if properly used togheter with domain expertise. A deeper description of what patents are and how these documents are used to mine technical knowledge can be found in section 6.2 In this section we present two methodologies capable of automatically segmenting a knowledge field. These selected knowledge field are: sustainable manufacturing and block-chain. The results of the methodologies are described, togheter with example of applications. 8.1 Sustainable Manufacturing: an Analysis of the 6R Framework One of the most important objectives of Sustainable Manufacturing (SM) is developing innovative and viable engineered materials, manufacturing processes and systems to provide multiple life-cycle of products. In SM the old concept “from cradle to grave” is now transforming into “from cradle to cradle” (Jawahir and Bradley 2016), tending toward multiple product life-cycles or even a “near-perpetual” product/material life. Scientific contributions in the sustainable manufacturing field mostly deals with energy and resource consumption. In this respect, two different main fields of causes can be identified: the process level and the material efficiency one. As a matter of facts, manufacturing processes have a significant role also in putting in place material efficiency strategies (Ingarao 2017). As far as the processes are concerned, a first classification of research contributions was discussed in the CIRP General Assembly (Duflou et al. 2012). There the authors state that research in manufacturing field, oriented to environmental impact reduction, can be clustered in 5 main sub-classes: unit process level (Individual device or machine tool in the manufacturing system) manufacturing system level, facility, multi-factory system up to considering the whole supply chain level. Another review paper was presented at the ASME international manufacturing science and engineering conference (Haapala et al. 2013). In that paper the authors scrutinize the research papers focusing more on the differentiation between manufacturing processes and manufacturing system. Considering the process level, Ingarao (Ingarao 2017) clustered the scientific papers in 4 subsections: 1. effect of process parameters 2. role of machine tool architecture and related technology 3. applied process 4. manufacturing approach selection. As concerns material efficiency options, a framework is presented by (Allwood 2014). The authors there provide strategies within three main classes corresponding to principles: reduce, reuse and recycling. Within each class guidelines for material efficiency practices are detailed. As concerns material efficiency options, a framework is presented by Allwood (Allwood 2014). There the authors provide strategies within three main classes (i.e. principle): reduce, reuse and recycling. Within each classes guideline for material efficiency practices are detailed. A good framework of all the possible reuses of materials is provided by Coopper (Cooper and Allwood 2012). In this research the authors identify four main reuse strategies for metals: Remanufacture, Reshape (applying metal shaping processes, additive, subtractive, mass conserving) to obtain a new geometry, Relocate: (recovering component and applying little refurbishment, components reused in the same type of products), Cascade: recovering component and use it in another less demanding use (downgrading). The role of manufacturing processes in putting in place material efficiency/reuse strategy was also outlined by Ingarao (Ingarao 2017). In fact, manufacturing processes deserve to be considered as means for enabling material efficiency strategies. SM may be pursued by several strategies, such as re-designing and/or even changing manufacturing practices to conceive new-generation products, as well as by creating a closed-loop of environmentally-friendly material flow. The 6R Framework All the life-cycle stages of the product development (namely, design, production, use and post-use) should be considered carefully, with a particular attention to the design phase. This represents the real difference introduced by sustainable manufacturing concept with respect to traditional manufacturing, to lean manufacturing or even to green manufacturing, precursors of the SM. In a word, SM paradigm embraces those principles belonging to the 6R framework, namely: Reduce, Reuse, Recycle, Recover, Redesign and Remanufacture. The latter three principles where not included into the previous 3R framework. To a certain extent, it appears that the better definition of sustainability for any manufacturing operation provided so far is the extent to these principles are applied. This statement justifies because in the search to find a common rationale behind the mess of SM applications, one possible way is to find common roots in the principles that inspired the same applications: namely principles, which allows to the decision maker either an effective way to search for new sustainable solutions or to a certain extent measure the “degree of sustainability”. The higher the number of principles that are satisfied, the higher the potential positive impact on sustainability can be for a given adopted solution. In this sense, far from stating that the 6R framework is a widespread and commonly adopted metric of SM, the starting assumption in this paper is that 6R is the only approach, to the author’s knowledge, that provides a criteria of classification and selection of solution, and thus indirectly to provide a clear definition of what a SM application may look like. The true question nowadays is if the 6R framework may capture the essence of SM, provided that it is really critical to clearly define the SM paradigm rationale to the scientific community. This paper aims to stimulate the reflection from the Italian perspective to this point, with a particular concern to the production technology side, by using an automatic classification of papers within the 6R framework and then by benchmarking approaches followed by the Italian Technologist research-network SOSTNERE on SM issues. Sustainability (from sustain plus ability) refers to the set of properties of a given system (either natural or artificial), which allows the same system to maintain itself for an almost indefinite period of time. This concept was officially introduced in a document of the World Commission on Environment and Development (WCED) entitled “Our Common Future”, where the Sustainable Development was defined as follows: “Sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs”(WCED 1987). Starting from this general definition, further conceptualizations have been produced for manufacturing activities and/or processes, as here briefly recalled. According to the Organization for Economic Co-operation and Development, Sustainable manufacturing is a formal name for an exciting new way of doing business and creating value. Different statements of sustainability in literature11 share the same focus on the following three aspects: economy, environment and society. It is thus possible to summarize from the literature analyzed a possible definition for sustainable manufactured processes/products, according to the following set of prescriptions: (-) minimize business risk; (-) minimize negative environmental impacts; (-) conserve energy and natural resources; (-) are safe for employees, communities and consumers; (-) are economically sound; (+) a new way of creating value; (+) are socially and creatively rewarding for all working people; (+) providing access to basic services, green and decent jobs and a better quality of life for all ; (+) adopt sustainable infrastructures. where the (-) sign stands for those prescriptions oriented to preservation of resources without any significant change of the present condition, while the (+) sign indicates those prescriptions aiming at ameliorating/modification of the trends with respect to traditionally manufactured processes/products. To summarize simplifying, sustainable manufacturing is all about minimizing business risks of any manufacturing operation while maximizing the new opportunities that arise from improving processes and products. Fascinating in principle, these general statements are really difficult to deploy into real operations and production settings. The focus of the present analysis is to derive a clear definition of the concept of manufacturing sustainability based on clear and paper based evidences, rather than referring to general ethical or social principles, which appear to be a top down definition. Conversely, information extracted from all the scientific papers available on SM can provide a sort of bottom up statement. Evidences are relevant words and short concepts that can be related to sustainability, as it will be explained in the following paragraphs by also providing some sound examples from the italian point of view. 8.1.1 Methodology In this section, we describe the process involved to analyse the papers in the sustainable manufacturing field. To give a more detailed vision of the topic, we consider this field divided in 6 sub-classes (one for each principle), adopting the well know 6R framework. The goal of the process is both to identify which the topics in each of the 6R are, then to accurately measure the way this framework corresponds to the topics of sustainable manufacturing. As seen in the flowchart of figure 1 the main activities of the process are four: Paper manual classification, Automatic Keywords Extraction, Keywords manual selection and Clustering. Each activity will be described in the next subsection. Assignment of 6R principle meanings The first critical issue is the selection criteria of 6R principles for the assignment. This was done by trying to assign a semantic specification of each R principle, as below explicated, built by summarizing all the possible definitions and concepts extracted by the selected bibliography (partially cited in the present paper). The semantic specification provided below refers to the functional scope of each class, intended as an activity to be performed. Figure 8.1: Flowchart of the 6R papers analysis process. The white boxes represent the activities and the yellow boxes the predicted documents. In the figure are also highlighted the input and the main outputs of the process. Reuse Reuse is the action or practice of using something again, whether for its original purpose (conventional reuse) or to fulfill a different function (creative reuse or repurposing). Reuse involves taking, but not reprocessing, previously used items in order to save time, money, energy, and resources. In particular reuse is useful for machineries, especially when they are expensive, and for their components. Besides, technologies can be also reused and readapted to different situations. In addition, reuse is important because it minimizes disposal needs and costs. Reduce In pre-manufacturing it is possible to implement the reduction minimizing the use of resources. During the production phase, it refers to the use of energy and material, for example, it may involve the use of lower cost materials, the elimination of unnecessary product characteristics, the reduction of overhead costs or the readjustment of product processes. It implies a decrease of costs. As a consequence, the derived effect of this principle is to minimize and optimize performance in terms of cost, time and waste and it focuses primarily on the first three stages of the four life cycle of the product before recalled (§1). Recycle Recycle involves the process of converting material that would otherwise be considered waste, into new products and it corresponds to the breaking down of used items to make raw materials for the manufacture of new goods. Recycling can prevent the waste of potentially useful materials and reduce the consumption of fresh raw materials, thereby decreasing: energy usage, air pollution (from incineration) and water pollution (from landfilling). Recyclable materials include many kinds of glass, paper and cardboard, metal, plastic, tires, textiles and electronics. Recover Product recovery operations refer to operational processes (e.g. disassembling, sorting and cleaning) on products at the end of their use, in order to make them usable in n subsequent life-cycles. Differently from recycling, the product life-cycle is shortened, skipping all that phases of retreatment of wastes up to their second use. The risks of implementing product recovery operations is the uncertainty of returned product quality and quantity and for this reason recovering activities have only recently been considered in manufacture. Redesign The redesign activity involves the act of redesigning the next generation of products, which would use components, materials and resources recovered from the previous life-cycle or previous generation of products. It refers to the evaluation of ideas turning them into concrete innovative products obtained from products in their post-use phase. Remanufacture Remanufacture involves the re-processing of used products, to restore them to their original like-new state through the reuse of as many parts as possible without loss of functionality. It includes and exceeds the activity of recovering, because a remanufactured product should match the same customer expectation as a new one. Despite this initial classification, some problem occurred for experts in assigning the papers to the different classes, due to the absence of the formalization that is needed to define unique criteria of belonging. This fact brought to the unbalance of the number of words within each class (corresponding to each principle), that drove to the fuzziness of the training set and, as a consequence, the difficulty in performing a real classification of scientific papers. This is the first critical issue of the followed approach so far, that may be overcome by alignment sessions of the decision makers. Automatic Text analysis and Keywords extraction In order to analyze the information coming from scientific papers belonging to topics encompassed within the 6R framework, we exploit text mining techniques, which employ methods from different fields of data mining to extract meaningful information 8. The second activity in our analysis process is thus devoted to transforming the set of papers in a set of numeric vectors to be elaborated by the clustering algorithm. To this aim, some text mining techniques are applied in sequence 12 to automatically extract meaningful information and knowledge from unstructured texts. The text mining process performed is summarized in the following. First, the information content of the document is converted into a structured form (vector space representation). In fact, most of text mining techniques are based on the idea that a document can be faithfully represented by the set of words contained in it (bag-of-words representation 9). According to this representation, each document j of a collection of documents is represented as an M-dimensional vector, where M is the number of words defined in the document collection, and w(tji) specifies the weight of the word ti in document j. The simplest weighting method assigns a binary value to w(tji), thus indicating the absence or the presence of the word ti, while other methods assign a real value to w(tji). In the following, the text mining steps performed are described (for details aobut the activities see section 5: Tokenization is the first step of our text mining process, and consists in transforming a string of characters into a string of processing units called tokens that could be syllables, words, or phrases. Typically, with tokenization other operations are performed with the aim of making the text cleaner. Such operations are the removal of punctuation and other non-text characters, and the normalization of symbols (e.g., accents, apostrophes, hyphens, tabs and spaces). In the proposed system, the tokenizer removes all punctuation marks and splits each text into tokens corresponding to words, bigrams and trigrams. Stop-word filtering consists in eliminating the words which provide little or no information to the text analysis, or that, in our case, could make the clustering process fuzzier. Common stop-words belong to certain part of speech classes, such as articles, conjunctions, prepositions, pronouns, etc. Other stop-words are those that typically appear very often in sentences of the considered language (language-specific stop-words), or in the set of texts being analysed (domain-specific stop-words). In our case, this second group consists of typical words of the scientific articles such as “paper”, “state-of-the-art”, “present work”. Stemming is the process of reducing each word (i.e., token) to its root form. The purpose of this step is to group words with the same theme having closely related semantics. In the proposed system, the stemmer exploits the Snowball Stemmer for the English language, based on the Porter’s algorithm. Feature representation consists in building, for each text, the corresponding vector of numeric features. Indeed, in order to cluster the texts, we have to represent them in the same feature space. In particular, we consider the F- dimensional set of features corresponding to the set of relevant stems. Manual keywords selection Keyword selection procedure was performed by hand from a team of 4 experts in the scientific field of sustainable manufacturing and with consolidated expertise on research and innovation. In order to prevent misunderstandings and difference in the judgement, a preliminary analysis of the single attitudes to classification was statistically performed and misalignment were eliminated by a guided training, based on the contemporary evaluation of a same sample from different experts so as to align the judgement. This process was done, as above mentioned, based on the 6R explicit framework and sharing the meaning of each principle. It is clear that assignment of keywords to classes is not a trivial task and susceptible of interpretation. Disambiguation process would be required to explain the criteria of affinity to a given class, hopefully by referring to the functional scope of each class. Still an ambiguity may result, which can hardly be removed without a more profound specification with appropriate examples, which is out of the scope of the present paper. According to this procedure the 6R refer to the following keywords: RECOVER: recovery, planning processes, renewable resources, new approach, waste recovery; process scrap recovery; energy recovery; heat recovery; collection; separation; design for environment; material recover optimization; recovery logistics; RECYCLE: recycle, adhesive technologies, materials conversions, government regulation; (Advanced) recycling technologies; Recyclability; End of first life; Down cycle; New/Old process scraps; Material scraps; Landfill taxes; Recycling benefit awarding; Secondary material production; Embodied energy saving; Environmental legislation REDESIGN: machining, cutting, lubrication, environment technologies, cryogenic; Material-efficient design; Design for Environment; New Materials; Eco-friendly design; Eco-friendly materials; Material reduction; Light- weighting; Design for Disassembly REDUCE: reduce, optimize, waste minimization, consumption, energy payback, pollution, reduction, emission, saving; Energy reduction; Waste reduction; Resource reduction; Material usage reduction; Process sustainability optimization; Energy efficiency; Heat efficiency; Manufacturing efficiency; Doing with less; REMANUFACTURING: remanufacturing, material processing, eco-efficiency, renewable, innovations, nanocomposites; Product renewal; Product upgrade; Reconditioning; Part replacement; Modularity; Disassembly; Inspection; Separation; Re-Assembly; Design for Remanufacturing REUSE: reuse, replacing, material flow analysis, eco-efficiency; Component reuse; product re-conditioning; product upgrade; non-destructive recycling; reuse supply chain; product maintenance; product repair; product monitoring; It is clear that the keyword extraction is not a trivial task, since the outcome of the process may range from a useless single word to a meaningful group of combined words that, on the other hand, could become too specific to be significant for the training set of the search engines. Paper Clustering Document clustering is the application of the general process of cluster analysis to texts. The practical applications of document clustering systems are several and vary from automatic document organization to automatic topic extraction. For further details see section 5.5.4. Independently from the kind of algorithm, before the clustering phase, each document has to be represented as a set of features. These features are typically the n-grams contained in documents, so a critical activity for clustering effectiveness is the n-grams extraction (or feature selection). This goal is achieved with a series of sub-activity. These are typically tokenization (the process of parsing text data into smaller units), stemming and lemmatization (reducing all tokens to its semantic base), removing stop-words (less important words), and finally computing term frequencies (or other measures of the relationship between documents and words). To get an exploratory view of the degree of precision with which the 6R framework represents the papers in analysis, the manually classified documents were clustered using a clustering Spherical K-Means Clustering algorithm (Buchta et al. 2012). We will then compare the output of the algorithm with the manual classification in the following paragraph. Spherical k-means exploit cosine dissimilarities to perform prototype-based partitioning of term weight representations of the documents. Prototypes are centroids defined in the same feature space of the documents. The aim of the algorithm is to minimize, given a set of objects (documents in our case) and prototypes described in the same features space (the selected keywords) the cosine distance between each element and the closest prototype. This implies that the output of the algorithm is a membership matrix, in which each document is assigned to a certain prototype. 8.1.2 Results In the present section, we describe the output of the application of the methodology described in section ?? on the 339 selected scientific papers on sustainable manufacturing extracted by the query “sustainable manufacturing” in the paper search field of the SCOPUS® database. Accordingly, apart the deliberate selection of the source database, no other filter was applied in the journal selection and this guarantees the significance of the journal sample selected. The main output, as highlighted in Figure 1, is the distribution of documents among the 6R principles (i.e. classes) as manually classified, the automatic keywords representations of the classes and the outputs of the clustering algorithm (which is an unsupervised assignment of the documents) compared to the manual classification. The assignment of a paper to a class was made by recognizing the application, or tools or scope of the paper to one 6R’s principles. In this sense, for instance, a paper belonging to a recover might have as scope, or keywords or approach or an activity performed with the principal task to assure the recovery of a give good. It is thus clear, from the beginning, how complex may be to recognize just one principle more than a multiple possibility to satisfy more principles. This fact will be discussed in the followings. Distributions of documents among the 6R The main point coming out from the histogram in Figure 8.2 is that the distribution of scientific papers assigned to the 6R principles (classes) is not homogeneous. If the four classes Reduce, Recycle, Redesign and Remanufacture collect a comparable number of papers, the other two (Reuse, Recover) are poorly represented. One possible explanation of this is that cases of reuses or recoveries are less generalizable compared to other “R”, and thus potentially less interesting for conferences and journals. Recovery and reuse, in fact, often refers to specific functions embedded into the product, and finding a general rule for assessing or designing such processes might be more difficult. They rather might find room in technical magazines, not included in the present analysis as concern processes. It worth noting that reuse and recover here refers to product and not explicitly to materials. Different approach concerns the design for reuse, which to a certain extent would belong to remanufacturing, as in the most of papers discussed, to mention a few, in the Annals of CIRP 18,19,20. It is difficult to think that the classification criteria, which plays a critical role into the assignment to classes, contributed to this situation, depending on the meaning assigned to each principle. Whether a different combination of classed have been provided, a different histogram would be expected to appear. Figure 8.2: Histogram of the number of papers manually assigned to each of the 6R principles. Keyword vector representation of 6R After the phases of automatic keyword extraction and manual keyword selection, we derived the paper/selected keywords matrix. A sample of the elements of this matrix (keyword and their occurrency) is: RECOVER: Life cycle 5, environmental impact 4, raw material 3, cycle assessement 3, tossii fuel 3, energy payback 2, cleaner products 2, energy cost 2, energy usage 2, pv System 2, product process 2, managements System 2, new technologies 2, energy save 2, climate change 2, risk assessements 2 RECYCLE:lite cycle 40, environmental impact 31, waste management 30, resource conservation recycle 25, solid waste 21, lite cycle assessement 18, recycle material 14, recycle process 12, reuse recycle 11, electronic equipement 11, global warm 9, waste disposai 9, heavy metal 8, environment friendly 8, waste treatment 8, waste stream 8 REDESIGN: manufacturing process 20, lite cycle assess 19, sustainable manufacturing 17, machine tool 14, naturai gas 14, cutting tool 13, raw material 13, tool life 13, cutting fluid 13, tool wear 13, mechanical engineering 12, System boundaries 11, machining process 11, manufacturing System 11, cutting speed 11, cutting zone 10 REDUCE: life cycle 40, environment impact 38, energy consumption 31, life cycle assessment 28, energy usage 22, energy requirements 18, solar energy 15, cleaner product 13, global warm 13, climate changes 12, solid waste 12, gas emission 11, solar celi 11, cutting conditions 11, co2 emissions 10, environmental management 10 REMANUFACTORING: sustainable manufacturing 28, manufacturing process 22, remanufacture product 18, supply chain 16, new product development 16, climate change 12, business model 12, product remanufacturing 12, manufacturing System 11, environmental performances 10, remanufacturing industry 10, management System 9, waste generation 9, design for environment 9, remanufacture and recycle 8, automotive industry 7 REUSE: life cycle 7, environment impact 7, energy usage 4, mechanical engineering 4, industriai ecology 4, cleaner product 3, life cycle assessement 3, environment management 3, design for manufacturing 3, environment protection agency 3, waste management 3, united nation 3, environment management System 2, process design 2, sustainable manufacturing 2, green chemistry 2 The keywords were divided according to the 6R principles (i.e. class), listing the number of the paper belonging to each class.The sample list of keywords and their occurrency, reflects the distribution of figure 8.2 and cannot thus be significant for classification but only for clustering purposes at the present. Clusters/classes confusion matrix To analyze from a semantic dimension how well 6R framework represent sustainable manufacturing definition through the scientific papers considered, the manually classified documents were clustered using a clustering Spherical K-Means Clustering algorithm. The results (the assignments of each paper to a cluster) was then compared with the manual classification. The number of resulted clusters was 6, having this number of clusters no specific relation with the number of 6R classes. The output of the clustering process is thus a 6x6 Matrix (Class(row)/Cluster (column)) shown in Figure 8.3. Numbers in the cells of the matrix represents centroids that can be characterized by the related keywords. Figure 8.3: Matrix: comparing the manual classification and the clustering output. 8.2 Sustainable Manufacturing: An Extended Mapping The methodology described in section ?? has been then re-adapted to give an extended anlysis of the 6R framework from a bottom-up perspective. In the present section, we map the state of the art of Sustainable Manufacturing using topic modelling, a widely used text mining techniques. The aim is to give a broad overview of how this knowledge field is approached world-wide and then, in section 3, to have a deeper look at the Italian way to sustainable manufacturing. 8.2.1 Methodology The process of topic extraction is composed by the sequent activities: Papers Collection Keyword Extraction Topic Modelling: Number of topic selection Topic Modelling: LDA Model fitting Manual topic labelling Each activity is described in the sequent sections. Papers collection The analysis starts form a corpus of papers on sustainable manufacturing. The papers were downloaded from the Scopus database searching for the query: \\[\\begin{equation*} TITLE-ABS-KEY(&quot;sustainable.*manufacturing&quot;) \\end{equation*}\\] At the date of 29/05/2018 the query results in 1,628 documents. Keyword extraction We represent each article as a set of keywords, merging Author Keywords and Index Keywords. The keywords are then “sanitized” following the sequent rules: Eliminate duplicated keywords Eliminate brackets and its content Substitute non-alphanumeric character with a blank space Merge synonyms, alternative spelling and keywords pointing to similar concepts Eliminate scientific literature specific keywords like “article” and “review” Filter the generic keywords. The metrics for the threshold is the percentage of papers that contains the keywords. The value has been set to 7.5%. Finally, we compute the Keywords Term Matrix. The matrix is composed of 1,628 documents and 26,272 keywords. Having a mathematical representation of the documents allows us to apply standard mathematical techniques to them. Number of Topic Selection The goal of the present section is to compute a topic model based on the keyword representation of the papers. A topic model allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document’s balance of topics is. To deploy it for the purposes of the present paper, keywords will cluster to form a topic: each topic will represent a different view (say, principle) of sustainable manufacturing. This approach brings a new perspective to the definition of sustainable manufacturing with respect to the 6R’s framework. To compute the topic model, we use the Latent Dirichlet Allocation (LDA) algorithm. To select the number of topics for LDA, the most efficient and effective way is to calculate multiple metrics of the quality of the results in function of the number of topics. All existing methods require to train multiple LDA models to select one with the best performance. Several approaches tried to take the problem of automatically finding the right number of topics contained in a set of documents. Every approach follows the idea of computing distances (or similarities) between pair of topics varying the number of topics. We used four different methods to evaluate the output of a topic model for different value of k. These methods are: Caojuan2009 (Cao et al. 2009): Minimize the average cosine distance between every pair of topics. The best topic number K has a minimal final distance between topics in Latent Dirichlet Allocation Arun2010 (Arun et al. 2010): Minimize the symmetric KL-Divergence of the salient distribution that are derived from the matrices of factors. These matrices are the re-projections of the documents on the topics and of the topics on the vocabulary (the selected tokens). The divergence values are higher for non-optimal K values. Griffiths2004 (Griffiths and Steyvers 2004): Maximize the likely of the data given the model built considering K topics. This is a problem of model selection using Bayesian statistics. Figure 8.4: Relationship between the measures of the quality of the topic modelling output and the number of topics. We computed these for measures fitting a LDA model for every K between 2 and 20. We choose the higher value because we wanted to obtain a reasonable number of topics representing the concepts (or we may say, principles) behind sustainable manufacturing. The results of the analysis are shown in figure 8.4. From this figure is evident how the measure we want to minimize intersect each other at a value between 11 and 12 topics. We thus decided to visualize the results of the LDA models for a number of topics k between 7 and 15: the expert panel interviewed then allowed to decide that the best results was at 12 topics. 8.2.2 Results We then extract the one-topic-per-term-per-row probabilities, beta. Beta measures what is the probability for each topic to produce a term. Figure 8.5 shows the top 5 terms for each topic. Here each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term). It is possible for some keywords to belong to multiple topics: for example, optimization belongs to topic 1 and 9. It is easy to spot these keywords because typically (belonging to two different topics) are far from the group of keywords of the topics they belong. Figure 8.5: Content of the 12 topics. Each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term). Manual Labelling of the topics To have a clearer representation of the topics, these were manually labelled by 6 independent experts in the field of manufacturing and sustainability. Then a seventh experts took in input the results of the manual naming and synthetized the label. The final results are the names of the 12 topics. These names are: Smartness for Sustainability Sustainable Machining Manufacturing Environmental Efficiency Modelling Manufacturing Sustainability Welding Sustainability AM Sustainability Life-cycle Product Management Advanced Material Sustainability Production Management for Sustainability Sustainable Energies Innovation for Sustainability Sustainable Logistic This final picture provides a clear representation of how topics recognized are mostly homogeneous, provided there are few outlier keywords per each topic, as in figure 8.5. This “new perspective” of sustainable manufacturing represents a way of defining its contents by the grace of the tools or domain of interest involved. For instance, topic n.1 (smart sustainability) refers to the use of the new group of tools and approaches belonging to the Industry 4.0 stream, thus defining specific contents and criteria useful to pursue the sustainability in manufacturing. 8.3 Blockchain In section 7.2 has been proposed a new usage of sentiment analysis to extract advantages (considered synonym of the words benefit, gain or profit) and disadvantages (drawback, failure) of technologies from patents (Chiarello, 2017) with the aim of helping researchers and designers to effectively develop new products, analyzing positive and negative properties of an innovation. The main contributions of the present analysis is to list the problems of a technological field. We want to understand which are the problems that research is trying to solve for a technology and if is there any focus from researchers on the solution of certain technical problem. Furthermore using text mining techniques it is possibile to correlate these problems between them and understand if is there any correlation between problems of technologies and if this map help researchers and companies to understand the research agenda. This section thus proposes an innovative methodology for the (semi-automatic) extraction of technical knowledge from academic articles, using text mining techniques. Among many information related to technical knowledge, thw methodology focus on the collection and the analysis of the problems of a technology. The meaning of problems of a technology is related to the concept of disadvantages of a technology but with a broader sense. In fact, using papers as a source makes it possible to collect not only technical disadvantages (typically described in patents) but also, organizational, business or even social problems, since these are faced by different intellectual fields in the scientific literature. We also propose a case study on the application of the method to the field of block-chain. We chose this technology because: It is highly innovative It is having an impact in many different field The problems of this technology are both technical and managerial How Blockchain Technology Works The blockchain can be exemplified as a process in which a set of subjects shares computer resources (memory, CPU, band) to make available all private users in which each participant has a copy of the data. The use of cryptographic validation techniques generates the mutual trust of the participants in the data stored by the blockchain, which makes it comparable to the registries managed in a centralized manner by recognized and regulated authorities (banks, insurance companies, etc.) (Pilkington 2016). A blockchain is an open and distributed register that can store transactions between parties in a safe, verifiable and permanent way. Once written, the data in a block can not be retroactively altered without the modification of all the subsequent blocks, and this, due to the nature of the protocol and the validation scheme, would require the consensus of the majority of the network. (Iansiti and Lakhani 2017) The blockchain is a continuously growing list of records, called blocks, which are linked to each other and made safe by the use of cryptography. Each block in the chain contains a hash pointer (that is a link to the previous block), a timestamp and the transaction data. The distributed nature and the cooperative model makes the validation process robust and secure, but it has considerable time and costs, due in large part to the price of the electricity needed to validate the blocks (Underwood 2016). Authentication takes place through mass collaboration and is fueled by collective interests. The result of all this is a robust workflow where participants’ data security expertise is not required. The use of this technology also makes it possible to overcome the problem of the infinite reproducibility of a digital asset and of the double expense without using a central server or an authority. (Karame, Androulaki, and Capkun 2012) There exist an overtrust about blochain technology for businesess, and there exist a growing interest about the drawbacks of this technology (Eyal and Sirer 2018. Lin and Liao (2017), Yli-Huumo et al. (2016)). Has been pointed out that a 51 percent attack 26 would be enough to access or control a private blockchain because, in most cases, the organization that controls it already controls one hundred percent of the block creation. In fact, if someone could attack or damage the blocking tool on a private company server, would be possibile to get complete management of the network and the ability to access and modify the data (Hampton 2016). This is because the centralization due to the privatization of the blockchain leads to a single point of failure, or a central break point, which in a public blockchain could not happen as distributed, so there would be no central point to attack. This could have profound implications in financial crises or debt crises like the 2007-08 financial crisis. 8.3.1 Methodology Considering our objectives, the approach we adopted for paper analysis is radically new with respect to the one traditionally established in the literature (bibliometric analysis and keyword approaches). These approaches do not allow a deeper understanding of the technical content described in the text. For this reason, we relied on text mining techniques supported by our technical knowledge base. For the sentiment computation in fact, we used a technical sentiment lexicon developed by the authors that extracts advantages and disadvantages of inventions from patents (7.2) and a novel dictionary lookup approach that incorporates weighting for valence shifters. Following a bottom up-approach, we redesigned and updated these lexicons for an optimal application to paper documents. Figure 8.6: Proposed workflow for the problem extraction from papers. The workflow of the proposed methodology is shown in figure 8.6. The process starts with the collection of abstracts belonging to the same technological field. The abstracts are downloaded using the Scopus API, extracting all the documents that contains keywords of the selected technology in the title, abstract or keywords fields. The texts are then pre-processed using state of the art natural language processing tools (sentence splitter, tokenizer, lemmatization). Then, for each sentence we computed a negative sentiment polarity and took into consideration only the sentences having a negative polarity score below a threshold level. We applied topic modelling algorithm on the negative sentences with the aim of clustering. The output of topic modelling is evaluated by technology domain experts with the aim of labeling the identified clusters. The application scenarios of our methodology have a wide range of users: Companies that want to rapidly map a certain technological field Policy maker that want to invest to solve problems of a technology to boost its innovation Journals, to understand the hot-topic of a specific field Research networks, to exploit possible collaborations and synergies between scholars. Document collection and selection The documents were extracted from Scopus searching for the query: \\[\\begin{equation*} TITLE-ABS-KEY(&quot;blockchain&quot; OR &quot;block chain&quot; OR &quot;block-chain&quot;) \\end{equation*}\\] At the date of 29/05/2018 the query results in 1,628 documents. The term blockchain and its morphological variations is used in other fields different from computer science and economics, in particular in chemistry. For this reason we filtered from the data set all the papers belonging to one of the sequent All Science Journal Classification (ASJC) classes: chemistry, physics and astronomy, chemical engineering, biochemistry, genetics and molecular biology, pharmacology, toxicology and pharmaceutics, immunology and microbiology. The result of this lead us to a set of 1,364 papers. To have a dataset with an higher precision, we also filtered using rules based on the content of the abstracts, since many conferences do not have an ASJC class in scopus. This characteristic of the scopus database could lead to bias in the results, especially for innovative technologies like block-chain for which the scientific discussion is stronger in conferences then in more structured journals. For this reason we analyzed the abstracts of the 264 articles belonging to one of the ASJC classes listed above (chemistry related) extracting the most frequent words and filtering for a blacklist of generic words contained in scientific articles. This results in a dictionary of words that has ben used to search and filter al the papers containing in the abstract one or more of the words in the dictionary. The final number of selected papers is 1,276. Sentence Splitting A sentence splitter (for furhter details see setction 5.4.1) is then applied to the abstract to divide the texts in sentences. From this step we had an output of 8,406 sentences. Figure 8.7: Histogram of the lengths of the sentences. The next step involved the measure of the sentences length. Some of the sentences in fact could be too long due to several fact for example the style of the author or an error in the sentences splitting phase). Since it is not a goal of the present work to analyze the style of the authors or to design a better sentence splitter, we decided to filter the sentences that are too long. The decision on the threshold level of number of word has been taken analyzing the distribution shown in figure 8.7. The 97% of the population of sentences contain a number of words that is lower than 53. Thus all the sentences contain more than 53 words has been filtered. From this step we had an output of 8143 sentences. Polarity Computation At this point has been possibile to apply a sentiment polarity measurement on the sentences (Rinker 2018). As output we had a polarity score between -1 (strongly negative) and 1 (strongly positive) for each sentence. Since we are interested in extracting the problems of the state-of-the-art blockchain systems, we visualize the distribution of the polarity scores of the sentences labeled as negative (having a polarity score lower then 0). The histogram of this distribution is shown in figure TOT. In this histogram are represented 1,108 different sentences. As we can see we have a distribution center in -0.2, with a tail that goes down to -0.8. This is an evidence that the system give a reasonable measures since it would be unreasonable to have more probable extreme values. Furthermore, the number of sentences having a polarity equal to 0 is 4,435 that is about the 50% of the sentences and the number of sentences having a positive polarity is 2,700 : it is reasonable that the higher number of sentences falls in these classes since the scientific jargon has to be neutral. The positive bias has been seen also for patent documents (see section 7.2)). Figure 8.8: Box plots of the distributions by years of the polarities of the sentences having a negative polarity score. One interesting evidence we found is that the number of outliers (having a polarity values lower than the 95% of the population) is growing over the year as shown in figure 8.8. Even if the mean remains near to -0.2, int he last five years the number of strongly negative sentences has increased. This shows how there has been a trend in the focus on the problems of blockchain. Tokenization and Token Filtering In the next phase we apply a state of the art natural language processing pipeline (see section 5 for more details) to the 1,108 negative sentences. The tokenization process results in 220,094 tokens. To extract the meaningful words (words that adds information about the problem that the sentence is describing) we applied a series of filtering steps. As meaningful unit to analyze (token) we decided to use the lemma of the word. The filtering chain we applied is described in table 8.1- The 9,451 final token is a reasonable number considering the 1,108 sentences in input: we have a mean of 9 words per sentence. Considering the mean of 25 words per sentence shown in figure 8.7, we now have a summary of the sentences contains only the meaningful words. This output is a clean input for the topic modelling phase. Table 8.1: Table of the filters applied to select the relevant tokens. Filter Description Filter Type Examples of filtered words Number of token in output Select only nouns, adjective and verbs POS tagging based the, a, anyway, with, because 20.365 Filter words contained in the advantages and drawback lexicon Stop-words impossibile, problem, insufficient, challenge, issue, dangerous 18.354 Filter words typical of scientific litterature Stop-words paper, study, work, research, review, discuss, find, focus 14.505 Filter words contained in more than 10% of the sentences Frequency filter block, chain, technology, be, have, datum 12.473 Filter words contained in less than 0.1% of the sentences Frequency filter philosophy, dialogue, prospect, liberalized, interorganizational, curriculum, multidimensional 9.988 Filter all the token shorter than 3 character Morphology filtering a, b, c, ©, %, us, bc 9.451 Topic modelling Topic modeling is a method for unsupervised classification of documents which finds groups of documents fitting a statistical model to the data. In other words, these models captures word correlations in a collection of documents with a set of topics. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language (for further details see section 5.5.6). If the topic modelling process is good, we have as output a structure where every topic is an understandable, meaningful and compact semantic cluster that in our case clearly represent a state-of-the-art problem of block chain technology. Document Term Matrix The first step is to take in input the 9,451 tokens and count how many times each one occurs in the 1,108 sentences. In this way we obtain a document-term matrix (DTM) where the documents are the sentences and the terms are our tokens (the lemmas). In our case the DTM has 1077 rows representing the sentences (31 sentences did not contain any relevant tokens) and 1147 tokens. Definition of the Optimal Number of Topics: a Human-Machine Hybrid Approach To fit a LDA model we have to give as a parameter the number of topic k that we think that best represent the corpus in analysis. In literature there exists many measurement to compute an optimal value of k. However it is worth to keep in mind that these measures are not always correlated with expert judgement about topics interpretability and coherence. For this reason in the present paper we use an hybrid approach in which we find an optimal neighborhood of value using state of the art k tuning methods and then we compute a graphical output for the 5 best k values to be evaluated by 4 different experts. Several approaches tried to take the problem of automatically finding the right number of topics contained in a set of documents. Every approach follow the idea of computing distances (or similarities) between pair of topics varying the number of topics. We used four different methods to evaluate the output of a topic model for different value of k. These methods are: Caojuan2009 (Cao et al. 2009): Minimize the average cosine distance between every pair of topics. The best topic number K has a minimal final distance between topics in Latent Dirichlet Allocation Arun2010 (Arun et al. 2010): Minimize the symmetric KL-Divergence of the salient distribution that are derived from the matrices of factors. These matrices are the re-projections of the documents on the topics and of the topics on the vocabulary (the selected tokens). The divergence values are higher for non-optimal K values. Griffiths2004 (Griffiths and Steyvers 2004): Maximize the likely of the data given the model built considering K topics. This is a problem of model selection using Bayesian statistics. Deveaud2014 (Deveaud, SanJuan, and Bellot 2014): Maximize the information divergence between all pairs of topics. The optimal value k is the value for which LDA modeled the most scattered topics. Figure 8.9: Measures of quality of the topic modelling results for growing number of topics. The figure is divided in metrics to minimize (top figure) and to maximized (bottom figure). We computed these for measures fitting a LDA model for every K between 2 and 30. We choose the higher value because we wanted to obtain a reasonable number of topics representing the problems of blockchain. After a brain storming with experts it emerges that there can not exists more than 15 classes of problems. We decide to take the double of this number to take in to consideration the biases of human experts. The results of the analysis are shown in figure 8.9. From this figure is evident how the measure we want to minimize interest each other at a value of 7 topics; the values we want to minimize are a little more unstable (especially Arun2010) and intersect both at level of 9 and 11. We thus decided to visualize and make choose the results of the LDA models for a number of topics k between 7 an 10. To help experts in their decision process, an interactive map of the problem hass been generated for 7, 8, 9, 10 and 11 topics. The map was created using Shiny (Chang et al. 2017). The interactive map of problems (a screen shot of the app is shown in figure 8.10) helped experts to understand the relationships between the problems. Here a multidimensional scaling algorithm compute the inter-topic distance that permits to visualize the distribution of topics in two dimensions. Clicking on a topic the experts can see its content, and for each word can see both at the estimated term frequency within the selected topic and at the overall term frequency. Figure 8.10: Screenshot of the shiny app used by experts to choose the optimal number of topcis. The experts agreed for a total number of 8 topics as optimal. 8.3.2 Results The output topic modelling phase is show in figure 8.11. The results of the topic modelling are visualized to make it possibile for the expert to explore and label the topic representing the problems of block chain technology. Each point is a word, and word belonging to the same topic are grouped. The size of the label is proportional to the probability ß that the word belong to that topic. Figure 8.11: Content of the 8 topics of problems of blockchain. Each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term 8.4 Precision Agriculture The agriculture is facing rapidly changing economic, social and environmental scenarios in 21st Century. Forecasts on worldwide population estimate that it might increase at about 9 billion by 2050 and led the Food and Agriculture Organization of the United Nations (FAO) to underline the growth of food needs of about 60% if compared to the annual average calculated between 2005 and 2007. The Committee on Agriculture and Rural Development of the European Parliament confirms these figures. In this regard, FAO focuses on how global agricultural and food systems can support the subsistence needs of the worldwide population according to different cultures and dietary habits of both developed and emerging countries. Compared to past agricultural policies, nowadays trends related to technological evolution, socio-political changes, water shortages, as well as the increase in energy needs and the emergence of new pests and diseases that affect agricultural production acquire greater importance in the discussion between policy makers and companies concerning the future of agricultural activities. In particular, technological innovation is necessary for the development of companies which aim to strengthen their production processes and organizational structures by exploiting automation and ICT within the cultivation and commercialization processes. Thus, digital technologies at the base of Precision Agriculture are the assets to leverage to deal with two major challenges for modern agriculture. On one hand, the need for an increase in production quantity by optimizing production factors. On the other hand, complying with production standards by combining appropriate quality levels and limited environmental impact. Precision Agriculture is a modern approach for agricultural management that exploits cutting-edge technologies to monitor and optimize agricultural production processes. The concept of Precision Agriculture was born in the United States in the early nineties, where the House of Representatives (Liaghat, Balasundram, and others 2010) defines it as “an integrated information- and production-based farming system that is designed to increase long term, site-specific and whole farm production efficiency, productivity and profitability while minimizing unintended impacts on wildlife and the environment”. Although it is a relatively well-known concept, Precision Agriculture still presents a low rate of adoption as reported by both academic surveys and professional reports (Pierpaoli et al. 2013). Different definitions given by researchers, practitioners and policy makers have gradually allowed to deepen the understanding of the constituent elements of the concept as shown in the following table. Table 1 presents a literature review carried out on the Scopus database focusing on the analysis of the definitions of the concept of Precision Agriculture, and shows how technology emerges as the enabling aspect of Precision Agriculture. Anyway, authors focused also on other elements that refers to Precision Agriculture. Pierce and Nowak (Pierce and Nowak 1999) highlight the centrality of Technology and Benefits as the two key elements of Precision Agriculture. Zhang et al. (Zhang, Wang, and Wang 2002) strengthen these two distinctive elements, which compare in all subsequent analyses (Stafford 2000, Kirchmann and Thorvaldsson (2000)), until the introduction of the concept of Sustainability (Bongiovanni and Lowenberg-DeBoer 2004). Indeed, Bongiovanni emphasizes the environmental topic by underlining the role of Precision Agriculture to manage harvest production inputs in an environmentally friendly manner. Finally, Gertsis et al. (Gertsis and Vasilikiotis 2018), define Precision Agriculture as “A modern farming management concept using digital techniques to monitor and optimize agricultural production processes”. They introduce the concepts of digital techniques and optimization of production processes by highlighting some possible concrete applications of Precision Agriculture. As emerged from the previous analysis, a fundamental role for the implementation of Precision Agriculture is played by digital technologies. In particular, object identification, georeferencing, measurement of physical and chemical parameters, satellite navigation, connectivity, data storage and analysis, process automation and vehicle driving are the most adopted (Schrijver, Poppe, and Daheim 2016). Precision agriculture is then based on a cyclical process of observation and acquisition of data, followed by an interpretation and evaluation of the information acquired, and by the implementation of a series of decisions that respond to them. Thanks to these technologies, farmers can increase production, optimize resources consumption (workforce included), costs and quantitative and qualitative production possibilities, according to the specific characteristics of the soil and cultivation. (Sawanta, Urkude, and Jawale 2016) shows that the implementation of digital technology in agriculture can increase total profitability from $ 55 to $ 110 per acre (1 acre = 4046.87 m²). However, whether the Precision Agriculture is still far from being widely adopted (Pierpaoli et al. 2013, Tey and Brindal (2012)). Indeed, the implementation difficulties related to the high initial investments and the lack of suitable skills among the farmers identified still represent significant obstacles. In fact, a study conducted by Pierpaoli et al. (Pierpaoli et al. 2013) identifies the so-called “non-adopters” farmers, (e.g. those who do not have sufficient skills or financial resources to manage the PA’s instruments). For these reasons measuring the concrete benefits of Precision Agriculture is not an easy task. However, Precision Agriculture is an increasingly pervasive concept within the agricultural sector and the constant evolution of technologies linked to it generates many opportunities which have not been fully explored yet. Technologies advancements are directly linked also to another pervasive innovative concept in worldwide economy: Industry 4.0. This new paradigm grounds on the exploitation of digital technologies in the development of business processes and its enabling technologies are also used in Precision Agriculture . For example, Cyber-Physical Systems can be seen as one of the cornerstones for the development of innovative solutions to monitor and manage processes in agricultural businesses. Starting from this relationship, the present chapter aims at analysing the technologies at the bases of the two domains to identify possible overlaps between Precision Agriculture and Industry 4.0. 8.4.1 Methodology The work focuses on the creation of a dictionary which identifies the most innovative technologies that are applied in Precision Agriculture by investigating the overlaps with Industry 4.0 technologies to create clusters and to analyze the connections between them. The dictionary aims to analyze the technologies related to the Precision Agriculture domain and to identify those belonging also to the Industry 4.0 paradigm. Concretely, the dictionary is a list of Precision Agriculture technologies that were identified by analysing papers retrieved from the international database. First of all, the twenty-five most cited scientific papers were identified by using the query “precision agriculture”, among those published between 2002 and 2017. Secondly, a text mining analysis on the 25 papers was conducted to identify the technologies mentioned and belonging to the Precision Agriculture domain. However, this list of technologies was not to considered exhaustive because of the reduced number of analyzed sources, which however provided the basic information to build the analysis context. To face this limitation, and given the proximity of the founding concepts of Industry 4.0 and Precision Agriculture, the Technimetro® was used to expand the list of technologies at the base of the dictionary. Technimetro® (Chiarello et al., 2018) is a dictionary that contains over 1500 technologies belonging to the Industry 4.0 paradigm and was created by selecting the Industry 4.0 technologies found in manuals, technical documents and scientific publications on Scopus. The relationships between these technologies were studied through a text mining activity to describe possible clusters and to understand how technologies are linked one-another. Therefore, the present work attempted to understand if the Technimetro® contains other Precision Agriculture technologies that were not identified with the analysis of the papers. To answer this question, all abstracts of the publications on Precision Agriculture (published on SCOPUS from 2002 to 2017 for a total number of 4320 papers) were analyzed using the software “R”. In this way, technologies belonging to the Technimetro® that were mentioned in the Precision Agriculture publications were identified. Therefore, technologies extracted through the Technimetro® have been checked to manually eliminate not applicable terms. Technologies ware removed with the help of control groups. Finally, the new list of technologies obtained, was compared to the list of technologies identified at the beginning with the analysis of twenty-five papers for removing duplicates, so the dictionary was ready to use. On one hand, this analysis confirmed the relationship between Industry 4.0 and Precision Agriculture domains, on the other hand, it allowed to create a list of over 1000 technologies referring to the Precision Agriculture domain, by expanding the list generated thanks to the analysis of the 25 most cited papers on Precision Agriculture. This analysis shows how the intersection between the technologies belonging to Industry 4.0 and Precision Agriculture is very broad and makes the two concepts very close from a technological point of view. To investigate the presence of possible technological clusters, a text mining on abstracts of papers belonging to the “Precision Agriculture” domain was performed. The block diagram in Figure 8.12 describes the process to create the dictionary. Figure 8.12: Process to create Precision Agriculture dictionary. 8.4.2 Results The dictionary of Precision Agricultural Technologies includes 324 terms. Thanks to a graph, the most cited technologies and the connections between them allowed to identify at least 6 technology clusters. The graph in Figure ?? shows the structure of the dictionary containing the technologies related to Precision Agriculture and the relationships between the technologies that compose it. This representation allows to deepen the connections between the different clusters and technologies. The connections are represented by the lines that join the different nodes (which represent Precision Agriculture technologies) of the graph. The size of the nodes varies proportionally to the number of papers they are cited in, instead their position depends on the number of connections between the different technologies: the most connected ones acquire a more central position into the graph and vice versa. THe content of each cluster is swhon in Table 8.2. Table 8.2: Clusters in the Precision Agriculture dictionary. CLUSTER TECHNOLOGIES Monitoring GPS, GIS, Data processing, GSM, Satellite, Ultrasound, Lidar, Broadband, Cellular, … IoT Wireless sensor network, Internet of things, RFID, Bluetooth, Zigbee, Wi-fi, Microcontroller, Arduino, … Automation Autonomous vehicle, Mobile Robot, Unmanned aerial vehicle, Agricultural robot, Computer vision, Data management, … Decision Artificial intelligence, Data mining, Expert systems, Forecasting, Machine learning, Semantic web, Smart grid, … Hardware Embedded system, Cyber-physical system, Manure spreader, Raspberry pi, CMOS, FPGA, … Laser Laser, Laser transmitter, Laser receiver, Laser surveying, Optical fiber, Photonic sensor, … References "],
["wikipedia.html", "Chapter 9 Wikipedia 9.1 Industry 4.0: Extracting and Mapping Technologies 9.2 Industry 4.0: a Comparison with Industrie 4.0", " Chapter 9 Wikipedia Wikipedia is a non-conventional source of technical knowledge, with respect to patent and papers. With respect to the literature on field delineation and clustering applied to science and technology we innovate by introducing this new source. In particular in the present chapter, Wikipedia is proposed as tool to map a technological field. The process start with a small number of documents following a procedure which is expert-independent, in order to minimize the distortions from subjective judgment. We then exploit the properties of Wikipedia in order to delineate the field and identifying the linkages between technologies. Further description of the structure of Wikipedia can be found in section 6.3 In this section we present two methodologies capable of automatically use Wikipedia inforamtion and its structure with the aim of mapping and analysing this content. The results of the methodologies are described, togheter with example of applications of the extracted entities for intelligence tasks. 9.1 Industry 4.0: Extracting and Mapping Technologies Industry 4.0 is getting the center of the scene with respect to the future of production systems in advanced countries and to its economic and social implications. It is considered as the new fundamental paradigm shift in industrial production. The new paradigm is based on the advanced digitalization of factories, the Internet, and future-oriented technologies bringing intelligence in devices, machines, and systems (Lasi et al. 2014). Despite its growing popularity and the great expectations in terms of innovation impact, the concept of Industry 4.0 remains strongly linked to technologies and frameworks that have been heavily researched and analyzed in the last decades. In particular, Industry 4.0 can be seen as a smart recombination of existing technologies and some new technologies and their application to the manufacturing environment (Trappey et al. 2016). This recombinant nature has led some authors to claim that it is nothing more than a re-labeling of old technologies, such as Computer Integrated Manufacturing (Apreda et al. 2016). Yet other authors claim that this new wave of technology is fundamentally different from previous technologies and not just an amalgamation. In order to address the question whether Industry 4.0 is a new paradigm, or rather a re-labeling of existing technologies, a preliminary activity is needed, namely the delineation of the field and the clustering of technologies covered in the perimeter. It turns out that this activity is extremely challenging in the case of Industry 4.0, for a number of reasons we discuss in great detail. Faced with the complexity of Industry 4.0 existing delineation and clustering methodologies can be considered inadequate. In this paper we develop a novel approach, test it, and show its superior performance with respect to other approaches. The key features of the approach are as follows: the description of Industry 4.0 is offered in the form of an “enriched dictionary”, or an ordered and comprehensive collection of lemmas, each of which are associated to full scale definitions and descriptions and to explicit linkages to other lemmas; the description of constituent technologies offered in the enriched dictionary is not obtained from individual experts, but is generated by accessing appropriate pages of the online encyclopedia Wikipedia; the total number of technologies covered is more than 1200, linked with more than 39,000 semantic relations; the perimeter of Industry 4.0 is not defined externally to the technology (by experts, government policies or other external sources) but is generated endogenously by examining the linkages between technologies described in the Wikipedia pages; the update of the descriptions of technologies in the dictionary takes place in real time due to the distributed, parallel and selfcontrolled activities of authors in the worldwide community of contributors to Wikipedia; new technologies are automatically included in the dictionary if they exhibit a given threshold of connectivity with those already included in the perimeter. Industry 4.0 as a multi-technology multi-stakeholder field Industry 4.0 is the main keyword used by researchers, policy makers and entrepreneurs when describing how worldwide industrial systems will evolve in the near future by leveraging Internet connected technologies to generate new added value for organizations and society (Roblek, Meško, and Krapež 2016). The growing interest is confirmed by the increasing number of academic papers focusing on topics that are related to the so-called “Fourth Industrial Revolution”. As shown in Figure 9.1 the query “Industry 4.0” generates 967 papers. Even if the query is very sharp and does not include all the research efforts on the single “enabling technology” it demonstrates an exponential growth of the topic. In Figure 9.1 a projection represented by the dotted line is included. The projection has been drawn by considering a constant increase of the derivative calculated as the average of the last 4 years. Our forecast is that in 2017 there will be 575 new papers in Scopus; this estimate is supported by the fact that about 200 papers have been already published before June 2017 (represented by the point). Since previous analyses in Scopus demonstrate a delay between publication and loading of 5–6 months our forecast seems to confirm a growing interest on the topic. Figure 9.1: Trend of publications on Industry 4.0 (Title, Abstract, Keywords). Source: Scopus. Date: 06/06/2017 Table 9.1 shows how the scientific production on Industry 4.0 is divided among the main research fields (multiple attributions are possible in Scopus). In particular, it is possible to identify field specific-technologies that refers just to one or few sectors/business areas, and general purpose technologies that can be exploited in several sectors/business areas. Table 9.1: Breakdown of industry 4.0 papers per research field. Source: SCOPUS date: 06/06/2017 Subject Area Number of Publications Engineering 645 Computer Science 410 Business, Management and Accounting 185 Decision Science 134 Material Science 90 Mathematics 87 Chemistry 52 Physics And Astronomy 45 Social Sciences 34 Energy 30 Formulated initially in Germany in 2011, the Industry 4.0 paradigm has been quickly translated, adapted and reinterpreted in developed and developing countries. Table (tab:tabledocs40scopus) offers a compilation of official documents of governments, agencies and international organizations that in a few years after the initial formulation have embraced the concept. Table 9.2: Technical document and Scientific articles used to create the Technology Seed List. Author(s) Name/title of the source Pages Country Year Publisher Typology France Government New Industrial France - Building France’s industrial future 112 France 2016 French Government Technical Document Geissbauer et al. Industry 4.0: Building the digital enterprise - Global Industry Survey 36 United Kingdom 2016 Price Watehouse &amp; Coopers Technical Document German Trade &amp; Invest Industrie 4.0 - Smart manufacturing for the future 40 Germany 2014 German Trade &amp; Invest Technical Document Heng, S. Industry 4.0 - Upgrading of Germany’s industrial capabilities on the horizon 16 Germany 2014 Deutsche Bank Technical Document National Intelligent Factories Cluster Research andInnovation Roadmap 88 Italy 2015 National Intelligent Factories Cluster Technical Document Rüßmann et al. Industry 4.0 - The future of productivity and growth in manufacturing industries 20 United Sates 2015 Boston Consulting Group Technical Document Siemens On the Way to Industrie 4.0 – The Digital Enterprise 27 Germany 2015 Siemens Technical Document Smit et al. Industry 4.0 - Study for the ITRE Committee. 94 Europe 2016 European Parliament Technical Document The Government Office for Science The future of manufacturing: a new era of opportunity and challenge for the UK 250 United Kingdom 2016 The Government Office for Science Technical Document Wee et al. Industry 4.0 - How to navigate digitization of the manufacturing sector 62 United States 2015 McKinsey Company Technical Document Gorecky et al. Human-machine-interaction in the industry 4.0 era 6 Germany 2014 Industrial Informatics (INDIN) - 2014 12th IEEE International Conference Scientific Article Despite this rapid and impressive convergence of interest (in itself a clear demonstration of the interdependence of policies across the world), there is no common ground in the definition and delineation of the field even if a first definition of the goal of industry 4.0 have been presented since 1998 (Council and others 1998). More precisely, while there is a reasonable convergence on the architectural definition of Industry 4.0, as defined in a relatively loose way, there is still considerable disagreement and misalignment with respect to constituent technologies (Riel et al. 2017,Smit et al. (2016),O’Halloran and Kvochko (2015)). Furthermore, many constituent technologies are included in the definition of Industry 4.0, and hence described in these documents, from a variety of perspective that reflect mainly the huge variety of application domains. In other words, technologies are often described not only with respect to their fundamental engineering principles and related dimensions of performance, but with respect to specific applications to various manufacturing or service operations. In these applications the specific working of technologies and the associated dimensions of performance are indeed quite diverse. Grangel and González (Grangel-González et al. 2016) develop a deductive rule-based system able to identify conflicts among AutomationML documents, named ALLIGATOR. It is interesting for the present work to notice how ALLIGATOR has the function to interoperate and align information models between a vast variety of areas (manufacturing, security, logistics) at a micro/plant level. In other words, this paper highlights the fact that one of the main problems of Industry 4.0 is the integration of models and concepts typically developed in their respective domains. To offer an example of this state of affairs, let us consider the case of RFID (Radio Frequency Identification and Detection) technology. One of the main uses of RFID technology, that is, the detection of the location of a tag moving along a known path with known speed can indeed be applied for largely different purposes (safety, tracking, localisation) and in various company areas (production, logistics, maintenance). In practice, each of these applications will develop the basic technology in different directions. Depending on the applications we will find largely different descriptions of the technology involved. Figure 9.2 offers a Value Chain-like representation of Industry 4.0, showing the wide range of applications of constituent technologies. Figure 9.2: A Porter-like Value chain framework for Industry 4.0 (courtesy of Towel Publishing). An interesting consequence of this state of affairs is that there is disagreement also at the higher level of government documents describing Industry 4.0 as the main object for innovation and industrial policies: when describing the main components of Industry 4.0 the French government uses 47 technologies, against 39 technologies for the Italian government. Summing up, the recombinant nature of Industry 4.0 creates several interrelated problems for profiling and mapping: - the number of constituent technologies is very large - the description and performance of constituent technologies depend critically on the specific application, hence on the business function/company area affected - the stakeholders are located in several organizational positions - the technical progress is very fast, with many (even if not all) constituent technologies facing rapid changes in their nature and performance. Faced with this situation, a traditional approach to profiling and mapping would require a massive effort of keyword definition. A number of experts would be recruited in order to offer a representation of the field from their disciplinary or industry perspective. Extensive domain knowledge would be mobilized in such a way to build up detailed yet comprehensive maps of technologies. Based on these maps, governments, statistical offices and international organizations would work for a few years in line in the effort to identify the trends of technologies, the most important actors, the shares of individual countries or regions in the global landscape. For sure, this is the approach underlying most of the documents illustrated in Table (tab:tabledocs40scopus) and this the approach that will be pursued in the years to come. We strongly suggest this approach does not deliver the expected result. Keyword-based approaches to emerging technologies are too dependent on subjective judgments of experts. Even when the experts involved are top class and disinterested (often the best researchers or industrialists), their vision is inevitably partial. Even more importantly, keyword-based representations cannot be updated with the same speed of technology. The set of keywords identified by experts becomes inevitably obsolete in a few months. This paper leverages on publications and open source repositories to design, develop and test a methodology that provides delineation and clustering of technologies of the Industry 4.0 paradigm. The methodology is based on a dictionary concerning the enabling technologies for industry 4.0 with full definitions and connections between them. Given the fast growth and the uncertainty that characterizes industry 4.0 technologies, the present methodology is designed to be a bottom-up and continuous evolving tool. The structure and measurements made on the tool refer to July 2017. Mapping and Clustering a complex emerging technology The first task for mapping a new technology is field delineation, or the definition of the perimeter of the field. Industry 4.0 is not a new technology, but a novel combination of partly existing, partly new technologies driven by the convergence of their trajectories. As a matter of fact, it is clearly an example of emerging technology (Rotolo, Hicks, and Martin 2015), as it shares the features of rapid growth, technological uncertainty, and market uncertainty. The delineation of new fields of science and technology is an issue addressed since the late ‘70s, after the pioneering period of bibliometrics. Field delineation is a necessary step when existing classifications do not offer timely, reliable or comprehensive coverage of a topics, for example of a new technology or a new technological field. Moving beyond existing classifications require undertaking a search which, in general, may follow a lexical approach, a citationist approach, or a mix between the two (Small 2006, Kreuchauff and Korzinov (2017)). In all cases there is a need to initialize the process, i.e. to identify a set of elements that constitute the starting point for searching. The main approach has been based on keywords, to be identified in various regions of documents (title, abstract, keywords, full text of an article; title, abstract, claims, full text of a patent) and to be used as queries. A query is a structured sequence of words, connected by logical elements, such as “or”, “and” and the like, to be launched on a database in order to build up profiling, indexing, or clustering a given field. In general the initial set of keywords are provided by experts in the field, usually organized in an expert panel. There are several limitations of the keyword/expert approach. First, expert based keyword definition, or patent classification is a very expensive activity (Tseng, Lin, and Lin 2007). Second, the keyword selection is based on subjective judgment, and when experts are asked to decide on relatedness measures (e.g. synonims, hypernims or hyponims), they do not apply systematic rules (Tseng, Lin, and Lin 2007,Noh, Jo, and Lee (2015)). Experts may be subject to a number of biases, such as for example the desirability bias (attributing higher probability of occurrence to preferred events) and many others (see section 5.8.2). Panels of experts are not immune by biases, such as group thinking. There is little research on the impact of expert subjective judgments on the delineation of emerging fields, but there is reason to believe it may be significant. Third, the delineation of perimeter of emerging technologies is not robust to slight differences in the queries. As it has been shown by Zitt and Bassecoulard, little differences in the wording of queries, or on the time window, may end up in completely different sets of documents (Zitt and Bassecoulard 2006). Therefore there is no proof that the method is reliable. Finally, and more problematic for the case of Industry 4.0, the methodology is static, as it is based on a fixed sets of words. This set can (and in practice often is) updated, but this introduces a delay in the process and does not deliver reliable results. Keeping updated a collection of keywords in a dynamic technological landscape is extremely difficult. These limitations have become evident in the last two decades, after the efforts of many authors to produce reliable perimeters of the emerging field of Nanotechnology. The initial efforts have been based on a classical expert-based approach. Panel of experts provided lists of keywords that were transformed into database queries. Among them, a consulting company called Lux, the Fraunhofer ISI in Germany, and CWTS in the Netherlands were the most active. Most studies delivered largely different delineations (Youtie, Shapira, and Porter 2008,Ghazinoory, Ameri, and Farnoodi (2013),Ozcan and Islam (2017)). In turn, these limitations opened the way to massive efforts to reduce the dependence on experts and exploit systematically the new opportunities opened by text mining, following what has been called “full text based scientometrics” (Boyack, Small, and Klavans 2013). Starting from the late ‘90 s several attempts have been made to apply text mining techniques to the patent corpus and the field is currently burgeoning (Joung and Kim 2017,Ozcan and Islam (2017)). Summing up, the existing approaches to the delineation of emerging technologies, taken together, suffer from the following limitations: 1. dependence on expert judgment 2. lack of robustness to alternative definitions of the perimeter 3. delay in update of technologies (static approach) iv) lack of transparency in the modeling of technology. We now turn to the second main task in text mining of technology documents, that is, clustering. Once the perimeter of the field is delineated, a task usually included in the mapping is clustering, or the creation of groups of entities in such a way to reduce the complexity of the representation. Within text mining the clustering of documents is based on various kinds of linkages that are considered a signal of similarity in topics (see section 5.5.4). In general, linkages among documents can be generated by citations (citationbased clustering) or by the extraction of features in texts (text-based clustering) (Leydesdorff and Hellsten 2006,Wang and Koopman (2017),Jaffe, Trajtenberg, and Henderson (1993)). The most used approaches to clustering assume that members of a cluster: – cite each other (citation analysis: (Jaffe, Trajtenberg, and Henderson 1993,Moed (2006),Verspagen (2007),Lee and Kim (2017))); – share certain words (co-word analysis: (Callon et al. 1983,Rip and Courtial (1984),Leydesdroff (1989),Engelsman and Raan (1994),Van Raan and Tijssen (1993),Yoon and Park (2004))); – share a reference in their bibliography (bibliographic coupling: (Glänzel and Czerwon 1996,Kuusi and Meyer (2007))); – share the same sub-fields in a classification (co-subfield analysis:(Chang, Wu, and Leu 2009)); – are cited by the same documents (co-citation analysis: (Small 1973,Small and Sweeney (1985))); – are cited by the same authors (author co-citation analysis: (White and Griffith 1981)). In the proposed methodology we exploit the hyperlink feature of Wikipedia in order to introduce a new approach to clustering. Hyperlinks are introduced by authors in order to establish a semantic linkage between the page and other pages. We exploit this feature as follows: members of the same cluster are those pages that share the hyperlinks to other pages, according to thresholds defined by an appropriate algorithm. Note that hyperlinks are only superficially similar to citations. The origin page “cites” another page by hyperlinking it, but in effect this linkage is not a citation (that is, a reference to a previous work) but a signal of semantic similarity, intended to guide the reader in the network of meanings. We suggest that the hyperlinks are similar to citations under some respect, but very different under some other respects. Like citations, hyperlinks are introduced in the text by authors and reflect intentionality. Unlike citations, they reflect semantic relations, not relations of credit assignment or tribute to scientific authority. Perhaps more importantly, citations are introduced only by the author(s) of a paper and remain unchanged after publication. Hyperlinks, on the contrary, are introduced also by subsequent readers of the Wikipedia page. If the introduction of hyperlinks is not considered appropriate by the community of contributors, as it may happen due to vandalism, they are immediately removed (see the discussion below). This means that they reflect all possible semantic connections among the pages, as collectively stated by a large community of authors, in a reliable and robust way. We believe this methodology offers a remarkable improvement with respect to existing approaches. How is similarity measured by using Wikipedia In general, semantic relatedness is a measure of the similarity between two terms. It can be computed by statistical methods without requiring a manually encoded taxonomy, for example by analyzing term co-occurrence in a large corpus (Resnik 1999,Jiang and Conrath (1997)). Wikipedia has been largely exploited in the literature in order to compute semantic relatedness. Gabrilovich and Markovitch (Gabrilovich and Markovitch 2007) developed an alternative to Latent Semantic Analysis and called this new technique Explicit Semantic Analysis (ESA). This methodology first uses a classifier that is centroid-based to map input text to a vector of weighted Wikipedia articles. Then the vectors are exploited to obtain the semantic relatedness between two terms by computing the cosine similarity. This technique could be applicable to individual words, phrases or even entire documents. Furthermore, the mapping developed in this work has been successfully utilized for documents categorization. A new version of this kind of systems was presented by Milne (Milne 2007). While in Gabrilovich and Markovitch (Gabrilovich and Markovitch 2007) the authors use the full text of Wikipedia articles to establish relatedness between terms, in this work only the internal hyperlinks are exploited. To compute the relatedness between two terms, they are first mapped to corresponding Wikipedia articles and then vectors are created containing the links to other Wikipedia articles that occur in these articles. The main problem facing semantic relatedness using Wikipedia is the disambiguation of terms. Several strategies have been developed to solve this problem. A first approach, described in Strube and Ponzetto (Strube and Ponzetto 2006), exploits the order in which entries occur in the disambiguation pages of Wikipedia to find the most likely correct meaning. On the other hand, Gabrilovich and Markovitch (Gabrilovich and Markovitch 2007) avoids disambiguation entirely by simultaneously associating a term with several Wikipedia articles. Milne (Milne 2007) approach hinges upon correct mapping of terms to Wikipedia articles. However, when terms are manually disambiguated, it has been shown that the systems of semantic relatedness computation are more accurate than the systems of automatic disambiguation (Medelyan et al. 2009). Summing up, a consistent literature in the field of computational linguistics and text mining supports the notion that the use of Wikipedia articles as a knowledge base is justified and promising. 9.1.1 Methodology In this section we give evidence of the methodological steps undertaken to build up the enriched dictionary referred to Industry 4.0. The dictionary contains technologies related to the Industry 4.0 paradigm, each of which is associated to the full set of relations with other technologies. The dictionary is semi-automatically generated using technical documents and the Wikipedia free online encyclopedia. The linkages between technologies have all a semantic content, since they are generated within the text of the articles of Wikipedia when a related topic or entry is considered necessary for the logical flow of the definition or description. Figure 9.3 shows the methodological steps needed to generate the enriched dictionary. In the flow diagram three elements are graphically displayed: activities (rectangular shape), check points (diamond shape) and documents created from the procedure (sheet of paper shape). Figure 9.3: Flow diagram of the adopeted methodology. Generation of the seed list As input for our methodology we used technical documents, official government documents and the most cited academic papers in the field of industry 4.0. The selection of seed documents has been made by: – taking the official government documents of the US government and of three large European countries (France, Germany, UK), all of which are strongly committed to the support of Industry 4.0 and are widely considered a benchmark in international documents (for example, OECD and European Union), plus a selection of technical documents cited in government papers, for a total of 10 documents; – taking the 10 most cited papers on Industry 4.0 according to Scopus. The reference to Industry 4.0 was explicit in the title, abstract and keywords of the papers. The extraction was made on Scopus Database in June 2017. We deliberately limit ourselves to a small number of documents. The reason is explained in conjunction with the chosen stopping rules. The documents have been manually parsed by a team of Master students in Engineering Management at the University of Pisa. The assignment was “Understand each document and extract the enabling technologies for industry 4.0”. The manual search in the documents continued until the team reached the goal of 100 different technologies extracted. For our purposes 100 different technologies represents a reasonable seed list of technologies, which will be used as input for the automatic expansion phase. The chosen documents are shown in Table (tab:tabledocs40scopus). Seed list expansion For each term in the seed list the corresponding page in Wikipedia was found. All technologies identified in the seed were covered by Wikipedia. This is a preliminary confirmation of its relevance as a source of knowledge. These pages formed the initial glossary. The expansion procedure automatically retrieved the pages and identified all hyperlinks included in the description of the technology. The pages that are the target of hyperlinks are classified manually according to the following categorization: links to pages already in the seed: these pages are labeled “anchors”, since they provide robust indicators of technologies that are, at the same time, mentioned explicitly in the seed documents and referred to in Wikipedia pages that deal with other technologies; links to pages not in the seed: these are labeled “missing technologies” and are stored in memory for later treatment as potential candidates to inclusion in the dictionary; links to pages with non-technological content: they are labeled “stopwords” and are eliminated from the procedure. The overall procedure is iterated up to the point in which a number of at least 1000 different technologies is reached. At this point the procedure will stop. Updates and changes of the dictionary can originate from new entries (new technologies) or from updates to existing pages. Given that the automatic extraction of Wikipedia pages can be run on a permanent basis, each version of the dictionary has a date. The current version, illustrated in the rest of the paper, is at the time point of July 15, 2017. The stopping rule for the manual and the automatic expansion The three stopping rules follow a sequential logic of order of mag- nitude: we start with approximately 101 documents, from which we extract 102 names of technologies, that, used as inputs to Wikipedia, deliver approximately 103 final technologies. More in detail we make use of: 20 input documents: all technical documents taken as reference for Industry 4.0 share the same framework (i.e. DIN:SPEC 91345:2106). Many of the documents contains the same informations/terms/ technologies. Furthermore these documents are technology focused and therefore from a low number of documents we obtain a large number of technologies. seed list of 100 technologies: as demonstrated in the past Wikipedia is a good source also for technical terms, therefore we assumed it was able to quickly expand the technologies related to Industry 4.0. For this reason the seed list is composed of no more than 102 entries. output list of at least 1000 technologies: of the automatic expansion works correctly the technologies should increase by an order of magnitude in few iterations. Since the list has to be revised manually we decided for 103 entries as a target thus reducing the impact of manual review. As a matter of fact, the Wikipedia network of semantic linkages delivers a total number of technologies related to Industry 4.0 which exceeds this target, again confirming the validity of Wikipedia as a source of knowledge. Structure of the enriched dictionary The enriched dictionary can be defined as a set of enabling technologies for industry 4.0, associated to their definitions and to the linkages between them. The digital version of the tool is a hyperlinked text.12 For the purpose of publication in an academic paper the tool can be represented as a table in which we have: Column 1- Technologies: Enabling technologies for industry 4.0, or a broad categorisation of technologies following a clustering procedure (see below) Column 2- Url: Links of the Wikipedia pages of the enabling tech- nologies for industry 4.0 Column 3- Definitions: Glossary, snippets from wikipedia page of the definition of the enabling technologies for industry 4.0 Column 4 Links: hyperlinks to other wikipedia pages from the wi- kipedia pages of enabling technologies for industry 4.0 Column 5- Anchors: hyperlinks to other wikipedia pages that are enabling technologies for industry 4.0 from the wikipedia pages of enabling technologies for industry 4. In the table shown in figure 9.4 shows a sample of the dictionary for four enabling technologies. An evidence is that there are conflicting definitions of “Augmented reality” (AR) and “Virtual reality” (VR); the first says that AR contrasts VR, while the second states that “AR systems may also be considered a form of VR”. This is an evidence of the ambiguity that exists in the definition of 4.0 technologies. Moreover, the table underlines how words such as “3D printing” and “Additive manufacturing” that are used in different ways within papers and technical documents (see Table 3), basically refer to the same concept. Table 9.4 also shows the difference between links and anchors. Links include all hyperlinks found in Wikipedia pages. By nature, a certain fraction of these links contain information that is not relevant to our task. It is likely that, in the course of discussion of a given topics, authors quote an author (e.g. Bob Sproull in Virtual Reality), an institution (e.g. British Museum or California Institute of Technology) or an event or application (e.g. Coachella Valley Music and Arts Festival). These links do not add to our knowledge of the Industry 4.0 field. It can be seen that, following the definition of anchor given above, these terms are eliminated in column (E), which includes only the anchors, or those entries that are added to the body of knowledge. Figure 9.4: Sample of enabling technologies showing the table structure of dictionary. 9.1.2 Results The dictionary is composed of 1.211 terms and 39.078 relationships between them. This generates a graph in which the node represents a technology and the edge represents a link in the Wikipedia page. The network structure naturally gives origin to graph-theoretic metrics. We exploit this property in order to generate a number of indicators that the readers may find it useful to examine. Graph analysis and Sub-graph selection Figure 9.5 gives an overview of the obtained. We compute for each node the in-degree (horizontal axis), the out-degree (vertical axis) and the number of links to other Wikipedia pages that each node has (color of the node). Our results show that in terms of the analyzed variables we can identify 4 different clusters of nodes (or technologies) each one having a different behaviour. The first group we take in consideration are the point having an indegree greater than 70 and an out-degree greater than 70. In this group on the top right of the page we have some outliers. These are terms like Microprocessor and Microcontroller, X86, 8-bit, 16-bit and 32-bit. Then we observe a sub-group centered on the coordinates (150, 100). Here we have terms like Program counter, Adressing mode, Instruction Cycle, Coprocessor, Symultaneus multitrading, SISD and MISD. Still within the first cluster we have another sub-group of terms centered in (80,110) with terms like Intel i860, Intel Atom, Intel 80286, Intel 80188 but also Bloomfiled and Wolfdale. The second cluster is one in which the points have an in-degree greater than 70 and an out-degree smaller than 70. Here we have terms like Machine learning, Artificial Neural Network, Cognitive Computing, Software, Random Access Memory, Internet, Firmware and C++. The third group collects points having an in-degree lower than 70 and an out-degree greater than 70. Here we have terms like Processors, Micro-Operation, Micro-Assembler, Application specific integrated circuit. The last and most populated cluster has an in-degree smaller than 70 and an out degree smaller than 70. Figure 9.5: Plot of the in-degree and the out-degree of the nodes of the graph. The colour of the node represents the number of Wikipedia internal links. This cluster is more precisely visualized in Figure 9.6, for which we have as before the in-degree (horizontal axis), the out-degree (vertical axis) and the number of links to other Wikipedia pages that each node has (colour of the node). A jitter process has been implemented to the points on the graph in order to better visualize the overlapped points. In this plot only the technologies having both an in-degree and an outdegree lower than 70 are shown. This generates a subgraph composed by 931 nodes and 10673 edges. Figure 9.6: Plot of the in-degree and the out-degree of the nodes of the sub-graph. Selection of the nodes having an in-degree and an out-degree lower than 70. The colour of the node represent the number of Wikipedia internal links. Graph representation and cluster analysis The structure of the graph offers it self naturally to clustering of technologies in order to obtain a readable mapping. The clustering algorithm receives as input the collection of technology terms T of the analyzed subgraph and returns a set of terms clusters C = {C1, C2,.., Cn} that cover the whole subgraph in analysis. Each cluster Ci is a subset of terms of T, and a term may belong to only one cluster. In Figure 9.7 we show a representation of the sub-graph made using Gephi software with its implementation of the Force Atlas algorithm (Bastian, Heymann, and Jacomy 2009) . In this representation, two nodes in the graph are represented closely if they share an edge. In this way also nodes that belongs to the same communities of nodes (nodes that can be grouped into sets such that each set is densely connected internally) but do not share any edge are represented closely. In other words, the visualization tends to be coherent with the clustering algorithm. The size of the node is proportional to its in-degree while the colour express the cluster to which each node belong. Finally some of the labels of nodes and clusters are shown. Figure 9.7: Representation of the graph of 4.0 technologies and of the clusters in wich them are arranged. Each node is a technology and each edge represent a Wikipedia link between the pages. The size of the nodes is proportional to the in-degree, and the colours represent the clusters. The algorithm we used to compute the modularity of each node and thus to assign a group to each of them is described in Blondel et al. (Blondel et al. 2008). The process resulted in 11 clusters. The content of each cluster is shown in Table 9.3, where for each cluster we can see the first 15 nodes in terms of in-degree (the most pointed terms). Table 9.3: Clustering of Industry 4.0 technologies. Clusters are manually labeled. The table also shows the top 15 technologies of the cluster in terms of the in-degree. # Label of the cluster Constituent technologies 1 Big Data Virtual machine, Data mining, User interface, Algorithm, Computer vision, Cryptography, Printed circuit board, Middleware, Real-time computing, Virtual reality, Augmented reality, Human–computer interaction, Multiprocessing, Decision support system, Supervised learning 2 Transactions, digital certification, digital currency Bitcoin, Cryptocurrency, Bitcoin network, Cryptocurrency tumbler, Digital currency exchanger, Alternative currency, Dogecoin, Ethereum, Litecoin, Monero (cryptocurrency), Namecoin, Peercoin, Virtual currency, Auroracoin, Blockchain, Lisk, Primecoin, Ripple (payment protocol), Titcoin, Zerocoin 3 Programming languages Python (programming language), Database, Computing platform, Ruby (programming language), C Sharp (programming language), HTML, Perl, Hypertext Transfer Protocol, XML, Java (software platform), Haskell (programming language), .NET Framework, Lua (programming language), Sun Microsystems, BASIC 4 Computing MacOS, IOS, Mainframe computer, Graphical user interface, Cloud computing, Home computer, Laptop, Solaris (operating system), Microcomputer, Personal digital assistant, QNX, Read-only memory, Tablet computer, ASCII, DOS 5 Embedded Systems Programmable logic controller, Zilog Z80, CMOS, Zilog Z8, Toshiba TLCS, Zilog eZ80, NEC µPD780C, MOS Technology 6502, R800 (CPU), U880, Zilog Z180, Zilog Z800, Zilog Z8000, КР1858ВМ1, Hitachi HD64180 6 Intel 3D XPoint, Intel ADX, Intel Clear Video, Intel SHA extensions, Intel System Development Kit, Intel 1103, Intel AZ210, Intel Cluster Ready, Intel Compute Stick, Intel Display Power Saving Technology, Intel Mobile Communications, Intel Modular Server System, Intel PRO/Wireless, Intel Quick Sync Video 7 Internet of Things Wireless sensor network, Near field communication, Arduino, NetSim, Z-Wave, OPNET, Telemetry, RIOT (operating system), Routing protocol, TinyOS, Internet of things, NesC, MiWi, Nano-RK, LinuxMCE 8 Protocols &amp; Architectures 1-Wire, Profibus, Smart meter, X10 (industry standard), Modbus, Local Interconnect Network, TTEthernet, Fleet Management System, Keyword Protocol 2000, Meter-Bus, MTConnect, OPC Unified Architecture, PROFINET, RAPIEnet, SAE J1587 9 Communication Network and Infrastructures Wi-Fi, Cellular network, Router (computing), Internet Protocol, Radiotelephone, ARPANET, Radio frequency, Digital subscriber line, General Packet Radio Service, Global Positioning System, CYCLADES, Beacon, Wireless, High Speed Packet Access, Evolution-Data Optimized 10 Production Laser, 3D printing, Home automation, Agricultural robot, Nanorobotics, Semantic Web, Machine vision, Nanotechnology, Robotics, Information and communications technology, Speech recognition, Smart grid, Memristor, OLED, Computer-generated holography 11 Identification Barcode, RFID, QR code, MaxiCode, Mobile tagging, Code 128, GS1 DataBar, High Capacity Color Barcode, Aztec Code, Barcode printer, Bokode, Codabar, CPC Binary Barcode, Interleaved 2 of 5, ITF-14 Let us examine more in depth a cluster, such as Identification (cluster 11). The size and colour of the words are proportional to the in-degree of the nodes as shown in Figure 9.8. Figure 9.8: Wordcloud of the words belonging to the class 11 labelled as Identification. In this figure the size is proportional to the logarithm of the in- degree of each node that represents a word. 9.2 Industry 4.0: a Comparison with Industrie 4.0 The Industry 4.0 paradygim can be understood and implemented differently in different industrial eco-systems, depending on the country in which it is adopted. In the present section we make use of Wikipedia to build a network of pages regarding industry 4.0 in such a way taht it is possible to map how a country is actually implementing the industry 4.0 paradigm. To analyze the differences between the industry 4.0 concept in the country of origin (Germany) and the rest of the world, our approach is thus to investigate the Wikipedia pages of Industrie 4.0 and Industry 4.0 and the pages they are connected to. Using clustering algorithm it is then possible to explore the different focus that countries have on different technological sub-field of Industry 4.0. All aspects during our analyses and the description of the project, e.g. page names, consist of two parts: one in English and one in German. For simplicity and better understandability, we well mostly refer to everything only by the English name. 9.2.1 Methodology In the present section we explain the methodology we adopted to build and analyze the two graphs of Wikipedia pages representing the concepts linked to industry 4.0 in Germany and in the rest of the world. As a proxy of the world-wide vision of industry 4.0 we analyzed the English version of Wikipedia. The main steps of our methods are represented in the workflow of figure 9.9 The first step consisted of extracting all pages of interest from Wikipedia. Then the pages and their connections were used to build two graphs (the German and English version). The nodes of the graphs were then cleaned using a novel approach based on the categories of the Wikipedia pages. Finally a clustering algorithm was used to illustrate communities within the graph. These steps are described in the following sections. Figure 9.9: The workflow shows the steps we followed to generate the two graphs of industry 4.0 related concepts. Wikipedia Pages Collection To list the pages connected to the concept of Industry 4.0, all Wikipedia pages that are linked to the page of Industry 4.0 (and Industrie 4.0) were downloaded. We will refer to these pages as level 1 connection. Then all the pages linked to the level 1 pages are collected and marked as level 2 pages. This process can be continued until a wanted level is reached: for the purposes of the present work we decided to show the effectiveness of the process stopping at level two. For each page, we collected the links to other pages and the categoriess of the page. Graph Generation Two pages that are linked by a link within Wikipedia shares a certain relations of which we ignore the meaning. Thus, it is possible that the approach we described until now, also collects pages that are not relevant for the analysis. In our case, this means that some of the extracted pages are not technologically related to the subject of Industry 4.0. Thus, the list of entries had to be cleaned from these non-relevant ones. To make the filtering process more efficient, we filtered considering the categories that had been extracted together with the pages. We considered as relevant all the categories of the pages of level 1. Then the level 2 pages were automatically considered to be relevant if they were in one of the formerly marked categories. For the nodes selection we also used a black-list of non-relevant Wikipedia pages (e.g. International Standard Name Identifier, International Standard Book Number, Digital object identifier, ISBN), previously developed by the authors in (see chapter 9.1). This list has been traduced in German for the purposes of the present work. Clustering In order to identify macro topics in the formerly created graphs, we applied clustering methods. The most beneficial results were found to be generated by applying a spin glass algorithm, which employs the spin glass model and uses simulated annealing to find communities in networks (Reichardt and Bornholdt 2016). In this approach the community structure of the network is interpreted as the spin configuration that minimizes the energy of the spin glass with the spin states being the community indices. 9.2.2 Results In this section we will focus on describing the differences that emerges between the two national technological approaches of industry 4.0 in Germany and in the rest of the world. The collected pages We collect 97 pages for the German graph and 95 pages for the English graph. Table 9.4 list a sample of forty pages per language, alphabetically ordered. Table 9.4: For each IPC class the ratio of the percentage of patents containing at least one ® or ™ character is computed for two patents sets 1995 to 2002 and from 2007 to 2014. German Version English Version Absatzwirtschaft Adaptive system Automatisierung Agent-assisted automation Automatisierungsgrad Ambient intelligence Automatisierungstechnik AmbieSense Autonomic Computing Analytics Autopoiesis Artificial intelligence Betriebswirtschaftslehre Automated reasoning Corporate Evolution Automation Cyber-physisches System Autonomic computing Dataset Autonomous car Daten Big data Datenabgleich Big Data Maturity Model Datenarchitektur Big memory Datenaustausch Carrier cloud Datenbasis Cloud-based design and manufacturing Datenelement Cloud analytics Datenfeld Cloud collaboration Datenmanagement Cloud computing Datenmodell Cloud computing comparison Datenmodellierung Cloud computing security Datensicherung Cloud database Datenstruktur Cloud engineering Datentyp Cloud Foundry Denken Cloud management Dienst (Informatik) Cloud manufacturing Dienstleistung Cloud research Digitales Objektgedächtnis Cognitive computer Elektronische Datenverarbeitung Cognitive computing Erfahrungskurve Community cloud Erinnerungsvermögen Competitions and prizes in artificial intelligence Feld (Datentyp) Computer-aided engineering Feldgerät Computer-aided technologies Fertigungslinie Computer-integrated manufacturing Fließbandabstimmung Computer vision Fließbandfertigung Continuous analytics Gegenwart Control loop Geschichte der Produktionstechnik Control system Glauben Cyber-physical system Gruppenfertigung Cyber manufacturing Halbzeug Datafication The Graphs The dimension of the graphs in terms of nodes, links and categories are shown in Figure 9.10. From this figure is evident how the two graphs has almost the same number of nodes; by the way, the mean number of categories for the German version is close to 0.5 while its 0.3 for the English version. Beside these measure, as it would be even more evident from further analysis, there is the broader approach to industry 4.0 in the rest world with respect to the more focused vision that Germany has. Figure 9.10: Number of nodes, links and categories for the German version of the graph (left) and the English version (right) The resulting graphs are shown in figure 9.11 (respectively the German version on the left and the English version on the right). Here the vertices of the graphs represent the Wikipedia entries, while the links are depicted by arcs that are directed from the originating page to the linked page. The structures of the two graphs and the difference in the number of links gives an evidence of the more focused network of concepts in Germany: the number of links is in fact lower for Germany, creating a more clustered graph. Figure 9.11: The German version (left) and the English version (right) of the Industry 4.0 related graphs. Graph metrics In this section we analyze the graphs using standard metrics. These measures allow investigations about relevant topology and specific features of a given graph. More precisely, we looked at the following measures: - Graph diameter: calculates the longest shortest path in a graph, thus, the minimal distance between two nodes that are the furthest away from each other. - Nodes Degree: Shows the number of nodes that every node is connected to. The mean value of the degree for the whole graph shows how interconnected the graph is: the higher the value, the more connections the nodes have with each other. - Nodes Distance: Gives back the minimum number of edges that connect one node with each other. Here again, the mean value shows the interconnectivity of the graph: A lower value means that the way from one node to another is shorter, thus that the graph is more highly connected. The results of these metrics for the two graphs are shown in table 9.5. From the diameter measure we can see that all of the pages are connected through a maximum of 4 edges. This is not surprising, since it was a premise of the method when setting the level to 2. Considering the mean degree, a level of 2.38 for the German version and 3.29 for the English version shows how the nodes of the English graph are more interconnected. This can also be qualitative observed when looking at the figures 3. Finally, the average distance from one node to another is higher in the German graph, supporting the finding of the measures discussed above. These findings goes in line with what is said by a study of Acatech (Kagermann 2006): in the USA, for example, the term Industry 4.0 is understood in a much wider meaning. Furthermore, Acatech study say that in Germany, the focus is mostly on technological dimensions, while in the USA, the development of the new business models in the area of big data analytics play a greater role, among others. Table 9.5: For each IPC class the ratio of the percentage of patents containing at least one ® or ™ character is computed for two patents sets 1995 to 2002 and from 2007 to 2014. Measures German English Diameter 4 4 Mean Degree 2.38 3.29 Mean Distance 3.25 3 Clustering Figure 9.12 shows the outcomes of the clustering process: the vertices of the graphs represent the Wikipedia pages, the links are depicted by arcs that are directed from the originating page to the linked page and the clusters are represented by the different colors of the nodes. In order to make these clusters comparable, the label of the German version are translated in English. The resulting, corresponding groups as well as the number of nodes they contain can be found in table 9.6. Figure 9.12: The German version (left) and the English version (right) of the Industry 4.0 clustered graphs. Considering together figure 4 and table 3, it is evident how the central node of the graph(Industry 4.0) is contained in the first cluster Internet of things &amp; manufacturing for both the graphs. It is also by design connected to all other clusters, so that no cluster is isolated from Industry 4.0 node. As evident from table 3, the clusters in the two graphs are similar. We have the same five clusters in both graphs and a different one for each language. This shows that Industry 4.0 framework if considered at a more abstract level, is similar in different countries. This is not surprising, since there must be something that made policy and industry call it the same name, but it is a strong result for our methods, able to map bottom-up this phenomenon. However, when looking at the clusters more closely, some interesting differences get clear. The cluster production, product life circle &amp; flow production, which is completely missing in the English graph, is the one with the higher number of nodes in the German one. This shows how the topic of Industry 4.0 in Germany is much more concentrated on production than elsewhere. In the study of acatech (Kagermann 2006), it is also explicitly stated that in Germany “ […] the focus is on optimizing production processes in terms of quality, price and flexibility and delivering better financial returns overall”. The focus on production for Germany is also confirmed by a monitor paper written by the by the European Commission (Comission 2016). To give a clearer view of this phenomena, we have to consider that the word production is very similar in meaning to manufacturing. The cluster including the latter (internet of things &amp; manufacturing) has more than the double number of nodes in the English graph with respect to the German one, so it might be that it is rather the wording that makes the difference. Anyway, by looking at the the disposition of the cluster (see figure 9.12) internet of things &amp; manufacturing in the two graphs, it can be observed that in the English version the cluster is strongly integrated with ITC-related topics (Data processing &amp; Analytics, Cognition and Cloud Computing), adding the adjective cyber to the topic of manufacturing. Table 9.6: Clusters names and number of nodes per cluster for each version of the Industry Graph 4.0. Cluster Number Cluster name Number of Nodes (German Version) Number of Nodes (English Version) 1 Internet of things &amp; manufacturing 12 28 2 Cognition 23 7 3 Data, processing &amp; analytics 23 19 4 Automation 12 20 5 Cloud computing - 21 6 Production, product life circle &amp; flow production 27 - Additionally, also automation has more entries in the English version and is highly connected not only to internet of things &amp; manufacturing, but also to could computing, which does not even exist in the German version of the graph: it is well known that Germany is not at the front when it comes to the integration of ICT in Industry 4.0. This found is again confirmed by the study of Acatech (Kagermann 2006), where it is said that “Germany is currently lagging behind with regard to data-driven business models and the development of large platform ecosystems”. The fact that cloud computing does not show up in the German graph is examined more closely. In fact, a Wikipedia page about this topic does exist in the German version of Wikipedia, but is not linked to the Industrie 4.0 page. As stated in section 4.3, contributors of the German version of Wikipedia tend to add only the most important connections between pages. This again shows that integrated ICT is not the most important topic in Germany now. Another great difference in the number of nodes is in the cluster cognition, which seems to be more important in the German version of the graph. The German Wikipedia page describes this as a processing and rearrangement of information between a human and a system. Acatech study (Kagermann 2006) does the same observation: “In Germany in particular, the focus is on integrating information, communication and manufacturing technologies”. This could explain even better the finding that the German graph lacks on the ICT side, giving a new interpretation: in Germany there is a different interpretation of the meaning of cognitive computing with respect to the rest of the world. Finally, while the group of data, processing and analytics nearly has the same amount of nodes, the links of this cluster with the other clusters is quite different in the two graphs and also its content changes from on nation to the other. Words like big data does not even exist in the list of German pages related to industry 4.0 where it is limited to just data. This emphasizes the careful approach of Germany with this topic, which was also observed in (Wee et al. 2015). Germany seems to be rather conservative when it comes to new emerging technological trends or even buzz-worlds like Big Data. References "],
["social-media.html", "Chapter 10 Social Media 10.1 Technical Sentiment Analysis", " Chapter 10 Social Media The technical knowledge generated by Social Media remains largely untapped. Some researchers proved that social media have been a valuable source to predict the future outcomes of some events such as box-office movie revenues or political elections. Social media are nowadays also used by companies to measure the sentiment of customers about their brand and products. This chpater proposes a new social media based model to measure how users perceive new products from a technical point of view. This model relies on the analysis of advantages and drawbacks of products, which are both important aspects evaluated by consumers during the buying decision process. This model is based on a lexicon developed in a related work (see chapter 7.2) to analyse patents and detect advantages and drawbacks connected to a certain technology. The results show that when a product has a certain technological complexity and fuels a more technical debate, advantages and drawbacks analysis is more efficient than sentiment analysis in producing technical-functional judgements. 10.1 Technical Sentiment Analysis Nowadays, social media have become an inseparable part of modern life, providing a vast record of mankind’s everyday thoughts, feelings and actions. For this reason, there has been an increasing interest in research of exploiting social media as information source of knowledge although extracting a valuable signal is not a trivial task since social media data is noisy and must be filtered before proceeding with the analysis. In this domain, sentiment analysis, which aims to determine the sentiment content of a text unit, is considered one of the best data mining method. It relies on different approaches (Collomb et al. 2014) and it has been used to answer research questions in a variety of fields comprised the measure of customers perception of new products (Mirtalaie et al. 2018b). In this section, we try to understand if sentiment analysis is really the best available method to analyse consumer’s perception of products, expecialy when we want to measure the perception of the technical content of the product. Thus we compare State of the art sentiment analysis techniques with a lexicon of advantages and drawbacks related to products. 10.1.1 Methodology Our work started with the selection of an event able to polarise Twitter users’ attention and products to analyse. In particular, we chose a premiere tradeshow for the video game industry, and two video game consoles disclosed during the event. We collected about 7 milions tweets about products published before, during and after the tradeshow. Since social media data is noisy (for example it may contains spam and advertising), before proceeding with the analyses, we filtered our dataset. In particular, after removing too short and non-English tweets, we manually classified a randomly extracted subset of posts to train a classifier which provide us the cleansed dataset. Then we conducted a sentiment analysis of the tweets using state of the art machine learning techniques. We classified each tweet as positive, negative or neutral. At this point we applied our lexicon identifying advantages tweets and drawbacks tweets. Finally we compared the outputs of the two analyses for the two product-related clusters of tweets. We found consistent differences between the extractions. The results shows that when a product has a certain technological complexity and fuels a more technical debate, advantages and drawbacks analysis is more able than sentiment in producing technical-functional judgements. For this reason we think that the proposed methodology peforms better then standard sentiment analysis techniques when a product has a certain technological complexity and fuels a more technical social media discourse. Selection of a triggering event and products We chose the Electronic Entertainment Expo as event able to polarise users’ attention. Commonly referred to as E3, it is a premier trade event for the video game industry, presented by the Entertainment Software Association (ESA). We chose two new video game consoles, disclosed at E3 2017, as products of which predicting the success or failure. The first is Xbox One X, a new high-end version of Xbox One with upgraded hardware and the other product is New Nintendo 2DS XL, a streamlined version of the handheld console New Nintendo 3DS XL. Data collection Twitter provides two possible ways to gather tweets: the Streaming Application Programming Interface (API) and the Search API. The first one allows user to obtain real-time access to tweets from an input query. The user first requests a connection to a stream of tweets from the server. Then, the server opens a streaming connections and tweets are streamed in as they occur, to the user. However, there are a few limitations of the Streaming API. First, language is not specifiable, resulting in a stream that contains tweets of all languages, including a few non-Latin-based alphabets, that complicates further analysis. Instead, Twitter Search API is a Representational State Transfer API which allows users to request specific queries of recent tweets. It allows filtering based on language, region, geolocation, and time. Unfortunately, using the Search API is expensive and there is a rate limit associated with the query. Because of these issues, we decided to go with the Twitter Streaming API instead. For each product, we detected related hashtags and keywords an constructed a query to download relevant tweets. We chose to collect tweets not only after the tradeshow, but also before. For these reason, we initially identified some products keywords with their provisional names and we updated them at a later stage. Tweets have been downloaded from CNR (Consiglio Nazionale delle Ricerche, Istituto di Informatica e Telematica, Area di Pisa) since 11th June 2017 h. 10:00 to 31th July 2017 h. 15:00. Data filtering The initial dataset resulted to be very noisy, containing tweets written in different languages, advertising and posts related to different products or subjects. We chose to keep into account only English tweets because sentiment and advantages/drawbacks lexicon is in this language. The data set is filtered removing tweets with less than five words and non-English posts with a language classifier. We obtained 7.165.216 of filtered tweets. At this point we created a golden set of relevant tweet to train a Supported Vector Machine classifier able to recognize relevant and unrelevant tweets. We defined characteristics that make a tweet: (i) relevant (posted by users or containing words or opinions related to our products of interests and their functionalities), (ii) irrelevant (tweets containing advertisings, links to e-commerce websites or messages related to other products or subjects). A researcher manually classified a subset made up of randomly extracted tweets. In particular, we exctract a subset composed of 6.500 finding 105 positive results and 6.395 negative. SVM model was then trained using this dataset, and computed a probability for each tweet to be relevant or irrelevant. A threshold of 0.7 has been chosen to label a tweet as relevant. The final dataset of filtered tweets, made up of 66.796 posts. We clustered tweets using product-related keywords. Clustering posts allowed us to further filter the final dataset which contained a small number of irrelevant tweets (Table 10.1). Table 10.1: Clusters of tweets. N° of tweets % of tweets Xbox One X 64885 0.9714 New N2DS 1706 0.0255 Irrelevant tweets 198 0.0030 Sentiment analysis Table 10.2 presents the results of the sentiment analysis. We classified each tweet according to its sentiment into positive, negative, or neutral. We used an established methodology developed by (Cimino et al. 2014). We pre-processed the tweets by removing mentions (@ character), URLs, product hashtags, emoticons and single characters. As a result, for each tweet we obtained a probability of belonging to a mood class. After a manual analysis, we used a class prediction probability threshold of 0.6 to filter out low confidence prediction, i.e. tweets that cannot be classified as positive or negative with a high confidence are classified as neutral instead. Table 10.2: Clusters of tweets. Positive Negative Neutral Xbox One X 0.3599 0.0465 0.5937 New N2DS 0.5299 0.0158 0.4543 Overall 0.3642 0.0457 0.5901 Advantages and drawbacks analysis To extract technical advantages and drawbacks from tweets we used the lexicon developed in chapter 7.2 that contains 657 Advantages words and 297 Drawbacks clues. These words are searched on our dataset finding different percentages of tweets with words from the lexicon in the two product-related clusters of tweets. Table 10.3 reports the results. Table 10.3: Percentages of tweets containing or not words from our lexicon. Tweets with adv Tweets with drw Tweets with adv &amp; drw Tweets with no adv or drw Tweets with adv or drw Xbox One X 0.0884 0.0374 0.0037 0.8705 0.1295 New N2DS XL 0.0662 0.0094 0.0000 0.9244 0.0756 10.1.2 Results We adapted the advantages &amp; drawbacks analysis to give as output a classification ef each tweet. We classified data coming from the latter analysis in this way: positive (tweets containing only advantages words) negative (tweets containing only drawbacks) neutral (tweets with no words of our lexicon or controversial tweets) As we can see in figure 10.1, sentiment analysis is more able to polarise tweets. In fact, with this analysis we found lower levels of neutral tweets, respectively 59.37 % for Xbox One X and 45.43% for the New Nintendo 2DS XL. Figure 10.1: Comparison between Sentiment analysis and Advantages/Drawbacks analysis. This was an expected result since this kind of analysis is designed to deal with colloquial language while our lexicon is technical, being derived from patents analysis. What surprised us is the different polarisation of the products that we see comparing the two analyses. In fact, while with sentiment analysis Nintendo achieves lower percentages of neutral tweets, with advantages and drawbacks analysis is the opposite, since Xbox tweets are more polarised. We also noted that we found more tweets with words of our lexicon in the Xbox subset than in the Nintendo one (10.3). We did the hypothesis that the differences between the percentages of tweets with words found for each product, and the differences of polarisation between the two analyses depend on the different marketing focus, target customer, and technological complexity of the two new video game consoles. Xbox One X targets hard-core gamers who really wants a premium experience.13 With its marketing campaign, Microsoft pushed the technical supremacy of its new machine over the competitors’ products, fuelling a debate about its technical features amongst the potential users. As a result, the campaign produced a more technical social discourse that allowed us achieving better results. Instead, the new Nintendo handheld console has been developed targeting children and families providing a model that falls somewhere in the middle of the line of 3DS consoles.14 We initially checked our hypothesis using Google Trend to compare users’ search interest about technical review of the two products during the data collection period (Figure 10.2). Then, we analysed the number of technical articles related to the new products published by the 25 most popular video games and technology websites in the U.S, according to the ranking of SimilarWeb, a digital marketing intelligence company which publishes insights about websites. We entered the queries \\[\\begin{equation*} allintitle: (4k OR hdr OR hardware OR graphics OR review OR resolution OR fps OR fast OR comparison OR frame OR enhanced OR performance OR cpu OR gpu OR ram) AND (&quot;xbox one x&quot;) site: ign.com \\end{equation*}\\] and \\[\\begin{equation*} allintitle: (graphics OR review OR screen OR comparison OR enhanced OR performance OR cpu OR gpu OR ram OR battery OR weight) AND “new nintendo 2ds xl” site: ign.com \\end{equation*}\\] into Google search engine to retrieve technical article within the web domains previously identified: we obtained 1.117 articles about Xbox and only 52 about Nintendo, proving that technical debate concerning Xbox is greater. This is and evidence of the fact that when a product has a certain technological complexity and fuels a more technical debate, advantages and drawbacks analysis is more able than sentiment in producing technical-functional judgements. The greater number of neutral tweets found with advantages and drawbacks analysis can also be explained with the Means-end chain model (Reynolds, Gengler, and Howard 1995). Consumers express themselves basing on personal consequences linked with product use or basing on personal values satisfied by the product itself. For these reasons, tweets contain a more colloquial language which sentiment analysis is more able to interpret than the latter tool. Figure 10.2: Google Trends comparison of search-terms “Xbox One X review” and “New Nintendo 2DS XL review” during the data collection period, since 11th June 2017 to 31st July 2017. Values on the vertical axis depict search interest compared to the highest point in the graph during the observation time. A value of 100 is the peak popularity for the term. On average, users searched for Xbox reviews with an approximately five times higher frequency. References "],
["marketing-the-user-side-of-competition.html", "Chapter 11 Marketing: The User Side of Competition", " Chapter 11 Marketing: The User Side of Competition "],
["research-and-development-enriched-dictionaries-for-innovation.html", "Chapter 12 Research and Development: Enriched dictionaries for Innovation", " Chapter 12 Research and Development: Enriched dictionaries for Innovation "],
["policy-making-impact-of-research-from-the-perspective-of-users-.html", "Chapter 13 Policy Making: Impact of research from the perspective of users. 13.1 Methodological challenges 13.2 Methodology 13.3 From text extraction to indicators 13.4 Data 13.5 Results 13.6 Discussion", " Chapter 13 Policy Making: Impact of research from the perspective of users. A substantive interest has been developed in the last 15 years on the so-called “impact revolution”, namely the increasing demand for showcasing the results of publicly funded research in order to justify public expenditure. Public funders are increasingly required to demonstrate the relevance of funded research not only for scientific communities but also for the economy and society at large. In other words, there is an increasing demand to prove that the users and beneficiaries of research results are not only the traditional academic audience - researchers and university students - but include a large number of social actors. Let use use here the concept of “societal impact”, as opposed to academic impact, to include all dimensions of impact on the society and economy that are realized through impact pathways that go beyond the institutional research and teaching activities. The issue of societal impact of public research has gained prominence in the specialized literature since the start of the century and has accelerated in recent years (Van der Meulen and Rip 2000, Ernø-Kjølhede and Hansson (2011), Bornmann (2013), Bornmann and Marx (2014), Bornmann and Haunschild (2017)). Indeed, a quick look at the most important journals in the field of innovation, research policy and research evaluation shows that the most largely cited, downloaded or read articles in the last five years are almost invariably dedicated to the issue of societal impact. This follows from the adoption of societal impact of research as one dimension of evalution of research, both ex ante and ex post, in many advanced countries. As discussed by several authors, societal impact has become one of the criteria of ex ante project selection in several institutions and countries (Kanninen and Lemola 2006, Dance (2013)). It is also a crucial chapter in the ex post research assessment in some countries, such as United Kingdom. Within the UK Research Excellence Framework (REF) the assessment of societal impact has been responsible for 20% of the total score, while an increase to 25% has been announced in September 2017 for the future exercise. The publication of REF case studies of societal impact has fueled a field of analysis (Derrick, Meijer, and Van Wijk 2014, Samuel and Derrick (2015), Khazragui and Hudson (2014)). Some authors advocate impact analysis as a way to examine the effects of research agenda on the societal priorities (Cozzens, Bobb, and Bortagaray 2002). This surge of policy interest, however, comes in a period in which the scientific analysis of the concept of societal impact and of the potential and limits of existing methodologies has not yet come to a general agreement. As succinctly stated by Lutz Bornmann, impact evaluation is “still in the infant stage” (Bornmann 2013). And Bozeman and Sarewitz (Bozeman and Sarewitz 2011) explained that “there has been remarkably little progress in the ability to measure directly, systematically, and validly the impacts of research on social change”, so that “we have no satisfactory analytical tools for characterizing the social impact (of research)” (Bozeman and Sarewitz 2011). This paper is a contribution to the substantive and methodological work on the assessment of societal impact of research. From the substantive point of view, it introduces the notion of target group, or group of potential users of research, as a necessary component of the design and implementation of research projects. From the methodological point of view, the paper strongly supports the idea, already advanced in the literature, that Text mining techniques are promising in this field, but suggests a major modification by introducing the Enriched dictionary methodology. To be more precise we argue that a necessary component for impact assessment is the definition of users of research at a granular level. In addition, we suggest that the more researchers are able to define precisely their target groups the more they are likely to reach them effectively and to increase the impact. We develop a full scale, replicable and scalable methodology to identify the user groups mentioned in research-based texts, such as research proposals (ex ante), impact case studies (ex post), or publications. We test the methodology on the collection of case studies developed under the Research Excellence Framework (REF) in the United Kingdom. We examine three main dimensions of user target groups (frequency, intensity and specificity) and disaggregate the data by broad discipline. 13.1 Methodological challenges 13.1.1 Variability in the identification of outcomes and users A first methodological issue is that in order to assess the societal impact of research, there is a need not only to identify observable elements that can be considered as an outcome of the research process, but also to define the actors that are affected, or benefit, from those outcomes. It turns that these elements are subject to huge variability across disciplines. Consequently, there are areas in which methodologies are more sophisticated and largely tested, and others in which there is remarkable lack of experience and methodological work (Stern et al. 2013, Mitton et al. (2007), Ţurcan (2015)) Among the former the health care sector is probably the one in which the impact assessment of research has made the largest progresses: a number of well structured research impact assessment methodologies have been developed and implemented. There in fact are as many as 16 different impact assessment models, according to Milat, Bauman and Redman (Milat, Bauman, and Redman 2015). An important reason for this accumulation of experience and knowledge is that the outcomes that demonstrate the expected impact are clearly identified and standardized, and the categories of users are clearly observed, given a high degree of professionalism. At the opposite side of the spectrum, there is still much uncertainty about the way in which the societal impact of research in social sciences and humanities (SSH) could be defined and observed, even less quantified and measured. The challenges associated with identifying the impact of outputs from these fields stem from a number of issues, most of which have been noted in earlier evaluation-based literatures. According to certain theoretical perspectives the very notion of impact is problematic for SSH (Blasi, Romagnosi, and Bonaccorsi 2018). From a historical perspective, it has become increasingly clear that research across SSH has had a large influence on modern societies on a long time scale (Bod 2013). It should be recognised therefore that a certain share of research need not be asked to demonstrate any impact, but be valued for its own sake (Small 2013). It is part of the millennial history of humankind that some people, some ages of life, some resources are dedicated to the search for intangible and priceless goals such as beauty or truth. Research from the arts and humanities is needed in order to preserve in society the ability to interpret, appreciate, enjoy and valorize symbolic values inherited from the past. Should the many scholars from this field be interrupted or deprived, modern societies would rapidly become unable to coordinate, administer and govern themselves. Consider the problem from the perspective of potential users of SSH research: the results or products are not necessarily used on the basis of direct access to scientific sources (as it happens more frequently with technological and biomedical research), but after some transformation and intermediation by specialised actors (e.g. journalists of popular magazines; social media). Furthermore they do not necessarily take the form of compelling evidence, or ultimate scientific authority, but enter into a social arena for public and political conversations and debate, where arguments may be advanced and refuted. In addition, audiences may be dispersed, non-institutionalized, or even transient (e.g. issue-based) and not professional. Finally, social behaviors are by definition slow to change, thus the impact of research is likely to be seen only after a long delay. This means that both outcomes and categories of users are much more difficult to identify and define. 13.1.2 Sources of information Current approaches can be classified, according to Morton (Morton 2015), as forward tracking, backward tracking, and evaluation of mechanisms. In forward tracking, researchers are asked to reconstruct the ways in which their research might be useful for given categories of users. Alternatively, users are asked to declare which kind of research results they are likely to utilize (Tang et al. 2000). The strong limitation of this approach is that it relies heavily on the researcher’s and research user’s own recollections of research use (Nutley, Walter, and Davies 2007, Donovan (2011)). In some sense, this is also the limitation of the use of case studies of research impact: it is difficult to verify whether they are a random collection or they are biased in one way or another. Backward tracking suffers less from these subjective biases. If the sample of final outcomes is well designed, it can offer important lessons for researchers and policy makers. However, it comes with long delays with respect to actual research results. It should be noted that this methodology is the one largely adopted in impact assessment of health-related research: once it is agreed that clinical guidelines are a suitable candidate for assessment, it is possible to trace back the impact using the citations to the medical literature. Evaluation of mechanisms is a partial methodology, which describes in great detail the pathways in which research results are channeled from their origin to the endpoint. 13.1.3 Text-based impact assessment More recently, an interesting alternative approach has been suggested. Based on the availbility of Machine learning and Text mining techniques, it has been argued that the evidence for the impact of research might be traced by extracting selected expressions from certain kinds of documents. We may distinguish between two kinds of documents: (a) produced by users of research; (b) produced by researchers themselves. There are several suggestions to use documents produced by users of research. In one of the most developed efforts to conceptualize the social impact of research, called Public Value Mapping (PVM) Bozeman and Sarewitz (Bozeman and Sarewitz 2011) suggested the use of three main sources of statements, from which it could be possible to trace the impact of research: government statements, academic literature, and public opinion polls containing public statements. More recently, Bornmann, Haunschild and Marx (Bornmann, Haunschild, and Marx 2016) have suggested to use the frequency of occurrence of policy-related words in policy documents as evidence of impact of research. In this paper we will make these suggestions operational. Yet another approach in the same line is to examine the documents produced within social media. A prominent approach is based on Altmetrics measures. The main tenet of Altmetrics is that citations, the basic unit of analysis of bibliometrics, capture only part of the impact of published research, so that “citation tracking has never been able to follow the less visible- but often more important- threads of invisible colleges, woven through personal connections and informal communications” (Priem, Piwowar, and Hemminger 2012). By accessing data on the personal use of published materials “Altmetrics could deliver information about impacts on diverse audiences, like clinicians, practitioners, and the general public, as well as help to track the use of diverse research products like datasets, software, and blog posts” (ib.). Sibele et al. (Fausto et al. 2012) examine in this light the phenomenon of research blogging. There are however severe limitations that might make Altmetrics problematic. The extent to which Altmetrics can capture traces of societal impact has recently been seriously contested (Bornmann and Marx 2014). Social media are used more for internal discussions within scientific communities, rather than a bridge between the research community and society at large. According to Haustein there is lack of evidence that social media events can serve as appropriate indicators of societal impact (Haustein et al. 2016). Among the documents produced by researchers, we might further distinguish between: (a) research proposals (ex ante documents) and (b) case studies produced after the realization of research projects (ex post, or documents produced within the research assessment process). It is this type of documents we suggest to examine as a novel methodology to assess the potential societal impact of research. In this paper we follow type (b) documents and use the collection of case studies produced by UK researchers under the REF exercise. In future studies we plan to use archives of proposals made available in public sources in order to examine the ex ante representation of researchers. The REF impact case studies, as already noted, have been the object of a large literature in recent years. Among these studies, King’s College and Digital Science (London and Science 2015) have indeed produced, using Text mining techniques, an interesting analysis of beneficiaries of UK research, publishing a fascinating infographics. This analysis, however, lists only a fairly small set of research users, most of which are defined with generic terms. It is our contention that much further work should be done in this direction. We propose a new lexicon-based methodology, called Enriched dictionary (see below), which allows a much more fine-grained analysis. 13.2 Methodology Basically we suggest to examine carefully the full text of documents produced by researchers and extract, in a highly structured and theory-dependent way, information on potential users of research. Users are defined as categories of human agents that share some characteristics that are relevant with respect to the object of interest. In the present context users are social groups, or target groups, that are potentially affected by research results and that use these results for their own purposes. Before entering into a technical description of the methodology let us address the rationale of assuming users as an important dimension of research impact. There are several compelling theoretical reasons for this choice. First, the literature has strongly underlined the interactive nature of societal research impact. As discussed above, the most recent literature and practice strongly suggest to abandon a unilinear model of impact, in which it is expected that researchers produce results, diffuse them in various channels, and see the results taken up by interested users. Let us call this approach a “percolation model”: researchers produce results that eventually percolate down into society, but without knowing the ways in which they flow, the obstacles they meet, the timing of the process, or the final destination of the flows. On the contrary, it is strongly suggested to adopt an interactive model of interaction, in which researchers actively engage into systematic relation with potential users. In an interactive model there must be a reflexive activity on one side about the nature (characteristics, interests, behavior, style) of the other side. Researchers must build up a representation of their potential users, and vice versa. How could researchers engage with users if they do not know them? And how they could know potential users if they do not engage into some sort of analysis, even as simple as description and characterization? For interaction to take place, there must be some preliminary recognition of the existence, nature, attitudes of those that may utilize the research results. According to our methodological suggestion, it is this representation that is the preliminary object of interest for impact assessment. If researchers have a representation of their potential users, they will leave traces of this representation in their written texts. When they write research proposals they will promise to address the issues of these users, and when they write case studies of research impact they will report on the takeup or use of their research activities by these users. Second, it has been shown that research activities have a huge variety of impact pahways, largely dependent on the scientific discipline. In turn, this implies that disciplines have at least partially different target groups. Research in political science is different from research in oncology not only because their scientific foundations, methods, objects and cognitive styles are different, but also because they talk to different user groups. The texts produced by researchers themselves are a necessary starting point to reconstruct the various impact pathways. Third, focusing on user groups has the advantage of shifting away the attention from discrete events or products to long term processes of interaction between research and society. The focus on discrete events or products is a typical feature of the narrow definition of research impact cultivated since long time in the so called “valorization of research”. This impact is defined and measured with reference to highly stylized entiities, such as patents, licensing contracts, research contracts, and spinoff companies. These are clearly defined, legally enforced, highly visible and measurable entities. Defining and measuring impact is easier by focusing on these entities because they convey the meaning of knowledge transfer from research to the market, and because the final outcome can be defined in monetary terms. We suggest to focus on user groups as a relatively permanent social entity, which is defined by a specific combination of social status, needs, culture, practices and routines. User groups survive the individual personality of people. They are a permanent, although often entirely informal, characterization of society. Finally, our methodology allows the large scale automatic analysis of large corpora. This means that the inevitable subjectivity in the reconstruction of impact by researchers in writing their proposals and/or impact case studies can be mitigated by examining large scale patterns. It is important to remark that the notion of users is consistent with other suggestions in the literature that adopt different definitions, such as stakeholders, constituencies, interest groups. Our definition is broader and admits more internal variability, as discussed below. 13.2.1 Operationalizing user groups using Natural Language Processing techniques A simple implication of our methodology is that researchers “leave a trace” of their representation of users, or the groups of social agents that are most likely to use or uptake the results of their research activity. By using state-of-the-art Text mining technologies we are able to identify these traces in written texts and to give them unambiguous meaning. By assuming target groups as units of analysis we suggest to introduce a number of concepts, from which suitable indicators can be derived. Definition 1 Stakeholders are entities influenced by the research activity. This definition covers all possible entities that engage an active or passive relation with the research activity. Definition 2 Target groups are entities or groups of entities on which researchers claim to have an effect. Given definition 2, it is clear that every target group is also a stakeholder, while the reverse does not hold true. Non-target stakeholders include the proponents themselves, managing autorities, funding agencies and so on. We need a formal technique for identifying target groups in the text of research documents. This technique as been developed as described in section 7.1. Dictionaries are a peculiar type of written text, characterized by authoritativeness, saturation and update. In other words, a dictionary must be composed of entries established by some authority, most often an academic one and/or an authority established since long time by reputation (e.g. editorial initiatives of prestigious publishers). Saturation means that all words that are related to the domain of the dictionary must be included. It is a major flaw of a dictionary the lack of important entries. A dictionary is characterized by a property of semantic saturation: all words that have a meaning associated to a given field are included in the dictionary. Using this tool it will be possible to count the occurrences of target groups, and develop indicators of frequency and intensity. Finally, update means that dictionaries have an internal organization (for example, an editorial board) that examines all new expressions, discusses their acceptability in the dictionary, and make official and authoritative decisions about inclusion or exclusions. These formal requisites, that used to be appropriate only for established dictionaries, are currently satisfied by a larger variety of sources. In particular, the huge power of Text mining techniques has made it possible to automatize at least some of the steps needed to create a formal dictionary. Section 7.1 illustrates the steps undertaken in order to build up an Enriched dictionary of users. It currently includes 76.857 entries, that have been shown to saturate the semantic field of users. It includes, among others, all jobs, work positions, professions, hobbys, patient roles, sports, creative and enterteinment roles, political, institutional and organizational roles, social roles, that have been classified in hundreds of official sources. In particular, this includes all stakeholders and target groups, as defined above. Our Natural Language Processing (NLP) system follows the following steps. - Sentence splitting and Tokenization: this process splits the text into sentences and then segments each sentence in orthographic units called tokens. Sentence splitting plays a key role since thanks to a given word, it is possible to find all sentences in which the word is used. - POS tagging and Lemmatization: The Part-Of-Speech tagging (or POS tagging) is the process of assigning unambiguous grammatical categories to words in a specific context. It plays a key role in NLP and in many language technology systems. Once the computation of the POS-tagged text is completed, the text is lemmatized according to the result of this analysis. - Target groups Annotation: The Target groups Extraction tool is based on lexicon methods. Among the various lexicon methods we adopt, as stated above, the Enriched dictionary approach. With respect to users, we use a lexicon composed of 76.857 entries. By launching this Extracion tool we are able to capture all the different ways in which each target group can be expressed in a research document. Table 13.1 shows the output of the NLP procedure for a sentence contained in the corpus (“Each year, in England alone, approximately 152,000 people suffer a stroke.”). As it can be seen, the automatic annotation system isolates the only word (“people”) that may be part of a target group. Table 13.1: Tokenization, lemmatization and annotation of a sentence in the corpus. Doc_id Sentence_id Token_id Token Lemma Xpos Full_target_group 1855 1 1 Each Each DT NA 1855 1 2 year Year NN NA 1855 1 3 , , , NA 1855 1 4 in in IN NA 1855 1 5 England england NN NA 1855 1 6 alone alone RB NA 1855 1 7 , , , NA 1855 1 8 approximately approximately RB NA 1855 1 9 152 152 CD NA 1855 1 10 people people NN People 1855 1 11 suffer suffer VBP NA 1855 1 12 a A DT NA 1855 1 13 stroke stroke NN NA 1855 1 14 . . . NA 13.3 From text extraction to indicators After having extracted all possible expression of target groups in research documents, we are in a position to develop indicators with suitable statistical properties. They are defined as follows. Frequency For a given document J, let us define T_j (number of target groups contained in J) and W_j (number of words contained in J). We then define: \\[\\begin{equation*} F_j= \\frac{T_j}{W_j}*100 \\end{equation*}\\] The frequency F of a document measures the percentage of words that are target groups. If a document shows high frequency it means that it cites many times target groups, even if it/they are always the same and they are generic. For example, an impact description that repeats many times the target group people will show high frequency. Diversity For a given document J, let us define Tu_j (number of different target groups contained in J) and Wu_j (number of different words contained in J). We then define: \\[\\begin{equation*} D_j= \\frac{Tu_j}{Wu_j}*100 \\end{equation*}\\] The diversity D of a document measures the percentage ratio of different words that are target groups. If a document has a high diversity it means that it cites many different target groups, even if they are generic. Specificity For a given target group i, let us define N (number of document in the corpus) and ni (number of documents that contains the target group i). We then define Si, the specificity of the target group i as: \\[\\begin{equation*} S_i= \\frac{log(N/n_i)}{log(N)} \\end{equation*}\\] The Specificity of a target group Si measures how rare, and thus specific, is a target group in the overall corpus. The specificity diminishes for target groups that occur very frequently in the corpus and increases for target groups that occur rarely (more specific target group). Let us take the example before, in which the annotation system identifies people as a target group. There will be a large amount of documents in which the word people will occur, so that the ratio between the total number of documents in the corpus and the number of those that include the word people will be close to one. On the contrary, highly specific words (say, free climbers) will occur less frequently, so that the above ratio will increase. Since we are interested in giving a measure for each document, having defined Si for each target group, if the document j contains k different target groups, we have that the specificity of the document j is: \\[\\begin{equation*} Sj=\\sum_{i=1}^{k}Si,j/k \\end{equation*}\\] The specificity of a document Sj measures how rare, and thus specific, are the target groups contained in that document and it is the mean of the specificity of all the target groups that it contains. If a document contains only rare target group (not cited by other impact descriptions) that document exhibits high specificity. For the previous example, suppose we have a document repeating many times that the research has an impact on people. Since the target group people is a common one (thus having a low specificity Si itself) the measure of the specificity of the document, resulting from the sum of low specificity values, will be low. An interesting example of application of these principles comes from the scientific study of popular science, or divulgation. In these fields authors use a language which must be understood by lay people, not by professional scientists. For this reason they tend to use generic words, rather than highly specific and professional words. This is, incidentally, one of the reasons why professional scientists ofte disregard popular science as a literary genre: they perceive the generic nature of language used as too coarse. Scholars of popular science have used quantitative linguistic techniques to distinguish between generic and specific terms in the same semantic field (Jacobi 1999) 13.3.1 The meaning of Frequency, Diversity and Specificity indicators for the analysis of research impact The above definitions are building blocks of a model of engagement of researchers with their potential users. At the outset, it is important to examine whether researchers include users in their representation of research activity at all. Thanks to the use of an Enriched dictionary, which by definition saturates the semantic field, we are in a position to establish whether user-related expressions are found in researchers’ texts or not. Since in this paper we use the impact case studies produced under the REF, the minimum level is by definition satisfied, as it was the condition for submitting the case study. At the same time, this indicator will be extremely useful for the ex ante evaluation of research proposals. The appropriateness of this indicator and its policy implications will be the object of future research. Authors of the REF documents are by definition aware of the existence of target groups of users. Once the awareness level is satisfied, frequency comes into play. Frequency is a standard measure in computational linguistics and Text mining techniques, since it gives evidence of the relative importance of words or expressions. The frequency by which target groups are mentioned in a document is a measure of their perceived importance. We are keen to examine the frequency by which users, or target groups, are mentioned in documents produced by researchers. Yet researchers may cite repeatedly a target group, but consider them as a unique entity. This approach is reasonable when the target group does not have internal differentiation (i.e. it is not segmented) and when the results of the research are equally useful for all its members. But in many relevant cases this undifferentiated approach does not work. Representing and addressing users as a single target group may weaken the potential for impact. There are two directions in which researchers can deepen their representation of target groups, and hence their impact. One is to address different target groups. This is similar to the notion of segmentation in consumer psychology: if target groups have sufficiently dissimilar characteristics with respect to the activity (in this case, the use of reserch results), then it is better to treat them differently. The other is to go in depth in addressing each of the target group, by refining their approach, using fine grained representations of the needs of the users. We capture these two directions by measuring diversity and specificity. It is our contention that the language used by reseachers is a clear signal of their approach to research impact. A high level of diversity implies that researchers understand the need to mention, identify, enumerate target groups that have different names. They stop using generic words (say, people) and start to introduce some of the many criteria for segmentation. In addition, or in alternative, they discover that within their target groups it is possible to go deep in the fine grained representation, by adding specificity. A spatial metaphor may help to capture the point: by increasing diversity of target groups researchers move horizontally, defining new regions of the space, while by increasing specificity they move vertically, drilling the ground in each of the regions. Frequency, diversity and specificity are not necessarily correlated. An interesting empirical issue is the relation between these two dimensions. Let us articulate an example. Suppose an expected impact of a given research is on policy making. A low specificity situation arises when researchers speak about “policy makers”, or “government”. A more mature and engaged approach should articulate the policy making process by identifying several specific user groups in addition to the various layers of political and legislative decision making. For example, interest groups. These social actors are extremely important in shaping the policy agenda. A well developed body of research in political theory has examined the way in which new policy issues are generated, framed in the public conversation, and pushed forward in the policy arena until they become established in the policy agenda (Sabatier, 1987). Pittman (2006) argues that the most important factors leading to government interest there is the role of domestic advocacy, as well as the interest in their international standing. Second, intermediary organizations or boundary organizations, such as technical agencies and regulatory bodies (Agrawale, Broad and Guston, 2001). Third, opinion-makers such as think tanks should be included. Many other examples could be added. These actors would be mentioned with more specific expressions. Finally, researchers may engage into identifying user groups “by name and surname”, that is, as concrete and localized actors with whom they plan to enter into interaction. Counting or measuring them would be the final stage of maturity of research engagement with users. 13.4 Data 13.4.1 Description of the corpus The corpus is composed of 6637 REF impact case studies. They generally follow a template illustrated in the REF criteria. The template has a Title and five main text sections, plus the name of the Submitting Institution and the Unit of Assessment. In addition to the Title of the case study, the text sections of the template and the indicative lengths, as recommended in the REF criteria are: Summary of the impact, 100 words Underpinning research, 500 words References to the research, 6 references Details of the impact, 750 words Sources to corroborate the impact, 10 references We take into consideration the sections Summary of the impact and Details of the impact. It is common practice in computational linguistics to examine the length of documents to be included in a corpus in order to ensure comparability. Figure 13.1 shows that the limits established by the REF criteria are not always respected. Nevertheless, since the distribution of the length is almost normal and there are not outliers it is appropriate to include all documents in the corpus. Figure 13.1: Distribution of number of words in relevant sections of the REF impact case studies. Within the REF repository projects are classified using three criteria: Impact type: There are eight Summary Impact Types. These follow the PESTLE convention (Political, Economic, Societal, Technological, Legal, and Environmental) widely used in government policy development, with the addition of Health and Cultural impact types. Units of assessment (UOA): Institutions were invited to make REF submissions in 36 subject areas, called units of assessment (UOAs), each of which had a separate expert panel. Research subject areas: The REF Impact case studies are assigned to one or more Research Subject Areas (to a maximum of three) by text analysis of the ‘Underpinning research’ (Section 2 of the Impact case study template). This is a guide to text search that uses a disciplinary structure that is more fine-grained than the one in the 36 Units of assessment. Figure 13.2 shows the number of documents per Unit of assessment. Figure 13.2: Number of documents per Unit of assessment (UoA) in REF impact case studies. 13.4.2 Preliminary analysis of the corpus In this section we present a descriptive analysis of the content of the documents to give an evidence of two important facts: 1. The description of the impact contains target groups; 2. This information is enough to make a significant statistical analysis. The corpus contains 8.230.598 words in total and 141.705 different words. By annotating the entire corpus with the entries of the Enriched dictionary we find that the total number of words referring to target groups is 169.037, while the number of different target groups is 1830, or 1.3% of different words.The number of documents that contain at least one target group is 6628, or 99,9% of the total. Only for nine documents we were unable to locate any word referring to a target group. Figure 13.3 offers a vivid demonstration of the issue of specificity of words referring to target groups. As many as 37% of all projects include people, and as many as 25% mention company as isolated words. Among the top 20 occurrences we find extremely generic words such as public, community, individual, organization, user, or society. Slightly more specific are the words referring to the school or youth context (child, school, student, teacher) or the health context (patient, patients). In order to find more specific words we have to go further down the ranking. Please note that in all these cases these words do not appear in combination with other that might increase the specificity, but in isolation. Should the same word appear in combination with other more semantically connotated words, they would form a separate target group. As an example, the word people is considered part of a separate expression in the following examples: people with cystic fibrosis, people with primordial dwarfism, people with rheumatoid arthritis, ordinary people in extraordinary situation, people in senior management, people from different background, key policy people in uk government, specific community of people, young people in deprived community in Glasgow. Each of these expressions is considered as a separate target group and their Specificity is computed according to the formula above. Figure 13.3: Top 20 occorrences of words referring to target groups in the corpus of REF impact case studies. To see the difference in the use of words in sentences consider the following examples from REF case studies: “The validation of the exit poll forecast allowed people to see the power of social scientific methods, and may have helped them to establish a level of trust in evidence-based information” (generic use of the word people) and “Key components of this are nurturing people with cross-cultural understanding, diversity in thinking and global leadership skills” (more specific use). The NLP procedure developed for this analysis is able to accurately distinguish these situations. 13.5 Results 13.5.1 Descriptive analysis Table 13.2 offers a snapshot of the value of indicators calculated across the entire corpus. The minimum value of indicators (zero) is represented by the 9 documents without target groups. On average the REF impact case studies contain words that represent target groups as 2% of total words, and words that represent distinct target groups as 2.5% of different words. In absolute terms, the median REF impact case study contains 10 different words thet refer to target groups, repeated 22 times in total. Table 13.2: Descriptive statistics of indicators of target groups in the corpus of REF impact case studies. Indicator Minimum Maximum 1st quartile 3rd quartile Mean Median Frequency:indicator 0 9.2 1.2 2.7 2 1.9 Frequency: absolute value 0 115 14 34 25469 22 Diversity: indicator 0 7.6 1.8 3.2 2.5 2.4 Diversity: absolute value 0 34 7 14 10518 10 Specificity 0 1 0.59 0.731 0.659 0.656 There is not large variability in the number of different target groups, as the first and third quartile at 7 and 14 are close to the median value. Interestingly, all distributions are close to the normal. A small number of documents use words that refer to target groups with much larger frequency and diversity. The document with maximum use of target groups identifies as many as 34 different words or combination of words. In terms of repetition, the document with the largest number of words uses 115 times a word representing target groups. When coming to specificity, the mean value of 0.659 implies a good level of specificity, given the range of the indicator. Figure 13.4 shows the distribution of documents by indicators. Figure 13.4: Top 20 occorrences of words referring to target groups in the corpus of REF impact case studies. 13.5.2 Findings by subject area We disaggregate the indicators with respect to the subject areas, or the Units of assessment (UoA), according to the REF nomenclature. This analysis offers a new perspective on the way in which he various disciplines describe their impact pathways. We use the conventional boxplot representation. Figure 13.5 shows a surprising finding. On top of the ranking by frequency of occurrence of words that refer to target groups of users we find Humanities: not only Education (which by nature refers to children, students and teachers as user groups), but also Music and drama, Classics, Language and literatire, and History. Or, in other words, disciplines that are not oriented towards users, but rather cultivate the goal of knowledge per se. At the bottom of the ranking we find, again surprisingly, Engineering disciplines, with a number of specializations, and Economics, that is, disciplines that, on the contrary, have a pragmatic orientation towards various target groups of users. Figure 13.5: Boxplot of the Frequency indicator in the corpus of REF impact case studies by subject area (Unit of assessment) Figure 13.6 confirms a similar ranking by subject area when we use the Diversity indicator, with slight variations. In the case of diversity we find also Theology and religious studies, as well as Art and design. Almost all subject areas in Humanities consistently show up at the top when we investigate the frequency by which they mention users of research and the number of different target groups they are able to identify. This is a remarkable finding. It is true that this comes from documents that are themselves retrospective reconstruction of impact, but this feature applies to all subject areas in the same way. This finding sheds light on one of the controversial issues in the literature on impact evaluation, that is, the role of Humanities and Arts. It seems that one of the tenets of the argument that Humanities and Arts are not sensitive to the audiences, or users, of their research results, is simply false. When asked to reflexively reconstruct their impact, they are systematically able to mention their target groups. Figure 13.6: Boxplot of the Diversity indicator in the corpus of REF impact case studies by subject area (Unit of assessment) This comes at a cost, however. Humanities rank top in the Frequency and Diversity of target groups, but rank at the bottom when coming to their Specificity. Figure 13.7 shows an almost reverse ranking of subject areas when the indicator chosen is Specificity. At the top of the Specificity ranking we find Social Sciences, such as Economics, Politics, Anthropology and Law. This is another interesting finding. These disciplines have been able to target highly specific groups of users, following a process of academic specialization. They have low Frequency and low Diversity- that is, do not spend much emphasis on their target users, but have high Specificity- that is, they know whom to target. Large part of Humanities are found at the bottom: Psychology, Education, Philosophy, Language and literature, Art and design. Engineering disciplines are more scattered. Finally, by combining the three indicators it is possible to examine more closely the pattern of impact of subject areas. Figure 13.7: Boxplot of the Specificiy indicator in the corpus of REF impact case studies by subject area (Unit of assessment) Frequency and Diversity are highly correlated (13.8). Humanities rank top in both indicators and are located in the top right region of the graph. Almost all disciplines in Engineering and Economics and Econometrics lie at the opposite corner. When coming to Specificity, correlation with the other indicators is on the contrary extremely low and is negative (- 0.016 with Intensity, - 0.146 with Diversity). Figure 13.8: Scatterplot of the relation between Frequency and Diversity indicators in the corpus of REF impact case studies by subject area (Unit of assessment) 13.6 Discussion The data show an intriguing disciplinary pattern: Humanities and Arts show remarkably higher frequency of terms related to users, but a significantly lower specificity. The opposite is found for many areas of STEM, namely Engineering and several Natural science disciplines. The results for Humanities are very intriguing. Research in Humanities is often considered as pure, abstract, not engaged with society. This representation is used by those governments that argue that research funding in these areas should be cut because their impact on society cannot be demonstrated. For Social Sciences the situation is slightly different, but there is a presumption that only a few social sciences have an impact, in particular those with instrumental value, such as Economics. Our data tell a different story. When asked to demonstrate the impact of their research, scholars in Humanities and Arts use a very rich vocabulary of users and mention users very frequently, twice as much as scholars in STEM. At the same time, they are less capable to transform their orientation in operational terms, by definining, identifying, targeting specific groups or audiences. At the opposite extreme, it seems that scholars in STEM mention very precise and well defined groups of users. This might be a result of the nature of their studies: researchers in Medicine follow specific targets in terms of disease and patients, or technologists have narrow industrial applications. This remarkable difference may create an imbalance in the assessment of research impact. Research in STEM finds more easily and unambiguously the groups of potential users, so that its impact is easier to observe and operationalize. As it has been noted by the studies on REF, evaluators may find it easier to use concepts drawn from STEM to evaluate all types of research, simply because they point to more observable and discrete products of research. From the field of social studies of evaluation it is well known that measurements lead to a feedback loop so that the immediate availability of indicators may lead to believe that important things are only those that can be measured. The notion of impact of research deserves further and intense research effort in the near future. We see several directions of research. First, apply the methodology to ex ante research documents, such as research proposals: do they include users? do they identify target groups with adequate Diversity and/or Specificity? Second, test whether there is a relation between the Specificity of identification of target groups and the assessment of the research, either ex ante (approval of a proposal) and ex post (score obtained in research assessment exercises). Third, extend the methodology to other kinds of documents. References "],
["design-exploiting-patent-information-in-novel-ways.html", "Chapter 14 Design: Exploiting patent information in novel ways", " Chapter 14 Design: Exploiting patent information in novel ways "],
["human-resources-defining-industry-4-0-professional-archetypes.html", "Chapter 15 Human Resources: Defining industry 4.0 professional archetypes", " Chapter 15 Human Resources: Defining industry 4.0 professional archetypes Text Mining has been used in the last years to manage human resources (HR) strategically, mainly with applications aiming at analyzing staff’s opinions, monitoring the level of employee satisfaction, as well as reading and storing CVs for the selection of new personnel. In the context of human resources management, the text mining techniques are often utilized to monitor the state of health of a company by means of the systematic analysis of informal documents. These documents are both internal (e.g. curricula, job description) and external (e.g. Linkedin, social networks). Nowadays in fact HR experts started to use text mining technique also with intelligence purposes. For this reason companies has to collect information about their employees, the HR market and their competitors, and to analyze enormous amount of documents. The aim of Competitive Intelligence in HR (Bolasco et al. 2005) is to select relevant information by automatic reading of this documents Once the material has been collected, it is classified into categories to develop a database, and analyzing the database to get answers to specific and crucial information for HR company strategies. The typical queries to collect the documents, concern the skills or the technological sectors of the competitors or the names of the employees of a company with a certain profile of competences. This is not a trivial task, and before the introduction of Text Minig, there was a division that was entirely dedicated to the continuous monitoring of information and answering the queries coming from other sectors of the company. In these cases the return on investment by the use of automatic document analysis technologies was self evident when compared to results previously achieved by manual operators. In some cases, if a scheme of categories is not defined a priori, clusterization algotyrhm (5.5.4) are used to classify the set of documents (considered) relevant with regard to a certain topic, in clusters of documents with similar contents. The analysis of the key concepts present in the single clusters gives an overall vision of the subjects dealt with in the single texts (Gupta, Lehal, and others 2009). Futhermore it has to be considered that as intellectual capital has become one of the most strategic assets of successful organizations, the ability to manage the expertise, skills, and experience of employees has become a key factor in overcoming the increasing competitiveness of the global market (Colucci et al. 2003). In today’s competitive business environment, companies need to accurately grasp the competency of their HR in order to be successful (Fazel-Zarandi and Fox 2009). As evidence of the increasing interest of TM techniques fot HR, an increasing number of publications are providing new research paths (Strohmeier and Piazza 2013, Al-Zegaier, Al-Zu’bi, and Barakat (2011), Zhao (2008), Çelik and Elçi (2012), Veit et al. (2001), Han and Lee (2016)). One study introduced an approach to improve the matching of profiles by searching job descriptions and applicant profiles using filters that represent the relevant skills and competencies (Paoletti, Martinez-Gil, and Schewe 2015). Nevertheless, several studies have found that résumés and work experience lists, which are composed of brief words or short sentences, are limited and have tried to improve HR solutions by adopting a semantic system approach, such as ontologies and text-mining methods. An example is the On-To-Knowledge project. The OnTo-Knowledge project focuses on the application-driven development of ontologies during the introduction of ontology-based knowledge management systems (Lau and Sure 2002). One research study has suggested that a possible approach could be addressing an intelligent decision support system composed of case-based reasoning and ontology (Zhukova et al. 2014). Another research stressed the importance of HR recruiting, selecting individuals for teams based on different skills and qualifications, determining who to train and what training programs to offer, and recommending the right expert to individuals for acquiring information or learning from within the organization (Fazel-Zarandi and Fox 2009). Another example of TM techniques often utilized to monitor the state of health of a company by means of the systematic analysis of informal documents is the case of ConocoPhilips, a fast-moving American company, which developed an internal system - the VSM (Virtual Signs Monitor) - able to find the intangible but crucial aspects of company life, the degree of experience and knowledge and the “productive” abilities. The approach chosen by Conoco was that of measuring the company mood by means of state of the art indicators (Ghoshal, Bartlett, and Kovner 1997), which contrasts a new model based on completely different pillars, like stretch, discipline, trust and reciprocal support with the traditional managerial model founded on concepts of constraint, contract, control and compliance. This managerial model, according to Ghoshal’s formulation encourages the cooperation and collaboration between the elements of an organisation, improving its results. Its collaboration with Temis enabled Conoco to refine its system for the monitoring of textual sources like e-mails, internal surveys of employees’ opinions, declarations of the management, internal and external chat lines, all representing important means for sounding the evolution of company culture. Finally, Web-Based Human Resources Systems had a strong impact on the HR processes. The most widely well-known web-based HR systems are Linkedin and OilandGas (Walker 2001). Their search systems are designed so that a user can input a query and look at the search results of résumés through keyword matching. With the special services of Linked-In (accessible to premium users only), users can also search for specialists and candidates from various industries. The OilandGas service is specialized for searching for experts in the oil and gas industry, using basically the same concept of discrete keywords matching. However, once this search process is completed, the recruiter has to download all the search results and résumés and then read through all the documents to pick the most suitable candidates. Furthermore, in HR nowadays the theme of Skills Assessment is of great interest. The term “assessment” indicates the action of assessing the potential, the skills, the attitudes and the adequacy of a professional profile. With the advent of the Fourth industrial revolution, it is increasingly important to carry out a continuous and precise assessment of skills. At the same time, it is essential to verify that the resources within the company are enough prepared to manage the phenomenon, or it is necessary to recruit new staff to face the challenge 4.0. Undoubtedly, the future demand for professional profiles will be conditioned by digital innovation and the ever more radical integration of new technologies. For these reasons, it is particularly sensitive to change. The literature is very focused on identifying the professional profiles that will survive during the epochal change and which instead will be eliminated. Osborne and Frey’s research is emblematic: they tried to define which would be the jobs that will resist and which not, according to the substitutability of individual skills (Frey and Osborne 2017). The results are quite pessimist: they distinguished between High, Medium and Low risk of computerisation, and, according to their estimates, around the 47% of jobs is in the high risk category. Moreover, there are opposing views on the social effects that the revolution will cause. Caruso outlines that the technological innovation could not improve worker’ conditions, performances and relationships because they cannot be determined by any technical innovation in itself, being it always socially shaped (Caruso 2017). On the other hand, the new technologies could represent an opportunity for the labour market and they could have positive impact on employment. The reason why it would probably happens is that 3d printing, Internet of Things, Augmented reality and Big data analytics demand a large quantity of new skills to be properly managed (Freddi 2017). Furthermore, MacCrory et al., performed a data analysis on occupational skill requirements of 674 occupations to study the effects of recent changes in automation. They identified the three main consequences of technological innovation, which could be summarized in: a significant reduction in skills that compete with automation; a significant increase in skills which complement machines; : “a significant reduction in skills that compete with machines, an increase in skills that complement machines; finally, an increase in skills where machines are not enough advanced (MacCrory et al. 2014). References "],
["conclusions-and-future-developments.html", "Conclusions and Future Developments", " Conclusions and Future Developments Quali altri stakeholders (quindi testi) We have finished a nice thesis "],
["glossary.html", "Glossary", " Glossary Generare automaticamente da wikipedia usando tagme. Morphology= the study of the way words are built up from smaller meaning-bearing units called morphemes. "],
["references.html", "References", " References "]
]
