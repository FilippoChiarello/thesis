<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Mining Technical Knowledge from Texts</title>
  <meta name="description" content="This document contains the PhD thesis of Filippo Chiarello.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Mining Technical Knowledge from Texts" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Mining Technical Knowledge from Texts" />
  
  <meta name="twitter:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

<meta name="author" content="Filippo Chiarello">


<meta name="date" content="2018-10-03">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="knowledge-intensive-manegement-engineering.html">
<link rel="next" href="sotadocuments.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Text Mining Techniques for Knowledge Extraction from Technical Documents</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="goal.html"><a href="goal.html"><i class="fa fa-check"></i><b>1</b> Goal</a></li>
<li class="chapter" data-level="2" data-path="problem.html"><a href="problem.html"><i class="fa fa-check"></i><b>2</b> Problem</a></li>
<li class="chapter" data-level="3" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>3</b> Solutions</a></li>
<li class="chapter" data-level="4" data-path="research-questions.html"><a href="research-questions.html"><i class="fa fa-check"></i><b>4</b> Research Questions</a></li>
<li class="chapter" data-level="5" data-path="stakeholders.html"><a href="stakeholders.html"><i class="fa fa-check"></i><b>5</b> Stakeholders</a></li>
<li class="chapter" data-level="6" data-path="knowledge-intensive-manegement-engineering.html"><a href="knowledge-intensive-manegement-engineering.html"><i class="fa fa-check"></i><b>6</b> Knowledge Intensive Manegement Engineering</a></li>
<li class="part"><span><b>II State of the Art</b></span></li>
<li class="chapter" data-level="7" data-path="sotatools.html"><a href="sotatools.html"><i class="fa fa-check"></i><b>7</b> Phases, Tasks, and Techniques</a><ul>
<li class="chapter" data-level="7.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolsprogram"><i class="fa fa-check"></i><b>7.1</b> Program</a></li>
<li class="chapter" data-level="7.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimport"><i class="fa fa-check"></i><b>7.2</b> Import</a><ul>
<li class="chapter" data-level="7.2.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimportretrieval"><i class="fa fa-check"></i><b>7.2.1</b> Document Retrieval</a></li>
<li class="chapter" data-level="7.2.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimportformat"><i class="fa fa-check"></i><b>7.2.2</b> Documents Format</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolstidy"><i class="fa fa-check"></i><b>7.3</b> Tidy</a></li>
<li class="chapter" data-level="7.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransform"><i class="fa fa-check"></i><b>7.4</b> Transform</a><ul>
<li class="chapter" data-level="7.4.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformsentencesplit"><i class="fa fa-check"></i><b>7.4.1</b> Sentence Splitting</a></li>
<li class="chapter" data-level="7.4.2" data-path="sotatools.html"><a href="sotatools.html#tokenization"><i class="fa fa-check"></i><b>7.4.2</b> Tokenization</a></li>
<li class="chapter" data-level="7.4.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformstemming"><i class="fa fa-check"></i><b>7.4.3</b> Stemming</a></li>
<li class="chapter" data-level="7.4.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformlemmatisation"><i class="fa fa-check"></i><b>7.4.4</b> Lemmatisation</a></li>
<li class="chapter" data-level="7.4.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformwi"><i class="fa fa-check"></i><b>7.4.5</b> Words importance metrics</a></li>
<li class="chapter" data-level="7.4.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformpos"><i class="fa fa-check"></i><b>7.4.6</b> Part-of-Speech Tagging</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodel"><i class="fa fa-check"></i><b>7.5</b> Model</a><ul>
<li class="chapter" data-level="7.5.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformngrams"><i class="fa fa-check"></i><b>7.5.1</b> N-Grams</a></li>
<li class="chapter" data-level="7.5.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodeldocclass"><i class="fa fa-check"></i><b>7.5.2</b> Document Classification</a></li>
<li class="chapter" data-level="7.5.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelsentanal"><i class="fa fa-check"></i><b>7.5.3</b> Sentiment Analsysis</a></li>
<li class="chapter" data-level="7.5.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelnetanal"><i class="fa fa-check"></i><b>7.5.4</b> Text Clustering</a></li>
<li class="chapter" data-level="7.5.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelner"><i class="fa fa-check"></i><b>7.5.5</b> Named Entity Recognition</a></li>
<li class="chapter" data-level="7.5.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodeltopicmodel"><i class="fa fa-check"></i><b>7.5.6</b> Topic Modelling</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolsvisualize"><i class="fa fa-check"></i><b>7.6</b> Visualize</a><ul>
<li class="chapter" data-level="7.6.1" data-path="sotatools.html"><a href="sotatools.html#the-grammar-of-graphics"><i class="fa fa-check"></i><b>7.6.1</b> The Grammar of Graphics</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="sotatools.html"><a href="sotatools.html#sotatoolscomunicate"><i class="fa fa-check"></i><b>7.7</b> Comunicate</a></li>
<li class="chapter" data-level="7.8" data-path="sotatools.html"><a href="sotatools.html#sotadocumentsunderstand"><i class="fa fa-check"></i><b>7.8</b> Understand</a><ul>
<li class="chapter" data-level="7.8.1" data-path="sotatools.html"><a href="sotatools.html#domain-expertise"><i class="fa fa-check"></i><b>7.8.1</b> Domain Expertise</a></li>
<li class="chapter" data-level="7.8.2" data-path="sotatools.html"><a href="sotatools.html#sotadocumentsunderstandbyas"><i class="fa fa-check"></i><b>7.8.2</b> The problem of byases</a></li>
<li class="chapter" data-level="7.8.3" data-path="sotatools.html"><a href="sotatools.html#sotadocumentsunderstandlexicons"><i class="fa fa-check"></i><b>7.8.3</b> The Importance of Lexicons for Technical Documents Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="sotadocuments.html"><a href="sotadocuments.html"><i class="fa fa-check"></i><b>8</b> Documents</a><ul>
<li class="chapter" data-level="8.1" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentspatents"><i class="fa fa-check"></i><b>8.1</b> Patents</a><ul>
<li class="chapter" data-level="8.1.1" data-path="sotadocuments.html"><a href="sotadocuments.html#metadata-approaches"><i class="fa fa-check"></i><b>8.1.1</b> Metadata Approaches</a></li>
<li class="chapter" data-level="8.1.2" data-path="sotadocuments.html"><a href="sotadocuments.html#keywords-approaches"><i class="fa fa-check"></i><b>8.1.2</b> Keywords Approaches</a></li>
<li class="chapter" data-level="8.1.3" data-path="sotadocuments.html"><a href="sotadocuments.html#natural-language-processing-approaches"><i class="fa fa-check"></i><b>8.1.3</b> Natural Language Processing approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentspapers"><i class="fa fa-check"></i><b>8.2</b> Papers</a></li>
<li class="chapter" data-level="8.3" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentswiki"><i class="fa fa-check"></i><b>8.3</b> Wikipedia</a></li>
<li class="chapter" data-level="8.4" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentstwitter"><i class="fa fa-check"></i><b>8.4</b> Social Media</a><ul>
<li class="chapter" data-level="8.4.1" data-path="sotadocuments.html"><a href="sotadocuments.html#economics"><i class="fa fa-check"></i><b>8.4.1</b> Economics</a></li>
<li class="chapter" data-level="8.4.2" data-path="sotadocuments.html"><a href="sotadocuments.html#marketing"><i class="fa fa-check"></i><b>8.4.2</b> Marketing</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Methods and Results</b></span></li>
<li class="chapter" data-level="9" data-path="patents.html"><a href="patents.html"><i class="fa fa-check"></i><b>9</b> Patents</a><ul>
<li class="chapter" data-level="9.1" data-path="patents.html"><a href="patents.html#usersresults"><i class="fa fa-check"></i><b>9.1</b> Users</a><ul>
<li class="chapter" data-level="9.1.1" data-path="patents.html"><a href="patents.html#method"><i class="fa fa-check"></i><b>9.1.1</b> Method</a></li>
<li class="chapter" data-level="9.1.2" data-path="patents.html"><a href="patents.html#results"><i class="fa fa-check"></i><b>9.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="patents.html"><a href="patents.html#advdrwresults"><i class="fa fa-check"></i><b>9.2</b> Advantages and Drawbacks</a><ul>
<li class="chapter" data-level="9.2.1" data-path="patents.html"><a href="patents.html#methodology"><i class="fa fa-check"></i><b>9.2.1</b> Methodology</a></li>
<li class="chapter" data-level="9.2.2" data-path="patents.html"><a href="patents.html#results-1"><i class="fa fa-check"></i><b>9.2.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="patents.html"><a href="patents.html#trademakrs"><i class="fa fa-check"></i><b>9.3</b> Trademakrs</a><ul>
<li class="chapter" data-level="9.3.1" data-path="patents.html"><a href="patents.html#methodology-1"><i class="fa fa-check"></i><b>9.3.1</b> Methodology</a></li>
<li class="chapter" data-level="9.3.2" data-path="patents.html"><a href="patents.html#results-2"><i class="fa fa-check"></i><b>9.3.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="papers.html"><a href="papers.html"><i class="fa fa-check"></i><b>10</b> Papers</a><ul>
<li class="chapter" data-level="10.1" data-path="papers.html"><a href="papers.html#sustainable-manufacturing-an-analysis-of-the-6r-framework"><i class="fa fa-check"></i><b>10.1</b> Sustainable Manufacturing: an Analysis of the 6R Framework</a><ul>
<li class="chapter" data-level="10.1.1" data-path="papers.html"><a href="papers.html#methodology-2"><i class="fa fa-check"></i><b>10.1.1</b> Methodology</a></li>
<li class="chapter" data-level="10.1.2" data-path="papers.html"><a href="papers.html#results-3"><i class="fa fa-check"></i><b>10.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="papers.html"><a href="papers.html#sustainable-manufacturing-an-extended-mapping"><i class="fa fa-check"></i><b>10.2</b> Sustainable Manufacturing: An Extended Mapping</a><ul>
<li class="chapter" data-level="10.2.1" data-path="papers.html"><a href="papers.html#methodology-3"><i class="fa fa-check"></i><b>10.2.1</b> Methodology</a></li>
<li class="chapter" data-level="10.2.2" data-path="papers.html"><a href="papers.html#results-4"><i class="fa fa-check"></i><b>10.2.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="papers.html"><a href="papers.html#blockchain"><i class="fa fa-check"></i><b>10.3</b> Blockchain</a><ul>
<li class="chapter" data-level="10.3.1" data-path="papers.html"><a href="papers.html#methodology-4"><i class="fa fa-check"></i><b>10.3.1</b> Methodology</a></li>
<li class="chapter" data-level="10.3.2" data-path="papers.html"><a href="papers.html#results-5"><i class="fa fa-check"></i><b>10.3.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="papers.html"><a href="papers.html#precision-agriculture"><i class="fa fa-check"></i><b>10.4</b> Precision Agriculture</a><ul>
<li class="chapter" data-level="10.4.1" data-path="papers.html"><a href="papers.html#methodology-5"><i class="fa fa-check"></i><b>10.4.1</b> Methodology</a></li>
<li class="chapter" data-level="10.4.2" data-path="papers.html"><a href="papers.html#results-6"><i class="fa fa-check"></i><b>10.4.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="wikipedia.html"><a href="wikipedia.html"><i class="fa fa-check"></i><b>11</b> Wikipedia</a><ul>
<li class="chapter" data-level="11.1" data-path="wikipedia.html"><a href="wikipedia.html#technimetrochap"><i class="fa fa-check"></i><b>11.1</b> Industry 4.0: Extracting and Mapping Technologies</a><ul>
<li class="chapter" data-level="11.1.1" data-path="wikipedia.html"><a href="wikipedia.html#methodology-6"><i class="fa fa-check"></i><b>11.1.1</b> Methodology</a></li>
<li class="chapter" data-level="11.1.2" data-path="wikipedia.html"><a href="wikipedia.html#results-7"><i class="fa fa-check"></i><b>11.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="wikipedia.html"><a href="wikipedia.html#industry-4.0-a-comparison-with-industrie-4.0"><i class="fa fa-check"></i><b>11.2</b> Industry 4.0: a Comparison with Industrie 4.0</a><ul>
<li class="chapter" data-level="11.2.1" data-path="wikipedia.html"><a href="wikipedia.html#methodology-7"><i class="fa fa-check"></i><b>11.2.1</b> Methodology</a></li>
<li class="chapter" data-level="11.2.2" data-path="wikipedia.html"><a href="wikipedia.html#results-8"><i class="fa fa-check"></i><b>11.2.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="social-media.html"><a href="social-media.html"><i class="fa fa-check"></i><b>12</b> Social Media</a><ul>
<li class="chapter" data-level="12.1" data-path="social-media.html"><a href="social-media.html#technical-sentiment-analysis"><i class="fa fa-check"></i><b>12.1</b> Technical Sentiment Analysis</a><ul>
<li class="chapter" data-level="12.1.1" data-path="social-media.html"><a href="social-media.html#methodology-8"><i class="fa fa-check"></i><b>12.1.1</b> Methodology</a></li>
<li class="chapter" data-level="12.1.2" data-path="social-media.html"><a href="social-media.html#results-9"><i class="fa fa-check"></i><b>12.1.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Impacts in Different Knowledge Fields</b></span></li>
<li class="chapter" data-level="13" data-path="marketing-the-user-side-of-competition.html"><a href="marketing-the-user-side-of-competition.html"><i class="fa fa-check"></i><b>13</b> Marketing: The User Side of Competition</a></li>
<li class="chapter" data-level="14" data-path="research-and-development-enriched-dictionaries-for-innovation.html"><a href="research-and-development-enriched-dictionaries-for-innovation.html"><i class="fa fa-check"></i><b>14</b> Research and Development: Enriched dictionaries for Innovation</a></li>
<li class="chapter" data-level="15" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html"><i class="fa fa-check"></i><b>15</b> Policy Making: Impact of research from the perspective of users.</a><ul>
<li class="chapter" data-level="15.1" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#methodological-challenges"><i class="fa fa-check"></i><b>15.1</b> Methodological challenges</a><ul>
<li class="chapter" data-level="15.1.1" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#variability-in-the-identification-of-outcomes-and-users"><i class="fa fa-check"></i><b>15.1.1</b> Variability in the identification of outcomes and users</a></li>
<li class="chapter" data-level="15.1.2" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#sources-of-information"><i class="fa fa-check"></i><b>15.1.2</b> Sources of information</a></li>
<li class="chapter" data-level="15.1.3" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#text-based-impact-assessment"><i class="fa fa-check"></i><b>15.1.3</b> Text-based impact assessment</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#methodology-9"><i class="fa fa-check"></i><b>15.2</b> Methodology</a><ul>
<li class="chapter" data-level="15.2.1" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#operationalizing-user-groups-using-natural-language-processing-techniques"><i class="fa fa-check"></i><b>15.2.1</b> Operationalizing user groups using Natural Language Processing techniques</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#from-text-extraction-to-indicators"><i class="fa fa-check"></i><b>15.3</b> From text extraction to indicators</a><ul>
<li class="chapter" data-level="" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#frequency"><i class="fa fa-check"></i>Frequency</a></li>
<li class="chapter" data-level="" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#diversity"><i class="fa fa-check"></i>Diversity</a></li>
<li class="chapter" data-level="" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#specificity"><i class="fa fa-check"></i>Specificity</a></li>
<li class="chapter" data-level="15.3.1" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#the-meaning-of-frequency-diversity-and-specificity-indicators-for-the-analysis-of-research-impact"><i class="fa fa-check"></i><b>15.3.1</b> The meaning of Frequency, Diversity and Specificity indicators for the analysis of research impact</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#data"><i class="fa fa-check"></i><b>15.4</b> Data</a><ul>
<li class="chapter" data-level="15.4.1" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#description-of-the-corpus"><i class="fa fa-check"></i><b>15.4.1</b> Description of the corpus</a></li>
<li class="chapter" data-level="15.4.2" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#preliminary-analysis-of-the-corpus"><i class="fa fa-check"></i><b>15.4.2</b> Preliminary analysis of the corpus</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#results-10"><i class="fa fa-check"></i><b>15.5</b> Results</a><ul>
<li class="chapter" data-level="15.5.1" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#descriptive-analysis"><i class="fa fa-check"></i><b>15.5.1</b> Descriptive analysis</a></li>
<li class="chapter" data-level="15.5.2" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#findings-by-subject-area"><i class="fa fa-check"></i><b>15.5.2</b> Findings by subject area</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="policy-making-impact-of-research-from-the-perspective-of-users-.html"><a href="policy-making-impact-of-research-from-the-perspective-of-users-.html#discussion"><i class="fa fa-check"></i><b>15.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="design-exploiting-patent-information-in-novel-ways.html"><a href="design-exploiting-patent-information-in-novel-ways.html"><i class="fa fa-check"></i><b>16</b> Design: Exploiting patent information in novel ways</a></li>
<li class="chapter" data-level="17" data-path="human-resources-defining-industry-4-0-professional-archetypes.html"><a href="human-resources-defining-industry-4-0-professional-archetypes.html"><i class="fa fa-check"></i><b>17</b> Human Resources: Defining industry 4.0 professional archetypes</a></li>
<li class="chapter" data-level="" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i>Conclusions</a></li>
<li class="chapter" data-level="" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mining Technical Knowledge from Texts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sotatools" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Phases, Tasks, and Techniques</h1>
<p>In this section I make a review of the most important techniques for Natural Language Processing in the context of technical documents analysis. The techniques (mainly algorithms) are grouped in phases (Import, Tidy, Transform, Model, Visualize, Communicate) showed in figure <a href="solutions.html#fig:mainworkflow">3.1</a> and each phases is dived in the NLP tasks that are the most important for the analysis of technical documents. This standard process has been disclosed in the framework of the tidyverse <span class="citation">(Wickham and Grolemund <a href="#ref-wickham2016r">2016</a>)</span>. The algorithms i reviewed in this section are summmarised in table tot, where the reader can see the relationship between tasks and techniques.</p>
<div id="sotatoolsprogram" class="section level2">
<h2><span class="header-section-number">7.1</span> Program</h2>
<p>Programming is a key activity to perform in order to effectively and efficiently perform text mining. It is not a phases per se because each phase is implemented trough programming. It is critical that an analysts has in mind the need of maximizing the probability that their analysis is reproducible, accurate, and collaborative. This goals can be reached only trough programming. The most used programming languages for text mining and natural language processing are R <span class="citation">(R Development Core Team <a href="#ref-r2008">2008</a>)</span> and Python <span class="citation">(Rossum <a href="#ref-py95">1995</a>)</span>. R and Python are both open-source programming languages with a large community of developers, and new libraries or tools are added continuously to their respective catalog. R is mainly used for statistical analysis and data science while Python is a more general purpose programming language. R has been developed by Academics and statisticians over two decades. R has now one of the richest ecosystems to perform data analysis and there are around 12000 packages available in CRAN (open-source repository of R). The rich variety of libraries makes R the first choice for statistical analysis. Another cutting-edge difference between R and the other statistical products is R-studio. RStudio is a free and open-source integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. Finally, it is widely recognized the great performances that R has for data visualisation and communication. Python can pretty much do the same tasks as R: data wrangling, engineering, feature selection web scrapping, app and so on. Anyway, Python has great performances in the deployment and implementation of machine learning at a large-scale. Furthermore, Python codes are easier to maintain and more robust than R.</p>
</div>
<div id="sotatoolsimport" class="section level2">
<h2><span class="header-section-number">7.2</span> Import</h2>
<p>The first activities to perform in a text mining pipeline is to find all the documents that contains useful information for the analysis and then import the corpus (the set of documents) in to the computer program. The present section is thus focused on techniques for document retrieval <a href="sotatools.html#sotatoolsimportretrieval">7.2.1</a> and on the most popular documents digital formats <a href="sotatools.html#sotatoolsimportformat">7.2.2</a>.</p>
<div id="sotatoolsimportretrieval" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Document Retrieval</h3>
<p>Document retrieval is the process of matching a user query against a set of documents. A document retrieval system has two main tasks:</p>
<p>1- Find the documents that are relevant with respect to the user queries 2- Measure the relevance of the matching results</p>
<p>Building a query means to use field specific knowledge and logical rules to write a text string that is the composition of keywords and Boolean operators. The set of keywords (single words or phrases) is chosen in such a way that these are likely to be contained in the searched documents. Boolean operators can also be used to increment the performance of the query. The AND operator, for example is used to retrieve all the document that contains both of the terms at the left and the right of it, OR for document that contains at least one of the two words. Another important tool for making a good query are regular expressions. Regular expression (regexp) is a language for specifying text search strings, an algebraic notation for characterizing a set of strings. This language widely used in modern word processor and text processing tools. A regular expression search function will search through the corpus, returning all texts that match the pattern. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. To evaluate the performance of a query is useful to understand the concepts of precision and recall.</p>
<p>Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned. Due to the ambiguities of natural language, full-text-search systems typically includes options like stop words to increase precision. Stop-words are words that filter all the document which contains them. On the other side, stemming to increase recall <a href="sotatools.html#sotatoolstransformstemming">7.4.3</a>. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision <span class="citation">(Yuwono and Lee <a href="#ref-yuwono1996search">1996</a>)</span>. Usually when a user performs a query, the main problem are false positives (the results that are returned by the systems but are not relevant to the user). False positives has a negative impact on the precision of the query. The retrieval of irrelevant documents is particularly strong for technical documents due to the inherent ambiguity of technical language. For this reason to understand and to use the rules of query building are fundamental to the technical document analysis, since without a good query is rare to have a good set of documents to analyze.</p>
</div>
<div id="sotatoolsimportformat" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Documents Format</h3>
<p>For the purpose of the present thesis documents are considered in a digital format, and there is no need to read it from a analogical source. From the computer science point of view, text is a human-readable sequence of characters and the words they form that can be encoded into computer-readable formats. There is no standard definition of a text file, though there are several common formats. The most common types of encoding are:</p>
<ul>
<li>ASCII, UTF-8 — plain text formats</li>
<li>.doc for Microsoft Word — Structural binary format developed by Microsoft (specifications available since 2008 under the Open Specification Promise)</li>
<li>HTML (.html, .htm), (open standard, ISO from 2000)</li>
<li>Office Open XML — .docx (XML-based standard for office documents)</li>
<li>OpenDocument — .odt (XML-based standard for office documents)</li>
<li>PDF — Open standard for document exchange. ISO standards include PDF/X (eXchange), PDF/A (Archive), PDF/E (Engineering), ISO 32000 (PDF), PDF/UA (Accessibility) and PDF/VT (Variable data and transactional printing). PDF is readable on almost every platform with free or open source readers. Open source PDF creators are also available.</li>
<li>Scalable Vector Graphics (SVG) - Graphics format primarily for vector-based images.</li>
<li>TeX — Popular open-source typesetting program and format. First successful mathematical notation language.</li>
</ul>
<p>For the R software there exist many packages that helps to import documents in several formats <span class="citation">(Wickham, Hester, and Francois <a href="#ref-readr2017r">2017</a>)</span>.</p>
</div>
</div>
<div id="sotatoolstidy" class="section level2">
<h2><span class="header-section-number">7.3</span> Tidy</h2>
<p>After that data are imported they have to be processed in such a way that it would be possible to perform the main task of data analysis (transformation, modelling and visualisation). This task of tidying data (usually referred to as data pre-processing) can be very time expensive, so it is important to have clear methods and techniques to perform this task.</p>
<p>Tidy data sets have structure and working with them is easy; they’re easy to manipulate, model and visualize <span class="citation">(Wickham and others <a href="#ref-wickham2014tidy">2014</a>)</span>. Tidy data sets main concept is to arrange data in a way that each variable is a column and each observation (or case) is a row. The characteristics of tidy data can be thus summarised as the points <span class="citation">(Leek <a href="#ref-leek2015elements">2015</a>)</span>:</p>
<ul>
<li>Each variable you measure should be in one column</li>
<li>Each different observation of that variable should be in a different row</li>
<li>If you have multiple tables, they should include a column in the table that allows them to be linked</li>
</ul>
<p>There main advantages of structuring the data in this way is that a consistent data structure make it easier to use the tools (programs) that work with it because they have an underlying uniformity. This lead to an advantage in reproducibility of code.</p>
<p>As stated before tidying data is not a trivial task, and applying this process to text is even harder for documents with respect to structured data <span class="citation">(Silge and Robinson <a href="#ref-silge2016tidytext">2016</a>)</span>. On the other side, is clear that using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use . Treating text as data frames of individual words allows us to manipulate, summarize, and visualize the characteristics of text easily and integrate natural language processing into effective workflows already used.</p>
<p>Tidy text format is designed as being a table with one-token-per-row. A token unit of text that is meaningful for the analysis to be performed (for example letters, words, n-gram, sentences, or paragraphs ). Tokenization is the process of splitting text into tokens. This one-token-per-row structure is in different from the ways documents are often stored in current analyses, mainly strings or document-term matrix. The term document matrix has each corpus word represented as a row with documents as columns. The document term matrix is the transposition of the TDM so each document is a row and each word is a column. The term document matrix or document term matrix is the foundation of bag of words text mining. The bag-of-words model is a simplifying representation of documents: a text is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity <span class="citation">(McTear, Callejas, and Griol <a href="#ref-mctear2016conversational">2016</a>)</span>.</p>
</div>
<div id="sotatoolstransform" class="section level2">
<h2><span class="header-section-number">7.4</span> Transform</h2>
<p>Transforming in the context of Natural Language Processing is what in computational linguistic is called text normalization. Normalizing text means converting it to a more convenient, standard form. Most of the task of technical document analysis in fact relies on first separating out or tokenizing sentences and words, strip suffixes from the end of the word, determining the root of a word or transform the text using regular expressions.</p>
<div id="sotatoolstransformsentencesplit" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Sentence Splitting</h3>
<p>The analysis of technical documents require as first process, that the input text is segmented in sentences. Since documents do not encode this information in a non ambiguous manner (using dots) due to common abbreviations (e.g.: “Mr., Dr.”), a sentence splitting process that does not rely only on a trivial <em>dot based</em> rule is required. This issue in the technical documents domain is even more problematic due to the presence of formulas, numbers, chemical entity names and bibliographic references. Furthermore, since sentence splitting is one of the first processes of an NLP pipeline, errors in this early stage are propagated in the following steps causing a strong decrease for what concerns their accuracy. One of the most advanced techniques are machine learning techniques: given a training corpus of properly segmented sentences and a learning algorithm, a statistical model is built. By reusing the statistical model, the sentence splitter is able to split sentences on texts not used in the training phase. ItalianNLP lab systems uses this approach <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>, <span class="citation">Attardi and Dell’Orletta (<a href="#ref-attardi2009reverse">2009</a>)</span>, <span class="citation">Attardi et al. (<a href="#ref-attardi2009accurate">2009</a>)</span>)</span>. For this reason this algorithm is used for the most of the application presented in this Thesis.</p>
</div>
<div id="tokenization" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Tokenization</h3>
<p>Since documents are unstructured information, these has to be divided into linguistic units. The definition of linguistic units is non-trivial, and more advanced techniques can be used (such as n-gram extraction) but most of the times these are words, punctuation and numbers. English words are often separated from each other by white space, but white space is not always sufficient. Solving this problems and splitting words in well-defined tokens defined as tokenization. In most of the application described in the present Thesis, the tokenizer developed by the ItalianNLP lab was integrated <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>; Attardi and Dell’Orletta <a href="#ref-attardi2009reverse">2009</a>; Attardi et al. <a href="#ref-attardi2009accurate">2009</a>)</span>. This tokenizer is regular expression based: each token must match one of the regular expression defined in a configuration file. Among the others, rules are defined to tokenize words, acronyms, numbers, dates and equations.</p>
</div>
<div id="sotatoolstransformstemming" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Stemming</h3>
<p>Stemming is a simpler but cruder methodology for chopping off of affixes. The goal of stemming is reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem of a word and its morphological root do not need to be identical; it is sufficient that related words map to the same stem, even if this stem is not a valid root. One of the most widely used stemming is the simple and efficient Porter algorithm <span class="citation">(Porter <a href="#ref-porter1980algorithm">1980</a>)</span>.</p>
</div>
<div id="sotatoolstransformlemmatisation" class="section level3">
<h3><span class="header-section-number">7.4.4</span> Lemmatisation</h3>
<p>Lemmatization is the task of determining the root of a words. The output allow to find that two words have the same root, despite their surface differences. For example, the verbs <em>am</em>, <em>are</em>, and <em>is</em> have the shared lemma <em>be</em>; the nouns <em>cat</em> and <em>cats</em> both have the lemma <em>cat</em>. Representing a word by its lemma is important for many natural language processing tasks. Lemmatisation in fact diminish the problem of sparsity of document-word matrix. Furthermore lemmatisaion is important for document retrieval <a href="sotatools.html#sotatoolsimportretrieval">7.2.1</a> web search, since the goal is to find documents mentioning motors if the search is for motor. The most recent methods for lemmatization involve complete morphological parsing of the word <span class="citation">(Hankamer <a href="#ref-hankamer1989morphological">1989</a>)</span>.</p>
</div>
<div id="sotatoolstransformwi" class="section level3">
<h3><span class="header-section-number">7.4.5</span> Words importance metrics</h3>
<p>Once that a document has been tokenized and the tokens has been transformed, an analyst usually wants to measure how important a word is to a document in a collection or corpus. Some of the metrics adopted are:</p>
<ul>
<li><em>Term Frequency</em>: the number of times that a term occurs in document.</li>
<li><em>Boolean frequency</em>: 1 if the term occurs in the document and 0 otherwise;</li>
<li><em>Term frequency adjusted for document length</em>: is raw count normalized for the number of words contained in the document</li>
<li><em>Logarithmically scaled frequency</em>: is raw count normalized for the natural logarithm of one plus the number of words contained in the document</li>
<li><em>Inverse document frequency</em>: is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. It is a measure of how much information the word provides, that is, whether the term is common or rare across all documents.</li>
<li><em>Term frequency–Inverse document frequency</em>: the product between <em>term frequency and inverse document frequency</em>. A high weight in tf–idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.</li>
</ul>
</div>
<div id="sotatoolstransformpos" class="section level3">
<h3><span class="header-section-number">7.4.6</span> Part-of-Speech Tagging</h3>
<p>The part of speech plays an central role in technical document analysis since it provides very useful information concerning the morphological role of a word and its morphosyntactic context: for example, if a token is a determiner, the next token is a noun or an adjective with very high confidence. Part of speech tags are used for many information extraction tools such as named entity taggers (see section <a href="sotatools.html#sotatoolsmodelner">7.5.5</a>) in order to identify named entities. In typical named entity task these are people and locations since tokens representing named entities follow common morphological patterns (e.g. they start with a capital letter). For the application to technical documents, technical entities (like the possible failures of a manufact) becomes more relevant. In this context a correct part-of-speech tagger becomes even more important since morphosyntactical rules can not be used. In addition part of speech tags can be used to mitigate problems related to polysemy since words often have different meaning with respect to their part of speech (e.g. “track”, “guide”). This information is extremely valuable in patent analysis, and some patent tailored part-of-speech tagger has been designed (see section <a href="sotadocuments.html#sotadocumentspatents">8.1</a>). The literature on pos-tagger is huge, and goes behind the scope of the present thesis to make a complete review. In most of the application presented in this work, was employed the ILC postagger <span class="citation">(Attardi <a href="#ref-attardi2006experiments">2006</a>)</span>. This postagger uses a supervised training algorithm: given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus.</p>
</div>
</div>
<div id="sotatoolsmodel" class="section level2">
<h2><span class="header-section-number">7.5</span> Model</h2>
<p>The goal of a model is to provide a simple low-dimensional summary of a dataset <span class="citation">(Wickham and Grolemund <a href="#ref-wickham2016r">2016</a>)</span>. Ideally, the model will capture patterns generated by the phenomenon of interest (true signals), and ignore random variations (noise). A good model at the same time is able to capture the weak signals that cab be easily confounded with noise. These information is particularly valuable in the context of technical document analysis, where great technical insight could come weak quasi-invisible signals. <span class="citation">(James et al. <a href="#ref-james2013introduction">2013</a>)</span></p>
<p>Probabilistic models are widely used in text mining nowadays, and applications range from topic modeling, language modeling, document classification and clustering to information extraction. The present section contains a review of the most used methods used to model textual information.</p>
<div id="sotatoolstransformngrams" class="section level3">
<h3><span class="header-section-number">7.5.1</span> N-Grams</h3>
<p>An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like “credit card”, “3d printing”, or ”printing machine”, and a 3-gram (or trigram) is a three-word sequence of words like “3d printing machine”. Statistical model can be used to extract the n-grams contained in a document. A first approach has the of predicting the next item in a sequence in the form of a (n − 1)–order Markov model<span class="citation">(Lafferty and Zhai <a href="#ref-lafferty2001document">2001</a>)</span>. The algorithm begin with the task of computing P(w|h), the probability of a word w given a word h.The way to estimate this probability is using relative frequency counts. To do that the algorithms count the number of times h is followed by the w. With a large enough corpus it is possible to build valuable models, able to extract n-grams <span class="citation">(Bellegarda <a href="#ref-bellegarda2004statistical">2004</a>)</span>. While this method of estimating probabilities directly from counts works for many natural language applications, in many cases a huge dimension of the corpus does make the model useful, and this is particularly true for technical documents <span class="citation">(Brants et al. <a href="#ref-brants2012large">2012</a>)</span>. This is because technical language has a strong ratio of evolution; as new artifact are invented, new chunks are created all the time, and has no sense to continuously count every word co-occurrence to update our model<span class="citation">(Gibson, Gibson, and Ingold <a href="#ref-gibson1994tools">1994</a>)</span>. A more useful method for chunk extraction fro technical document uses part-of-speech-tagging and regular expression. Once a document is pos-taggerd each word is associated whit a particular part of speech: each sentence is represented as a sequence of part-of-spech. Once this representation is ready, it is possible to extract only certain sequences of part-of-speeches, the ones that whit an high level of confidence are n-grams.</p>
</div>
<div id="sotatoolsmodeldocclass" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Document Classification</h3>
<p>Classification is a general process that has the goal of taking an object, extract features, and assign to the observation one of a set of discrete classes. This process is largely used for documents <span class="citation">(Borko and Bernick <a href="#ref-borko1963automatic">1963</a>)</span> and there exist many methods for document classification <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>.</p>
<p>Regardless of technological sector, most organizations today are facing the problem of overload of information. When it comes to classify huge amount of documents or to separate the useful documents from the irrelevant, document classification techniques can reduce the process cost and time.</p>
<p>The simplest method for classifying text is to use expert defined rules. These systems are called expert systems or knowledge engineering approach. Expert rule-based systems are programs that consist of rules in the IF form condition THEN action (if condition, then action). Given a series of facts, expert systems, thanks to the rules they are made of, manage to deduce new facts. The expert systems therefore differ from other similar programs, since, by referring to technologies developed according to artificial intelligence, they are always able to exhibit the logical steps that underlie their decisions: a purpose that, for example, is not feasible from the human mind or black box-systems. There are many type of documents for which expert based classifiers constitute a state-of-the-art system, or at least part of it. Anyway, rules can be useless in situations such as: - data change over time - the rules are too many and interrelated</p>
<p>Most systems of documents classification are instead done via supervised learning: a data set of input observations is available and each observation is associated with some correct output (training set). The goal of the algorithm is to build a static model able to learn how to map from a new observation (test set) to a correct output. The advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains.</p>
<p>In the supervised document classification process, is used a training set of N documents that have each been typically hand-labeled with a class: (d1, c1),….,(dN, cN). I say typically, because other less expensive methods could be designed, as it will be shown for the task of Named Entity Recognition (another supervised learning task, that classifies words instead of documents <a href="sotatools.html#sotatoolsmodelner">7.5.5</a>). The goal of the supervised document classification task is to learn a statistical model capable of assign a new document d to its correct class c ∈ C. There exist a class of these classifier, probabilistic classifiers, that additionally will tell us the probability of the observation being in the class.</p>
<p>Many kinds of machine learning algorithms are used to build classifiers <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>, such as:</p>
<ul>
<li><p><em>Decision Tree Classifiers</em>: Decision tree documents classifier are systems that has as output a classification tree <span class="citation">(Sebastiani <a href="#ref-sebastiani2002machine">2002</a>)</span>. In this tree internal nodes are terms contained in the corpus under analysis, branches departing are labeled by the weight (see section <a href="sotatools.html#sotatoolstidy">7.3</a>) that the term has in the test document, and leafs are labeled by categories. There exists many methods to automatically learn trees from data. A tree can be build by splitting the data source into subsets based on an test feature. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions.</p></li>
<li><p><em>Rule Based Classifiers</em>: Rule-based classifiers are systems in which the patterns which are most likely to be related to the different classes are extracted from a set of test documents. The set of rules corresponds to the left hand side to a word pattern, and the right-hand side to a class label. These rules are used for the purposes of classification. In its most general form, the left hand side of the rule is a Boolean condition, which is expressed in Disjunctive Normal Form (DNF). However, in most cases, the condition on the left hand side is much simpler and represents a set of terms, all of which must be present in the document for the condition to be satisfied <span class="citation">(Yang, Li, and Wang <a href="#ref-yang2004building">2004</a>)</span>.</p></li>
<li><p><em>Support Vector Machines (SVM) Classifiers</em>: SVM Classifiers attempt to partition the data space with the use of linear or non-linear delineations between the different classes. The main principle of SVM algorithm is to determine separators in the feature space which can best separate the different classes <span class="citation">(Joachims <a href="#ref-joachims1998text">1998</a>,<span class="citation">Manevitz and Yousef (<a href="#ref-manevitz2001one">2001</a>)</span>)</span>.</p></li>
<li><p><em>Baeysian Classifiers</em>: Bayesian classifiers build a probabilistic classifier based on modeling the underlying word features in different classes. The idea is then to classify documents using the posterior probability of the documents belonging to the different classes on the basis of the word presence in the documents <span class="citation">(Pop <a href="#ref-pop2006approach">2006</a>)</span>.</p></li>
<li><p><em>Neural Netword Classifiers</em>: The basic unit in a neural network is a neuron. Each neuron receives a set of inputs, which are denoted by the vector <em>Xi</em>, which are the values of the feature vector for a certain instance. Each neuron is also associated with a set of weights, which are used in order to compute a function of its inputs. Neural Networks Classifier are able, thank to a process called learning phase, to adjust their weights in such a way that the function is able to effectively classify new instances. Neural networks are nowadays one of the best method for documents classification, and are used in a wide variety of applications <span class="citation">(Manevitz and Yousef <a href="#ref-manevitz2007one">2007</a>)</span>. Great performances has also been reached by deep neural networks, which are neural networks whit a large number o neurons arranged in multiple layers <span class="citation">(Lai et al. <a href="#ref-lai2015recurrent">2015</a>. <span class="citation">Kim (<a href="#ref-kim2014convolutional">2014</a>)</span>)</span>.</p></li>
</ul>
</div>
<div id="sotatoolsmodelsentanal" class="section level3">
<h3><span class="header-section-number">7.5.3</span> Sentiment Analsysis</h3>
<p>Sentiment analysis techniques are algorithms able to measure from text, people’s opinions and emotions toward events, topics, products and their attributes <span class="citation">(Pang, Lee, and others <a href="#ref-pang2008opinion">2008</a>)</span>. For example, businesses (particularly marketeers) are interested in finding costumers opinions about their products and services.</p>
<p>Thanks to the growth of social media (forums, blogs and social networks), individuals and organizations are producing a huge quantity of their written opinion. This has make it possible to scholars to study this phenomena and to develop many different and effective sentiment analysis techniques <span class="citation">(Liu and Zhang <a href="#ref-liu2012survey">2012</a>)</span>. In the past decade, a considerable amount of research has been done by scholars and there are also numerous commercial companies that provide opinion mining services. However, measuring sentiment in documents and distilling the information contained in them remains a challenging task because of the diversity of documents from which is possible to extract sentiment.</p>
<p>The approaches to perform sentiment analysis are many. Among all, the most interesting for technical documents analysis are:</p>
<ul>
<li><p><em>Dictionary Base Approaches</em> : This approach has the aim of collecting words that are clues for positive or negative sentiment. In literature these words are called opinion words, opinion-bearing words or sentiment words. Examples of positive opinion words are: good, nice and amazing. Examples of negative opinion words are bad, poor, and terrible. Collectively, they are called the opinion lexicon. The most simple and widely used techniques to produce a dictionary of opinion words is based on bootstrapping using a small set of seed opinion words and an online dictionary such as WordNet <span class="citation">(Miller <a href="#ref-miller1995wordnet">1995</a>)</span>. The works that used this approach <span class="citation">(Hu and Liu <a href="#ref-hu2004mining">2004</a>, <span class="citation">Kim and Hovy (<a href="#ref-kim2004determining">2004</a>)</span>)</span>, adopts a process that consist in two phases: first collect set of opinion words manually, then grow this set by searching in the WordNet for their synonyms and antonyms. The process stops when no more new words are found. After that a manual inspection can be carried out to remove and/or correct errors. Scholars has developed several opinion lexicons <span class="citation">(Ding, Liu, and Yu <a href="#ref-ding2008holistic">2008</a>, <span class="citation">Baccianella, Esuli, and Sebastiani (<a href="#ref-baccianella2010sentiwordnet">2010</a>)</span>, <span class="citation">Hu and Liu (<a href="#ref-hu2004mining">2004</a>)</span>, <span class="citation">Philip et al. (<a href="#ref-philip1966general">1966</a>)</span>, <span class="citation">Wiebe, Bruce, and O’Hara (<a href="#ref-wiebe1999development">1999</a>)</span>)</span> The lexicon based approach has the characteristic of being strongly context specific. This is an advantage when the goal is to design a method able to extract sentiment in a specific context <span class="citation">(Chiarello et al. <a href="#ref-chiarello2017product">2017</a>)</span>, but is a major shortcoming if the goal is to design a general purpose method.</p></li>
<li><p><em>Supervised Learning Approaches</em>: Sentiment analysis can be formulated as a document classification problem with three classes: positive, negative and neutral<span class="citation">(Mullen and Collier <a href="#ref-mullen2004sentiment">2004</a>)</span>. Training and test sets of documents are typically collected from product reviews, movies reviews or are created by scratch using manual annotation. Any learning algorithm can be applied to sentiment classification (naive Bayesian classification, and support vector machines <span class="citation">(Prabowo and Thelwall <a href="#ref-prabowo2009sentiment">2009</a>)</span>). The crucial phase for Supervised Learning sentiment analysis is the features presentation of the data. It was shown <span class="citation">(Pang, Lee, and Vaithyanathan <a href="#ref-pang2002thumbs">2002</a>)</span> that using uni-grams (a bag of individual words) as features in classification performed well with either naive Bayesian or SVM. Subsequent research used many more features and techniques in learning <span class="citation">(Pang, Lee, and others <a href="#ref-pang2008opinion">2008</a>)</span>.</p></li>
</ul>
</div>
<div id="sotatoolsmodelnetanal" class="section level3">
<h3><span class="header-section-number">7.5.4</span> Text Clustering</h3>
<p>The goal of clustering methods is to find groups of similar objects in the data thanks to the measure of a similarity function <span class="citation">(Jain and Dubes <a href="#ref-jain1988algorithms">1988</a>, <span class="citation">Kaufman and Rousseeuw (<a href="#ref-kaufman2009finding">2009</a>)</span>)</span>. Clustering techniques has been widely applied in the text domain, where the objects of the clustering can be documents (at different level of granularity) or terms. In the context of technical documents analysis Clustering is especially useful documents retrieval <span class="citation">(Anick and Vaithyanathan <a href="#ref-anick1997exploiting">1997</a>, <span class="citation">Cutting, Karger, and Pedersen (<a href="#ref-cutting1993constant">1993</a>)</span>)</span>. Clustering problems has been and are studied widely outside the text domain. Methods for clustering have been developed focusing on quantitative/non-textual data <span class="citation">(Guha, Rastogi, and Shim <a href="#ref-guha1998cure">1998</a>, <span class="citation">Han, Kamber, and Tung (<a href="#ref-han2001spatial">2001</a>)</span>, <span class="citation">Zhang, Ramakrishnan, and Livny (<a href="#ref-zhang1996birch">1996</a>)</span>)</span>.</p>
<p>In the context of text analysis, the problem of clustering finds applicability for a number of tasks, such as Document Organization and Browsing <span class="citation">(Cutting et al. <a href="#ref-cutting2017scatter">2017</a>)</span>, Corpus Stigmatization using documents maps <span class="citation">(Schütze and Silverstein <a href="#ref-schutze1997projections">1997</a>)</span> or word clusters <span class="citation">(Baker and McCallum <a href="#ref-baker1998distributional">1998</a>, <span class="citation">Bekkerman et al. (<a href="#ref-bekkerman2001feature">2001</a>)</span>)</span>. It is useful also to use a Soft clustering approach, that associates each document with multiple clusters with a given probability.</p>
<p>However, standard techniques for cluster analysis (k-means or hierarchical clustering) do not typically work well for clustering textual data in general or more specific technical documents. This is because of the unique characteristics of textual data which implies the design of specialized algorithms for the task.</p>
<p>The distinguishing characteristics of the text representation are the following <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>:</p>
<ul>
<li><p>There is a problem of course of dimensionality. The dimensionality of the bug-of-words representation is very large and the underlying data is sparse. In other words, the lexicon from which the documents are drawn may be of the order of millions, but a given document may contain only a few hundred words.This problem is even more serious for technical documents in which the lexicon is even more large.</p></li>
<li><p>The words are correlated with one another and thus the number of concepts (or principal components) in the data is much smaller than the feature space. This necessitates the careful design of algorithms which can account for word correlations in the clustering process.</p></li>
<li><p>The number of words (or non-zero entries) in the different documents may vary widely. Therefore, it is important to normalize the document representations appropriately during the clustering task.</p></li>
</ul>
<p>The problems of sparsity and high dimensionality necessitate the design of specific algorithms text processing. The topic has been heavily studied in the information retrieval literature where many techniques have been proposed <span class="citation">(Ricardo and Berthier <a href="#ref-ricardo2011modern">2011</a>)</span>.</p>
</div>
<div id="sotatoolsmodelner" class="section level3">
<h3><span class="header-section-number">7.5.5</span> Named Entity Recognition</h3>
<p>Named Entity Recognition is the task of identifying entity names like people, organizations, places, temporal expressions or numerical expressions. An example of an annotated sentence for a NER extraction system tailored for user entity extraction from patents, is the following:</p>
<p><em>Traditionally, &lt; user &gt; guitar players &lt; user/ &gt; or &lt; user &gt; players &lt; user/ &gt; of other stringed instruments may perform in any of a number of various positions, from seated, with the stringed instru- ment supported on the leg of the<user>performer<user/>, to standing or walking, with the stringed instrument suspended from a strap.</em></p>
<p>Methods and algorithms to deal with the entity extraction task are different, but the most effective are the ones based on supervised methods. Supervised methods tackle this task by extracting relevant statistics from an annotated corpus. These statistics are collected from the computation of features values, which are strong indicators for the identification of entities in the analyzed text. Features used in NLP for NER purposes are divided in two main categories: - Linguistically motivated features, such as n-gram of words (sequences of n words), lemma and part of speech - External resources features as, for example, external lists of entities that are candidates to be classified in the extraction process.</p>
<p>The annotation methods of a training corpus can be of two different kinds: human based, which is time expensive, but usually effective in the classification phase; automatically based, which can lead to annotation errors due to language ambiguity. For instance driver can be classified both as a user (the operator of a motor vehicle), or not a user (a program that determines how a computer will communicate with a peripheral device). Different training algorithms, such as Hidden Markov Models <span class="citation">(Eddy <a href="#ref-eddy1996hidden">1996</a><a href="#ref-eddy1996hidden">a</a>)</span>, Conditional Random Fields (CRF) <span class="citation">(Lafferty, McCallum, and Pereira <a href="#ref-lafferty2001conditional">2001</a><a href="#ref-lafferty2001conditional">a</a>)</span> Support Vector Machines (SVM) <span class="citation">(Hearst, Dumais, Osuna, et al. <a href="#ref-hearst1998support">1998</a>)</span>, or Bidirectional Long Short Term Mermory-CRF Neural Networks <span class="citation">(Lample et al. <a href="#ref-lample2016neural">2016</a>, <span class="citation">Misawa et al. (<a href="#ref-misawa2017character">2017</a>)</span>)</span> are used to build a statistical model based on features that are extracted from the analyzed documents in the training phase.</p>
</div>
<div id="sotatoolsmodeltopicmodel" class="section level3">
<h3><span class="header-section-number">7.5.6</span> Topic Modelling</h3>
<p>Topic modeling is a form of dimension reduction that uses probabilistic models to find the co-occurrence patterns of terms that correspond to semantic topics in a collection of documents <span class="citation">(Crain et al. <a href="#ref-crain2012dimensionality">2012</a>)</span>. To understand topic modelling it is useful to understand its differences with clustering <a href="sotatools.html#sotatoolsmodelnetanal">7.5.4</a> and the problem they both solves: the course of dimensionality. Both these techniques has in fact the goal of representing documents in such a way that they reveals their internal structure and interrelations. Clustering measures the similarity (or dissimilarity) between documents to place documents into groups. Representing each document by considering the belonging to a group, clustering induces a low-dimensional representation for documents. However, it is often difficult to characterize a cluster in terms of meaningful features because the clustering is independent of the document representation, given the computed similarity. Topic modeling integrates soft clustering (assigning each element to a cluster with a given probability and not with a Boolean variable) with dimension reduction. Each document is associated with a number of latent topics: a topic can be seed as both document clusters and compact group of words identified from a corpus. Each document is assigned to the topics with different weights: this feature can be seen both as the degree of membership in the clusters, as well as the coordinates of the document in the reduced dimension space. The result is an understandable representation of documents that is useful for analyzing the themes in documents. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model <span class="citation">(Blei, Ng, and Jordan <a href="#ref-blei2003latent">2003</a>)</span>. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.</p>
</div>
</div>
<div id="sotatoolsvisualize" class="section level2">
<h2><span class="header-section-number">7.6</span> Visualize</h2>
<p>Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests: the process of developing the outputs (the paper, the report, the dashboard, or other deliverable) is less frequently analyzed. This problem influences also text mining <span class="citation">(Parker <a href="#ref-parker2017opinionated">2017</a>)</span>. One of the most studied problems of output production is data visualisation. Data visualisation involves the creation and study of the visual representation of data <span class="citation">(Friendly and Denis <a href="#ref-friendly2001milestones">2001</a>)</span>. Data visualization uses statistical graphics, plots, information graphics and other tools to communicate information in a clearl and efficient way. The main process of data visualisation is the visual encoding of numbers. Numerical data may be encoded in many ways, using a wide range of shapes: the main used are dots, lines, and bars <span class="citation">(Wickham <a href="#ref-wickham2016ggplot2">2016</a>)</span>. The main goal of visualizations is to help users (students, researchers, companies and many others) analyze and reason about evidences hidden in data. It is possible thanks to the ability of visualisation to make complex data more accessible, understandable and usable. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.</p>
<p>Da visualisation has become in the last year a well enstablished discipline thanks to the increased amounts of data created by Internet activity and an expanding number of sensors in the environment are referred to as “big data” or Internet of things. It is important to undeline how the way this data is comunicated present ethical and analytical challenges for data visualization practitioner <span class="citation">(Bikakis <a href="#ref-bikakis2018big">2018</a>)</span>. The field of data science and practitioners called data scientists help address this challenge <span class="citation">(Loukides <a href="#ref-loukides2011data">2011</a>)</span>.</p>
<p>Users of information displays are executing (consciously or not) particular analytical tasks such as making comparisons or determining causality <span class="citation">(Tufte, Goeler, and Benson <a href="#ref-tufte1990envisioning">1990</a>)</span>. The design principle of the information graphic should thus support the analytical task, showing the comparison or causality <span class="citation">(Tufte <a href="#ref-tufte2006beautiful">2006</a>)</span>.</p>
<p>Graphical displays and principles for effective graphical display is defined as the ability to communicate complex statistical and quantitative ideas with clarity, precision and efficiency <span class="citation">(Mulrow <a href="#ref-mulrow2002visual">2002</a>)</span>. For this reason graphical displays should:</p>
<ul>
<li>show the data</li>
<li>induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production or something else</li>
<li>avoid distorting what the data has to say</li>
<li>present many numbers in a small space</li>
<li>make large data sets coherent</li>
<li>encourage the eye to compare different pieces of data</li>
<li>reveal the data at several levels of detail, from a broad overview to the fine structure</li>
<li>serve a reasonably clear purpose: description, exploration, tabulation or decoration</li>
<li>be closely integrated with the statistical and verbal descriptions of a data set</li>
</ul>
<p>In litterature are identified eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message <span class="citation">(Few <a href="#ref-few2012show">2012</a>)</span>:</p>
<ul>
<li>Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.</li>
<li>Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.</li>
<li>Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.</li>
<li>Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.</li>
<li>Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis. A boxplot helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc.</li>
<li>Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.</li>
<li>Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.</li>
<li>Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.</li>
</ul>
<p>Data visualisation practionires has to consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis.</p>
<div id="the-grammar-of-graphics" class="section level3">
<h3><span class="header-section-number">7.6.1</span> The Grammar of Graphics</h3>
<p>Even if trial and error is and will remain an important part of data visualisation, some works as tried to give to data visualisation practitioners a well structured framework able to guide the process of data visualisation. Among the many frameworks, the most used is the <em>Grammar of Graphics</em> <span class="citation">(Wilkinson <a href="#ref-wilkinson2006grammar">2006</a>)</span> and its implementation <span class="citation">(Wickham, Chang, and others <a href="#ref-wickham2008ggplot2">2008</a>)</span>. The grammar of graphics is a coherent system for describing and building graphs. Like other kind of grammars, it describes to basic rules to use the element of data visualization with the goal of comunicating some content. The main concept in the grammar of graphics is that graphs are made by multiple layers. Layers are responsible for creating the objects that we perceive on the plot. A layer is composed of four parts:</p>
<ul>
<li><em>Data and aesthetic mapping</em>: Data are independent from the other components: we can construct a graphic that can be applied to multiple datasets. Along with the data, we need a specification of which variables are mapped to which aesthetics.</li>
<li><em>Statistical transformation</em>: A statistical transformation transforms the data, typically by summarizing them in some manner.</li>
<li><em>Geometric object</em>: Geometric objects control the type of plot that is created. For example, using a point geom will create a scatterplot, whereas using a line geom will create a line plot. Geometric objects can be classified by their dimensionality.</li>
<li><em>Position adjustment</em>: Sometimes there exist th need to tweak the position of the geometric elements on the plot, when otherwise they would obscure each other. This is most common in bar plots, where we stack or dodge (place side-by-side) the bars to avoid overlaps.</li>
</ul>
<p>Multyple layers togheter are used to create complex plots.</p>
<p>Togheter with the layer the designer can control the <em>scale.</em> A scale controls the mapping from data to aesthetic attributes, and one scale for each aesthetic property used in a layer is needed. Scales are common across layers to ensure a consistent mapping from data to aesthetics.</p>
<p>After the decision of the scale, the designer has ti decid the <em>coordinate system</em> for the layer. A coordinate system maps the position of objects onto the plane of the plot. Position is often specified by two coordinates (x, y), but could be any number of coordinates. The Cartesian coordinate system is the most common coordinate system for two dimensions, whereas polar coordinates and various map projections are used less frequently. For higher dimensions, we have parallel coordinates (a projective geometry), mosaic plots (a hierarchical coordinate system), and linear projections onto the plane. Coordinate systems affect all position variables simultaneously and differ from scales in that they also change the appearance of the geometric objects.</p>
<p>Finally, the last element of the grammar are <em>facets</em>. Faceting makes it easy to create small multiples of different subsets of an entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions. The faceting specification describes which variables should be used to split up the data, and how they should be arranged.</p>
</div>
</div>
<div id="sotatoolscomunicate" class="section level2">
<h2><span class="header-section-number">7.7</span> Comunicate</h2>
<p>The last task to perform in the process of knowledge extraction from technical documents is communications. If it means to comunicate the results of an anlysis inside a team or to the world, it doesn not matter how great an analysis is unless it is impossibile to explain it to others <span class="citation">(Wickham and Grolemund <a href="#ref-wickham2016r">2016</a>)</span>. For the purposes of the present thesis, the focus is on the review of technical mechanics of communication especialli in the R <span class="citation">(R Core Team <a href="#ref-R-base">2018</a>)</span> enviroment. One of the most important innovation for the task of communication in data science is R Markdown <span class="citation">(Allaire et al. <a href="#ref-R-rmarkdown">2018</a>)</span>. R Markdown provides an unified authoring framework for data science, combining code, results, and comments. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.</p>
<p>R Markdown files are designed to be used in three ways:</p>
<ul>
<li><p>For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis.</p></li>
<li><p>For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them ( i.e. the code).</p></li>
<li><p>As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.</p></li>
</ul>
<p>Togheter with reports (and usually contained in them) there are visualisation. Making graphics for communication follow all the rules and framework previously revised in section <a href="sotatools.html#sotatoolsvisualize">7.6</a>, but when a graph has to be used to comunicate to a wide audience there are some more rules to follow. The reason why this happen is that the audience likely do not share the background knowledge of the anlysit and do not be deeply invested in the data. To help others quickly build up a good mental model of the data, the analyst need to invest considerable effort in making plots as self-explanatory as possible. For this reason has been developed many tools to help data scientist to make effective comunication graphs<span class="citation">(Wickham <a href="#ref-wickham2016ggplot2">2016</a>, <span class="citation">Chang et al. (<a href="#ref-shiny2017">2017</a>)</span>, <span class="citation">Sievert et al. (<a href="#ref-plotly2017">2017</a>)</span>, <span class="citation">Pedersen (<a href="#ref-ggprah2018">2018</a>)</span>, <span class="citation">Bastian, Heymann, and Jacomy (<a href="#ref-ICWSM09154">2009</a>)</span>)</span> .</p>
</div>
<div id="sotadocumentsunderstand" class="section level2">
<h2><span class="header-section-number">7.8</span> Understand</h2>
<p>The most difficult challenge in technology intelligence is not how to detect the large trends- they are visible anyway. It is, rather, how to detect weak signals, or information that initially appears with low frequency, in unrelated or unexpected regions of the technology landscape, and associated with large noise (Apreda et al. 2016). These signals escape from traditional statistical detection techniques, exactly because it is difficult to distinguish them from pure statistical noise. Metadata are not the appropriate source of data for detecting weak signals. As a matter of fact, they can be detected only by using a fine-grained domain knowledge structure, or using the full text of documents. As an example, classification-based clustering has been shown to be flawed because the patent class used is usually only the first one listed in patents, generating loss of granularity (Benner and Waldfogel, 2008; Aharonson and Schilling, 2016).</p>
<p>Hypothesis</p>
<p>postulation</p>
<div id="domain-expertise" class="section level3">
<h3><span class="header-section-number">7.8.1</span> Domain Expertise</h3>
<p>(collins)</p>
<p>Sheela Jasanow</p>
<p>Taleb?</p>
</div>
<div id="sotadocumentsunderstandbyas" class="section level3">
<h3><span class="header-section-number">7.8.2</span> The problem of byases</h3>
<p>Each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs. The average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them. Moreover, it is also known that human analysis of text information is subject to considerable biases, e.g., people often pay greater attention to opinions that are consistent with their own preferences. People also have difficulty, owing to their mental and physical limitations, producing consistent results when the amount of information to be processed is large. Automated opinion mining and summarization systems are thus needed, as subjective biases and mental limitations can be overcome with an objective sentiment analysis system.</p>
</div>
<div id="sotadocumentsunderstandlexicons" class="section level3">
<h3><span class="header-section-number">7.8.3</span> The Importance of Lexicons for Technical Documents Analysis</h3>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wickham2016r">
<p>Wickham, Hadley, and Garrett Grolemund. 2016. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-r2008">
<p>R Development Core Team. 2008. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="http://www.R-project.org" class="uri">http://www.R-project.org</a>.</p>
</div>
<div id="ref-py95">
<p>Rossum, Guido. 1995. “Python Reference Manual.” Amsterdam, The Netherlands, The Netherlands: CWI (Centre for Mathematics; Computer Science).</p>
</div>
<div id="ref-yuwono1996search">
<p>Yuwono, Budi, and Dik Lun Lee. 1996. “Search and Ranking Algorithms for Locating Resources on the World Wide Web.” In <em>Data Engineering, 1996. Proceedings of the Twelfth International Conference on</em>, 164–71. IEEE.</p>
</div>
<div id="ref-readr2017r">
<p>Wickham, Hadley, Jim Hester, and Romain Francois. 2017. <em>Readr: Read Rectangular Text Data</em>. <a href="https://CRAN.R-project.org/package=readr" class="uri">https://CRAN.R-project.org/package=readr</a>.</p>
</div>
<div id="ref-wickham2014tidy">
<p>Wickham, Hadley, and others. 2014. “Tidy Data.” <em>Journal of Statistical Software</em> 59 (10). Foundation for Open Access Statistics: 1–23.</p>
</div>
<div id="ref-leek2015elements">
<p>Leek, Jeff. 2015. “The Elements of Data Analytic Style.” <em>J. Leek.—Amazon Digital Services, Inc</em>.</p>
</div>
<div id="ref-silge2016tidytext">
<p>Silge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” <em>The Journal of Open Source Software</em> 1 (3): 37.</p>
</div>
<div id="ref-mctear2016conversational">
<p>McTear, Michael, Zoraida Callejas, and David Griol. 2016. <em>The Conversational Interface: Talking to Smart Devices</em>. Springer.</p>
</div>
<div id="ref-dell2009ensemble">
<p>Dell’Orletta, Felice. 2009. “Ensemble System for Part-of-Speech Tagging.” <em>Proceedings of EVALITA</em> 9: 1–8.</p>
</div>
<div id="ref-attardi2009reverse">
<p>Attardi, Giuseppe, and Felice Dell’Orletta. 2009. “Reverse Revision and Linear Tree Combination for Dependency Parsing.” In <em>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</em>, 261–64. Association for Computational Linguistics.</p>
</div>
<div id="ref-attardi2009accurate">
<p>Attardi, Giuseppe, Felice Dell’Orletta, Maria Simi, and Joseph Turian. 2009. “Accurate Dependency Parsing with a Stacked Multilayer Perceptron.” <em>Proceedings of EVALITA</em> 9: 1–8.</p>
</div>
<div id="ref-porter1980algorithm">
<p>Porter, Martin F. 1980. “An Algorithm for Suffix Stripping.” <em>Program</em> 14 (3). MCB UP Ltd: 130–37.</p>
</div>
<div id="ref-hankamer1989morphological">
<p>Hankamer, Jorge. 1989. “Morphological Parsing and the Lexicon.” In <em>Lexical Representation and Process</em>, 392–408. MIT Press.</p>
</div>
<div id="ref-attardi2006experiments">
<p>Attardi, Giuseppe. 2006. “Experiments with a Multilanguage Non-Projective Dependency Parser.” In <em>Proceedings of the Tenth Conference on Computational Natural Language Learning</em>, 166–70. Association for Computational Linguistics.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
<div id="ref-lafferty2001document">
<p>Lafferty, John, and Chengxiang Zhai. 2001. “Document Language Models, Query Models, and Risk Minimization for Information Retrieval.” In <em>Proceedings of the 24th Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 111–19. ACM.</p>
</div>
<div id="ref-bellegarda2004statistical">
<p>Bellegarda, Jerome R. 2004. “Statistical Language Model Adaptation: Review and Perspectives.” <em>Speech Communication</em> 42 (1). Elsevier: 93–108.</p>
</div>
<div id="ref-brants2012large">
<p>Brants, Thorsten, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2012. “Large Language Models in Machine Translation.” Google Patents.</p>
</div>
<div id="ref-gibson1994tools">
<p>Gibson, Kathleen R, Kathleen Rita Gibson, and Tim Ingold. 1994. <em>Tools, Language and Cognition in Human Evolution</em>. Cambridge University Press.</p>
</div>
<div id="ref-borko1963automatic">
<p>Borko, Harold, and Myrna Bernick. 1963. “Automatic Document Classification.” <em>Journal of the ACM (JACM)</em> 10 (2). ACM: 151–62.</p>
</div>
<div id="ref-aggarwal2012survey">
<p>Aggarwal, Charu C, and ChengXiang Zhai. 2012. “A Survey of Text Classification Algorithms.” In <em>Mining Text Data</em>, 163–222. Springer.</p>
</div>
<div id="ref-sebastiani2002machine">
<p>Sebastiani, Fabrizio. 2002. “Machine Learning in Automated Text Categorization.” <em>ACM Computing Surveys (CSUR)</em> 34 (1). ACM: 1–47.</p>
</div>
<div id="ref-yang2004building">
<p>Yang, Qiang, Tianyi Li, and Ke Wang. 2004. “Building Association-Rule Based Sequential Classifiers for Web-Document Prediction.” <em>Data Mining and Knowledge Discovery</em> 8 (3). Springer: 253–73.</p>
</div>
<div id="ref-joachims1998text">
<p>Joachims, Thorsten. 1998. “Text Categorization with Support Vector Machines: Learning with Many Relevant Features.” In <em>European Conference on Machine Learning</em>, 137–42. Springer.</p>
</div>
<div id="ref-manevitz2001one">
<p>Manevitz, Larry M, and Malik Yousef. 2001. “One-Class Svms for Document Classification.” <em>Journal of Machine Learning Research</em> 2 (Dec): 139–54.</p>
</div>
<div id="ref-pop2006approach">
<p>Pop, Ioan. 2006. “An Approach of the Naive Bayes Classifier for the Document Classification.” <em>General Mathematics</em> 14 (4). University: 135–38.</p>
</div>
<div id="ref-manevitz2007one">
<p>Manevitz, Larry, and Malik Yousef. 2007. “One-Class Document Classification via Neural Networks.” <em>Neurocomputing</em> 70 (7-9). Elsevier: 1466–81.</p>
</div>
<div id="ref-lai2015recurrent">
<p>Lai, Siwei, Liheng Xu, Kang Liu, and Jun Zhao. 2015. “Recurrent Convolutional Neural Networks for Text Classification.” In <em>AAAI</em>, 333:2267–73.</p>
</div>
<div id="ref-kim2014convolutional">
<p>Kim, Yoon. 2014. “Convolutional Neural Networks for Sentence Classification.” <em>arXiv Preprint arXiv:1408.5882</em>.</p>
</div>
<div id="ref-pang2008opinion">
<p>Pang, Bo, Lillian Lee, and others. 2008. “Opinion Mining and Sentiment Analysis.” <em>Foundations and Trends in Information Retrieval</em> 2 (1–2). Now Publishers, Inc.: 1–135.</p>
</div>
<div id="ref-liu2012survey">
<p>Liu, Bing, and Lei Zhang. 2012. “A Survey of Opinion Mining and Sentiment Analysis.” In <em>Mining Text Data</em>, 415–63. Springer.</p>
</div>
<div id="ref-miller1995wordnet">
<p>Miller, George A. 1995. “WordNet: A Lexical Database for English.” <em>Communications of the ACM</em> 38 (11). ACM: 39–41.</p>
</div>
<div id="ref-hu2004mining">
<p>Hu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer Reviews.” In <em>Proceedings of the Tenth Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 168–77. ACM.</p>
</div>
<div id="ref-kim2004determining">
<p>Kim, Soo-Min, and Eduard Hovy. 2004. “Determining the Sentiment of Opinions.” In <em>Proceedings of the 20th International Conference on Computational Linguistics</em>, 1367. Association for Computational Linguistics.</p>
</div>
<div id="ref-ding2008holistic">
<p>Ding, Xiaowen, Bing Liu, and Philip S Yu. 2008. “A Holistic Lexicon-Based Approach to Opinion Mining.” In <em>Proceedings of the 2008 International Conference on Web Search and Data Mining</em>, 231–40. ACM.</p>
</div>
<div id="ref-baccianella2010sentiwordnet">
<p>Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. 2010. “Sentiwordnet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.” In <em>Lrec</em>, 10:2200–2204. 2010.</p>
</div>
<div id="ref-philip1966general">
<p>Philip, J, Dexter C Dunphy, Marshall S Smith, Daniel M Ogilvie, and others. 1966. “The General Inquirer: A Computer Approach to Content Analysis.” The MIT Press Cambridge.</p>
</div>
<div id="ref-wiebe1999development">
<p>Wiebe, Janyce M, Rebecca F Bruce, and Thomas P O’Hara. 1999. “Development and Use of a Gold-Standard Data Set for Subjectivity Classifications.” In <em>Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics</em>, 246–53. Association for Computational Linguistics.</p>
</div>
<div id="ref-chiarello2017product">
<p>Chiarello, Filippo, Gualtiero Fantoni, Andrea Bonaccorsi, and others. 2017. “Product Description in Terms of Advantages and Drawbacks: Exploiting Patent Information in Novel Ways.” In <em>DS 87-6 Proceedings of the 21st International Conference on Engineering Design (Iced 17) Vol 6: Design Information and Knowledge, Vancouver, Canada, 21-25.08. 2017</em>, 101–10.</p>
</div>
<div id="ref-mullen2004sentiment">
<p>Mullen, Tony, and Nigel Collier. 2004. “Sentiment Analysis Using Support Vector Machines with Diverse Information Sources.” In <em>Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</em>.</p>
</div>
<div id="ref-prabowo2009sentiment">
<p>Prabowo, Rudy, and Mike Thelwall. 2009. “Sentiment Analysis: A Combined Approach.” <em>Journal of Informetrics</em> 3 (2). Elsevier: 143–57.</p>
</div>
<div id="ref-pang2002thumbs">
<p>Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. “Thumbs up?: Sentiment Classification Using Machine Learning Techniques.” In <em>Proceedings of the Acl-02 Conference on Empirical Methods in Natural Language Processing-Volume 10</em>, 79–86. Association for Computational Linguistics.</p>
</div>
<div id="ref-jain1988algorithms">
<p>Jain, Anil K, and Richard C Dubes. 1988. “Algorithms for Clustering Data.” Prentice-Hall, Inc.</p>
</div>
<div id="ref-kaufman2009finding">
<p>Kaufman, Leonard, and Peter J Rousseeuw. 2009. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. Vol. 344. John Wiley; Sons.</p>
</div>
<div id="ref-anick1997exploiting">
<p>Anick, Peter G, and Shivakumar Vaithyanathan. 1997. “Exploiting Clustering and Phrases for Context-Based Information Retrieval.” In <em>ACM Sigir Forum</em>, 31:314–23. SI. ACM.</p>
</div>
<div id="ref-cutting1993constant">
<p>Cutting, Douglass R, David R Karger, and Jan O Pedersen. 1993. “Constant Interaction-Time Scatter/Gather Browsing of Very Large Document Collections.” In <em>Proceedings of the 16th Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 126–34. ACM.</p>
</div>
<div id="ref-guha1998cure">
<p>Guha, Sudipto, Rajeev Rastogi, and Kyuseok Shim. 1998. “CURE: An Efficient Clustering Algorithm for Large Databases.” In <em>ACM Sigmod Record</em>, 27:73–84. 2. ACM.</p>
</div>
<div id="ref-han2001spatial">
<p>Han, Jiawei, Micheline Kamber, and Anthony KH Tung. 2001. “Spatial Clustering Methods in Data Mining.” <em>Geographic Data Mining and Knowledge Discovery</em>. Cambridge, UK: Cambridge University Press, 188–217.</p>
</div>
<div id="ref-zhang1996birch">
<p>Zhang, Tian, Raghu Ramakrishnan, and Miron Livny. 1996. “BIRCH: An Efficient Data Clustering Method for Very Large Databases.” In <em>ACM Sigmod Record</em>, 25:103–14. 2. ACM.</p>
</div>
<div id="ref-cutting2017scatter">
<p>Cutting, Douglass R, David R Karger, Jan O Pedersen, and John W Tukey. 2017. “Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.” In <em>ACM Sigir Forum</em>, 51:148–59. 2. ACM.</p>
</div>
<div id="ref-schutze1997projections">
<p>Schütze, Hinrich, and Craig Silverstein. 1997. “Projections for Efficient Document Clustering.” In <em>ACM Sigir Forum</em>, 31:74–81. SI. ACM.</p>
</div>
<div id="ref-baker1998distributional">
<p>Baker, L Douglas, and Andrew Kachites McCallum. 1998. “Distributional Clustering of Words for Text Classification.” In <em>Proceedings of the 21st Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 96–103. ACM.</p>
</div>
<div id="ref-bekkerman2001feature">
<p>Bekkerman, Ron, Ran El-Yaniv, Naftali Tishby, and Yoad Winter. 2001. “On Feature Distributional Clustering for Text Categorization.” In <em>Proceedings of the 24th Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 146–53. ACM.</p>
</div>
<div id="ref-ricardo2011modern">
<p>Ricardo, BY, and RN Berthier. 2011. “Modern Information Retrieval: The Concepts and Technology Behind Search Second Edition.” <em>Addision Wesley</em> 84 (2).</p>
</div>
<div id="ref-eddy1996hidden">
<p>Eddy, Sean R. 1996a. “Hidden Markov Models.” <em>Current Opinion in Structural Biology</em> 6 (3). Elsevier: 361–65.</p>
</div>
<div id="ref-lafferty2001conditional">
<p>Lafferty, John, Andrew McCallum, and Fernando CN Pereira. 2001a. “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.”</p>
</div>
<div id="ref-hearst1998support">
<p>Hearst, Marti A., Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. 1998. “Support Vector Machines.” <em>IEEE Intelligent Systems and Their Applications</em> 13 (4). IEEE: 18–28.</p>
</div>
<div id="ref-lample2016neural">
<p>Lample, Guillaume, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. “Neural Architectures for Named Entity Recognition.” <em>arXiv Preprint arXiv:1603.01360</em>.</p>
</div>
<div id="ref-misawa2017character">
<p>Misawa, Shotaro, Motoki Taniguchi, Yasuhide Miura, and Tomoko Ohkuma. 2017. “Character-Based Bidirectional Lstm-Crf with Words and Characters for Japanese Named Entity Recognition.” In <em>Proceedings of the First Workshop on Subword and Character Level Models in Nlp</em>, 97–102.</p>
</div>
<div id="ref-crain2012dimensionality">
<p>Crain, Steven P, Ke Zhou, Shuang-Hong Yang, and Hongyuan Zha. 2012. “Dimensionality Reduction and Topic Modeling: From Latent Semantic Indexing to Latent Dirichlet Allocation and Beyond.” In <em>Mining Text Data</em>, 129–61. Springer.</p>
</div>
<div id="ref-blei2003latent">
<p>Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3 (Jan): 993–1022.</p>
</div>
<div id="ref-parker2017opinionated">
<p>Parker, Hilary. 2017. “Opinionated Analysis Development.” <em>PeerJ Preprints</em> 5. PeerJ Inc. San Francisco, USA: e3210v1.</p>
</div>
<div id="ref-friendly2001milestones">
<p>Friendly, Michael, and Daniel J Denis. 2001. “Milestones in the History of Thematic Cartography, Statistical Graphics, and Data Visualization.” <em>URL Http://Www. Datavis. ca/Milestones</em> 32: 13.</p>
</div>
<div id="ref-wickham2016ggplot2">
<p>Wickham, Hadley. 2016. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer.</p>
</div>
<div id="ref-bikakis2018big">
<p>Bikakis, Nikos. 2018. “Big Data Visualization Tools.” <em>arXiv Preprint arXiv:1801.08336</em>.</p>
</div>
<div id="ref-loukides2011data">
<p>Loukides, Mike. 2011. <em>What Is Data Science?</em> “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-tufte1990envisioning">
<p>Tufte, Edward R, Nora Hillman Goeler, and Richard Benson. 1990. <em>Envisioning Information</em>. Vol. 126. Graphics press Cheshire, CT.</p>
</div>
<div id="ref-tufte2006beautiful">
<p>Tufte, Edward R. 2006. <em>Beautiful Evidence</em>. Vol. 1. Graphics Press Cheshire, CT.</p>
</div>
<div id="ref-mulrow2002visual">
<p>Mulrow, Edward J. 2002. “The Visual Display of Quantitative Information.” Taylor &amp; Francis.</p>
</div>
<div id="ref-few2012show">
<p>Few, Stephen. 2012. <em>Show Me the Numbers: Designing Tables and Graphs to Enlighten</em>. Analytics Press.</p>
</div>
<div id="ref-wilkinson2006grammar">
<p>Wilkinson, Leland. 2006. <em>The Grammar of Graphics</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-wickham2008ggplot2">
<p>Wickham, Hadley, Winston Chang, and others. 2008. “Ggplot2: An Implementation of the Grammar of Graphics.” <em>R Package Version 0.7, URL: Http://CRAN. R-Project. Org/Package= Ggplot2</em>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2018. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-R-rmarkdown">
<p>Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, and Winston Chang. 2018. <em>Rmarkdown: Dynamic Documents for R</em>. <a href="https://CRAN.R-project.org/package=rmarkdown" class="uri">https://CRAN.R-project.org/package=rmarkdown</a>.</p>
</div>
<div id="ref-shiny2017">
<p>Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2017. <em>Shiny: Web Application Framework for R</em>. <a href="https://CRAN.R-project.org/package=shiny" class="uri">https://CRAN.R-project.org/package=shiny</a>.</p>
</div>
<div id="ref-plotly2017">
<p>Sievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2017. <em>Plotly: Create Interactive Web Graphics via ’Plotly.js’</em>. <a href="https://CRAN.R-project.org/package=plotly" class="uri">https://CRAN.R-project.org/package=plotly</a>.</p>
</div>
<div id="ref-ggprah2018">
<p>Pedersen, Thomas Lin. 2018. <em>Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks</em>. <a href="https://CRAN.R-project.org/package=ggraph" class="uri">https://CRAN.R-project.org/package=ggraph</a>.</p>
</div>
<div id="ref-ICWSM09154">
<p>Bastian, Mathieu, Sebastien Heymann, and Mathieu Jacomy. 2009. “Gephi: An Open Source Software for Exploring and Manipulating Networks.”</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="knowledge-intensive-manegement-engineering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sotadocuments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["thesis_source.pdf", "thesis_source.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
