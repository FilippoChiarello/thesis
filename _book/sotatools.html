<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="This document contains the PhD thesis of Filippo Chiarello.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  <meta name="twitter:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="structure-and-rationale.html">
<link rel="next" href="sotadocuments.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Text Mining Techniques for Knowledge Extraction from Technical Documents</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="introproblems.html"><a href="introproblems.html"><i class="fa fa-check"></i><b>1</b> Problems and Goals</a><ul>
<li class="chapter" data-level="1.1" data-path="introproblems.html"><a href="introproblems.html#big-data-many-information-less-knowledge"><i class="fa fa-check"></i><b>1.1</b> Big Data, Many Information, Less Knowledge?</a></li>
<li class="chapter" data-level="1.2" data-path="introproblems.html"><a href="introproblems.html#a-new-challenge-for-management-engineers"><i class="fa fa-check"></i><b>1.2</b> A New Challenge for Management Engineers</a></li>
<li class="chapter" data-level="1.3" data-path="introproblems.html"><a href="introproblems.html#human-machines"><i class="fa fa-check"></i><b>1.3</b> Human &amp; Machines</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>2</b> Solutions</a><ul>
<li class="chapter" data-level="2.1" data-path="solutions.html"><a href="solutions.html#introdesres"><i class="fa fa-check"></i><b>2.1</b> The Design Science Approach</a></li>
<li class="chapter" data-level="2.2" data-path="solutions.html"><a href="solutions.html#design-processes-for-knowledge-extraction-the-data-science-workflow"><i class="fa fa-check"></i><b>2.2</b> Design Processes for Knowledge Extraction: the Data Science Workflow</a></li>
<li class="chapter" data-level="2.3" data-path="solutions.html"><a href="solutions.html#the-exploitation-of-natural-language-processing-techniques"><i class="fa fa-check"></i><b>2.3</b> The Exploitation of Natural Language Processing Techniques</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="structure-and-rationale.html"><a href="structure-and-rationale.html"><i class="fa fa-check"></i><b>3</b> Structure and rationale</a></li>
<li class="part"><span><b>II State of the Art</b></span></li>
<li class="chapter" data-level="4" data-path="sotatools.html"><a href="sotatools.html"><i class="fa fa-check"></i><b>4</b> Phases, Tasks, and Techniques</a><ul>
<li class="chapter" data-level="4.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolsprogram"><i class="fa fa-check"></i><b>4.1</b> Program</a></li>
<li class="chapter" data-level="4.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimport"><i class="fa fa-check"></i><b>4.2</b> Import</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimportretrieval"><i class="fa fa-check"></i><b>4.2.1</b> Document Retrieval</a></li>
<li class="chapter" data-level="4.2.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimportformat"><i class="fa fa-check"></i><b>4.2.2</b> Documents Format</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolstidy"><i class="fa fa-check"></i><b>4.3</b> Tidy</a></li>
<li class="chapter" data-level="4.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransform"><i class="fa fa-check"></i><b>4.4</b> Transform</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformsentencesplit"><i class="fa fa-check"></i><b>4.4.1</b> Sentence Splitting</a></li>
<li class="chapter" data-level="4.4.2" data-path="sotatools.html"><a href="sotatools.html#tokenization"><i class="fa fa-check"></i><b>4.4.2</b> Tokenization</a></li>
<li class="chapter" data-level="4.4.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformstemming"><i class="fa fa-check"></i><b>4.4.3</b> Stemming</a></li>
<li class="chapter" data-level="4.4.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformlemmatisation"><i class="fa fa-check"></i><b>4.4.4</b> Lemmatisation</a></li>
<li class="chapter" data-level="4.4.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformwi"><i class="fa fa-check"></i><b>4.4.5</b> Words importance metrics</a></li>
<li class="chapter" data-level="4.4.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformpos"><i class="fa fa-check"></i><b>4.4.6</b> Part-of-Speech Tagging</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodel"><i class="fa fa-check"></i><b>4.5</b> Model</a><ul>
<li class="chapter" data-level="4.5.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformngrams"><i class="fa fa-check"></i><b>4.5.1</b> N-Grams</a></li>
<li class="chapter" data-level="4.5.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodeldocclass"><i class="fa fa-check"></i><b>4.5.2</b> Document Classification</a></li>
<li class="chapter" data-level="4.5.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelsentanal"><i class="fa fa-check"></i><b>4.5.3</b> Sentiment Analsysis</a></li>
<li class="chapter" data-level="4.5.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelnetanal"><i class="fa fa-check"></i><b>4.5.4</b> Text Clustering</a></li>
<li class="chapter" data-level="4.5.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelner"><i class="fa fa-check"></i><b>4.5.5</b> Named Entity Recognition</a></li>
<li class="chapter" data-level="4.5.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodeltopicmodel"><i class="fa fa-check"></i><b>4.5.6</b> Topic Modelling</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolsvisualize"><i class="fa fa-check"></i><b>4.6</b> Visualize</a><ul>
<li class="chapter" data-level="4.6.1" data-path="sotatools.html"><a href="sotatools.html#the-grammar-of-graphics"><i class="fa fa-check"></i><b>4.6.1</b> The Grammar of Graphics</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="sotatools.html"><a href="sotatools.html#sotatoolscomunicate"><i class="fa fa-check"></i><b>4.7</b> Comunicate</a></li>
<li class="chapter" data-level="4.8" data-path="sotatools.html"><a href="sotatools.html#sotadocumentsunderstand"><i class="fa fa-check"></i><b>4.8</b> Understand</a><ul>
<li class="chapter" data-level="4.8.1" data-path="sotatools.html"><a href="sotatools.html#sotadocumentsunderstandbyas"><i class="fa fa-check"></i><b>4.8.1</b> The problem of byases</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="sotadocuments.html"><a href="sotadocuments.html"><i class="fa fa-check"></i><b>5</b> Documents</a><ul>
<li class="chapter" data-level="5.1" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentspatents"><i class="fa fa-check"></i><b>5.1</b> Patents</a><ul>
<li class="chapter" data-level="5.1.1" data-path="sotadocuments.html"><a href="sotadocuments.html#metadata-approaches"><i class="fa fa-check"></i><b>5.1.1</b> Metadata Approaches</a></li>
<li class="chapter" data-level="5.1.2" data-path="sotadocuments.html"><a href="sotadocuments.html#keywords-approaches"><i class="fa fa-check"></i><b>5.1.2</b> Keywords Approaches</a></li>
<li class="chapter" data-level="5.1.3" data-path="sotadocuments.html"><a href="sotadocuments.html#natural-language-processing-approaches"><i class="fa fa-check"></i><b>5.1.3</b> Natural Language Processing approaches</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentspapers"><i class="fa fa-check"></i><b>5.2</b> Papers</a></li>
<li class="chapter" data-level="5.3" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentswiki"><i class="fa fa-check"></i><b>5.3</b> Wikipedia</a></li>
<li class="chapter" data-level="5.4" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentstwitter"><i class="fa fa-check"></i><b>5.4</b> Social Media</a><ul>
<li class="chapter" data-level="5.4.1" data-path="sotadocuments.html"><a href="sotadocuments.html#economics"><i class="fa fa-check"></i><b>5.4.1</b> Economics</a></li>
<li class="chapter" data-level="5.4.2" data-path="sotadocuments.html"><a href="sotadocuments.html#marketing"><i class="fa fa-check"></i><b>5.4.2</b> Marketing</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Methods and Results</b></span></li>
<li class="chapter" data-level="6" data-path="patents.html"><a href="patents.html"><i class="fa fa-check"></i><b>6</b> Patents</a><ul>
<li class="chapter" data-level="6.1" data-path="patents.html"><a href="patents.html#usersresults"><i class="fa fa-check"></i><b>6.1</b> Users</a><ul>
<li class="chapter" data-level="6.1.1" data-path="patents.html"><a href="patents.html#method"><i class="fa fa-check"></i><b>6.1.1</b> Method</a></li>
<li class="chapter" data-level="6.1.2" data-path="patents.html"><a href="patents.html#results"><i class="fa fa-check"></i><b>6.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="patents.html"><a href="patents.html#advdrwresults"><i class="fa fa-check"></i><b>6.2</b> Advantages and Drawbacks</a><ul>
<li class="chapter" data-level="6.2.1" data-path="patents.html"><a href="patents.html#methodology"><i class="fa fa-check"></i><b>6.2.1</b> Methodology</a></li>
<li class="chapter" data-level="6.2.2" data-path="patents.html"><a href="patents.html#results-1"><i class="fa fa-check"></i><b>6.2.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="patents.html"><a href="patents.html#trademakrs"><i class="fa fa-check"></i><b>6.3</b> Trademakrs</a><ul>
<li class="chapter" data-level="6.3.1" data-path="patents.html"><a href="patents.html#methodology-1"><i class="fa fa-check"></i><b>6.3.1</b> Methodology</a></li>
<li class="chapter" data-level="6.3.2" data-path="patents.html"><a href="patents.html#results-2"><i class="fa fa-check"></i><b>6.3.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="papers.html"><a href="papers.html"><i class="fa fa-check"></i><b>7</b> Papers</a><ul>
<li class="chapter" data-level="7.1" data-path="papers.html"><a href="papers.html#sustainable-manufacturing-an-analysis-of-the-6r-framework"><i class="fa fa-check"></i><b>7.1</b> Sustainable Manufacturing: an Analysis of the 6R Framework</a><ul>
<li class="chapter" data-level="7.1.1" data-path="papers.html"><a href="papers.html#methodology-2"><i class="fa fa-check"></i><b>7.1.1</b> Methodology</a></li>
<li class="chapter" data-level="7.1.2" data-path="papers.html"><a href="papers.html#results-3"><i class="fa fa-check"></i><b>7.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="papers.html"><a href="papers.html#sustainable-manufacturing-an-extended-mapping"><i class="fa fa-check"></i><b>7.2</b> Sustainable Manufacturing: An Extended Mapping</a><ul>
<li class="chapter" data-level="7.2.1" data-path="papers.html"><a href="papers.html#methodology-3"><i class="fa fa-check"></i><b>7.2.1</b> Methodology</a></li>
<li class="chapter" data-level="7.2.2" data-path="papers.html"><a href="papers.html#results-4"><i class="fa fa-check"></i><b>7.2.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="papers.html"><a href="papers.html#blockchain"><i class="fa fa-check"></i><b>7.3</b> Blockchain</a><ul>
<li class="chapter" data-level="7.3.1" data-path="papers.html"><a href="papers.html#methodology-4"><i class="fa fa-check"></i><b>7.3.1</b> Methodology</a></li>
<li class="chapter" data-level="7.3.2" data-path="papers.html"><a href="papers.html#results-5"><i class="fa fa-check"></i><b>7.3.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="papers.html"><a href="papers.html#precision-agriculture"><i class="fa fa-check"></i><b>7.4</b> Precision Agriculture</a><ul>
<li class="chapter" data-level="7.4.1" data-path="papers.html"><a href="papers.html#methodology-5"><i class="fa fa-check"></i><b>7.4.1</b> Methodology</a></li>
<li class="chapter" data-level="7.4.2" data-path="papers.html"><a href="papers.html#results-6"><i class="fa fa-check"></i><b>7.4.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="wikipedia.html"><a href="wikipedia.html"><i class="fa fa-check"></i><b>8</b> Wikipedia</a><ul>
<li class="chapter" data-level="8.1" data-path="wikipedia.html"><a href="wikipedia.html#technimetrochap"><i class="fa fa-check"></i><b>8.1</b> Industry 4.0: Extracting and Mapping Technologies</a><ul>
<li class="chapter" data-level="8.1.1" data-path="wikipedia.html"><a href="wikipedia.html#methodology-6"><i class="fa fa-check"></i><b>8.1.1</b> Methodology</a></li>
<li class="chapter" data-level="8.1.2" data-path="wikipedia.html"><a href="wikipedia.html#results-7"><i class="fa fa-check"></i><b>8.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="wikipedia.html"><a href="wikipedia.html#industry-4.0-a-comparison-with-industrie-4.0"><i class="fa fa-check"></i><b>8.2</b> Industry 4.0: a Comparison with Industrie 4.0</a><ul>
<li class="chapter" data-level="8.2.1" data-path="wikipedia.html"><a href="wikipedia.html#methodology-7"><i class="fa fa-check"></i><b>8.2.1</b> Methodology</a></li>
<li class="chapter" data-level="8.2.2" data-path="wikipedia.html"><a href="wikipedia.html#results-8"><i class="fa fa-check"></i><b>8.2.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="social-media.html"><a href="social-media.html"><i class="fa fa-check"></i><b>9</b> Social Media</a><ul>
<li class="chapter" data-level="9.1" data-path="social-media.html"><a href="social-media.html#technical-sentiment-analysis"><i class="fa fa-check"></i><b>9.1</b> Technical Sentiment Analysis</a><ul>
<li class="chapter" data-level="9.1.1" data-path="social-media.html"><a href="social-media.html#methodology-8"><i class="fa fa-check"></i><b>9.1.1</b> Methodology</a></li>
<li class="chapter" data-level="9.1.2" data-path="social-media.html"><a href="social-media.html#results-9"><i class="fa fa-check"></i><b>9.1.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Applications of the Results</b></span></li>
<li class="chapter" data-level="10" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html"><i class="fa fa-check"></i><b>10</b> Exploiting patent information in novel ways</a><ul>
<li class="chapter" data-level="10.1" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#towards-formal-definitions-of-advantages-and-drawbacks"><i class="fa fa-check"></i><b>10.1</b> Towards formal definitions of Advantages and Drawbacks</a></li>
<li class="chapter" data-level="10.2" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#methodology-9"><i class="fa fa-check"></i><b>10.2</b> Methodology</a><ul>
<li class="chapter" data-level="10.2.1" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#advantages-and-drawbacks-extraction"><i class="fa fa-check"></i><b>10.2.1</b> Advantages and drawbacks extraction</a></li>
<li class="chapter" data-level="10.2.2" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#advantages-and-drawbacks-adio-classification"><i class="fa fa-check"></i><b>10.2.2</b> Advantages and Drawbacks ADIO classification</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#results-10"><i class="fa fa-check"></i><b>10.3</b> Results</a><ul>
<li class="chapter" data-level="10.3.1" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#patent-set"><i class="fa fa-check"></i><b>10.3.1</b> Patent set</a></li>
<li class="chapter" data-level="10.3.2" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#extraction-of-advantages-and-drawbacks"><i class="fa fa-check"></i><b>10.3.2</b> Extraction of Advantages and Drawbacks</a></li>
<li class="chapter" data-level="10.3.3" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#a-d-i-o-representation"><i class="fa fa-check"></i><b>10.3.3</b> A-D-I-O Representation</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="exploiting-patent-information-in-novel-ways.html"><a href="exploiting-patent-information-in-novel-ways.html#discussion"><i class="fa fa-check"></i><b>10.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html"><i class="fa fa-check"></i><b>11</b> Enriched dictionaries for Innovation</a><ul>
<li class="chapter" data-level="11.1" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#an-overview-of-dictionaries-for-technology-intelligence"><i class="fa fa-check"></i><b>11.1</b> An overview of dictionaries for technology intelligence</a><ul>
<li class="chapter" data-level="11.1.1" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#publicly-available-dictionaries"><i class="fa fa-check"></i><b>11.1.1</b> Publicly available dictionaries</a></li>
<li class="chapter" data-level="11.1.2" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#research-based-dictionaries"><i class="fa fa-check"></i><b>11.1.2</b> Research-based dictionaries</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#the-value-added-of-enriched-dictionaries"><i class="fa fa-check"></i><b>11.2</b> The value added of enriched dictionaries</a></li>
<li class="chapter" data-level="11.3" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#methodology-10"><i class="fa fa-check"></i><b>11.3</b> Methodology</a></li>
<li class="chapter" data-level="11.4" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#results-11"><i class="fa fa-check"></i><b>11.4</b> Results</a></li>
<li class="chapter" data-level="11.5" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#discussions"><i class="fa fa-check"></i><b>11.5</b> Discussions</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html"><i class="fa fa-check"></i><b>12</b> Impact of Research from the Perspective of Users</a><ul>
<li class="chapter" data-level="12.1" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#methodological-challenges"><i class="fa fa-check"></i><b>12.1</b> Methodological challenges</a><ul>
<li class="chapter" data-level="12.1.1" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#variability-in-the-identification-of-outcomes-and-users"><i class="fa fa-check"></i><b>12.1.1</b> Variability in the identification of outcomes and users</a></li>
<li class="chapter" data-level="12.1.2" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#sources-of-information"><i class="fa fa-check"></i><b>12.1.2</b> Sources of information</a></li>
<li class="chapter" data-level="12.1.3" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#text-based-impact-assessment"><i class="fa fa-check"></i><b>12.1.3</b> Text-based impact assessment</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#methodology-11"><i class="fa fa-check"></i><b>12.2</b> Methodology</a><ul>
<li class="chapter" data-level="12.2.1" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#operationalizing-user-groups-using-natural-language-processing-techniques"><i class="fa fa-check"></i><b>12.2.1</b> Operationalizing user groups using Natural Language Processing techniques</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#from-text-extraction-to-indicators"><i class="fa fa-check"></i><b>12.3</b> From text extraction to indicators</a><ul>
<li class="chapter" data-level="" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#frequency"><i class="fa fa-check"></i>Frequency</a></li>
<li class="chapter" data-level="" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#diversity"><i class="fa fa-check"></i>Diversity</a></li>
<li class="chapter" data-level="" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#specificity"><i class="fa fa-check"></i>Specificity</a></li>
<li class="chapter" data-level="12.3.1" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#the-meaning-of-frequency-diversity-and-specificity-indicators-for-the-analysis-of-research-impact"><i class="fa fa-check"></i><b>12.3.1</b> The meaning of Frequency, Diversity and Specificity indicators for the analysis of research impact</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#data"><i class="fa fa-check"></i><b>12.4</b> Data</a><ul>
<li class="chapter" data-level="12.4.1" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#description-of-the-corpus"><i class="fa fa-check"></i><b>12.4.1</b> Description of the corpus</a></li>
<li class="chapter" data-level="12.4.2" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#preliminary-analysis-of-the-corpus"><i class="fa fa-check"></i><b>12.4.2</b> Preliminary analysis of the corpus</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#results-12"><i class="fa fa-check"></i><b>12.5</b> Results</a><ul>
<li class="chapter" data-level="12.5.1" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#descriptive-analysis"><i class="fa fa-check"></i><b>12.5.1</b> Descriptive analysis</a></li>
<li class="chapter" data-level="12.5.2" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#findings-by-subject-area"><i class="fa fa-check"></i><b>12.5.2</b> Findings by subject area</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="impact-of-research-from-the-perspective-of-users.html"><a href="impact-of-research-from-the-perspective-of-users.html#discussion-1"><i class="fa fa-check"></i><b>12.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html"><i class="fa fa-check"></i><b>13</b> Defining Industry 4.0 Professional Archetypes</a><ul>
<li class="chapter" data-level="13.1" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#digital-competences-development"><i class="fa fa-check"></i><b>13.1</b> Digital Competences Development</a></li>
<li class="chapter" data-level="13.2" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#methodology-12"><i class="fa fa-check"></i><b>13.2</b> Methodology</a></li>
<li class="chapter" data-level="13.3" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#the-archetypes"><i class="fa fa-check"></i><b>13.3</b> The Archetypes</a></li>
<li class="chapter" data-level="13.4" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#discussions-1"><i class="fa fa-check"></i><b>13.4</b> Discussions</a></li>
</ul></li>
<li class="part"><span><b>V Conclusions and Future Developments</b></span></li>
<li class="chapter" data-level="14" data-path="limitations-and-future-developments.html"><a href="limitations-and-future-developments.html"><i class="fa fa-check"></i><b>14</b> Limitations and Future Developments</a></li>
<li class="chapter" data-level="15" data-path="sotadocumentsunderstandlexicons.html"><a href="sotadocumentsunderstandlexicons.html"><i class="fa fa-check"></i><b>15</b> The Importance of Lexicons Design for Technical Documents Analysis</a></li>
<li class="chapter" data-level="" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sotatools" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Phases, Tasks, and Techniques</h1>
<p>In this section I make a review of the most important techniques for Natural Language Processing in the context of technical documents analysis. The techniques (mainly algorithms) are grouped in phases (Import, Tidy, Transform, Model, Visualize, Communicate) showed in figure <a href="solutions.html#fig:mainworkflow">2.1</a> and each phases is dived in the NLP tasks that are the most important for the analysis of technical documents. This standard process has been disclosed in the framework of the tidyverse <span class="citation">(Wickham and Grolemund <a href="#ref-wickham2016r">2016</a>)</span>.</p>
<div id="sotatoolsprogram" class="section level2">
<h2><span class="header-section-number">4.1</span> Program</h2>
<p>Programming is a key activity to perform in order to effectively and efficiently perform text mining. It is not a phases per se because each phase is implemented trough programming. It is critical that an analysts has in mind the need of maximizing the probability that their analysis is reproducible, accurate, and collaborative. This goals can be reached only trough programming. The most used programming languages for text mining and natural language processing are R <span class="citation">(R Development Core Team <a href="#ref-r2008">2008</a>)</span> and Python <span class="citation">(Rossum <a href="#ref-py95">1995</a>)</span>. R and Python are both open-source programming languages with a large community of developers, and new libraries or tools are added continuously to their respective catalog. R is mainly used for statistical analysis and data science while Python is a more general purpose programming language. R has been developed by Academics and statisticians over two decades. R has now one of the richest ecosystems to perform data analysis and there are around 12000 packages available in CRAN (open-source repository of R). The rich variety of libraries makes R the first choice for statistical analysis. Another cutting-edge difference between R and the other statistical products is R-studio. RStudio is a free and open-source integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. Finally, it is widely recognized the great performances that R has for data visualisation and communication. Python can pretty much do the same tasks as R: data wrangling, engineering, feature selection web scrapping, app and so on. Anyway, Python has great performances in the deployment and implementation of machine learning at a large-scale. Furthermore, Python codes are easier to maintain and more robust than R.</p>
</div>
<div id="sotatoolsimport" class="section level2">
<h2><span class="header-section-number">4.2</span> Import</h2>
<p>The first activities to perform in a text mining pipeline is to find all the documents that contains useful information for the analysis and then import the corpus (the set of documents) in to the computer program. The present section is thus focused on techniques for document retrieval <a href="sotatools.html#sotatoolsimportretrieval">4.2.1</a> and on the most popular documents digital formats <a href="sotatools.html#sotatoolsimportformat">4.2.2</a>.</p>
<div id="sotatoolsimportretrieval" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Document Retrieval</h3>
<p>Document retrieval is the process of matching a user query against a set of documents. A document retrieval system has two main tasks:</p>
<p>1- Find the documents that are relevant with respect to the user queries 2- Measure the relevance of the matching results</p>
<p>Building a query means to use field specific knowledge and logical rules to write a text string that is the composition of keywords and Boolean operators. The set of keywords (single words or phrases) is chosen in such a way that these are likely to be contained in the searched documents. Boolean operators can also be used to increment the performance of the query. The AND operator, for example is used to retrieve all the document that contains both of the terms at the left and the right of it, OR for document that contains at least one of the two words. Another important tool for making a good query are regular expressions. Regular expression (regexp) is a language for specifying text search strings, an algebraic notation for characterizing a set of strings. This language widely used in modern word processor and text processing tools. A regular expression search function will search through the corpus, returning all texts that match the pattern. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. To evaluate the performance of a query is useful to understand the concepts of precision and recall.</p>
<p>Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned. Due to the ambiguities of natural language, full-text-search systems typically includes options like stop words to increase precision. Stop-words are words that filter all the document which contains them. On the other side, stemming to increase recall <a href="sotatools.html#sotatoolstransformstemming">4.4.3</a>. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision <span class="citation">(Yuwono and Lee <a href="#ref-yuwono1996search">1996</a>)</span>. Usually when a user performs a query, the main problem are false positives (the results that are returned by the systems but are not relevant to the user). False positives has a negative impact on the precision of the query. The retrieval of irrelevant documents is particularly strong for technical documents due to the inherent ambiguity of technical language. For this reason to understand and to use the rules of query building are fundamental to the technical document analysis, since without a good query is rare to have a good set of documents to analyze.</p>
</div>
<div id="sotatoolsimportformat" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Documents Format</h3>
<p>For the purpose of the present thesis documents are considered in a digital format, and there is no need to read it from a analogical source. From the computer science point of view, text is a human-readable sequence of characters and the words they form that can be encoded into computer-readable formats. There is no standard definition of a text file, though there are several common formats. The most common types of encoding are:</p>
<ul>
<li><p>ASCII, UTF-8: plain text formats</p></li>
<li><p>.doc for Microsoft Word: Structural binary format developed by Microsoft (specifications available since 2008 under the Open Specification Promise)</p></li>
<li><p>HTML (.html, .htm): open standard, ISO from 2000</p></li>
<li><p>Office Open XML .docx: XML-based standard for office documents</p></li>
<li><p>OpenDocument .odt: XML-based standard for office documents</p></li>
<li><p>PDF: Open standard for document exchange. ISO standards include PDF/X (eXchange), PDF/A (Archive), PDF/E (Engineering), ISO 32000 (PDF), PDF/UA (Accessibility) and PDF/VT (Variable data and transactional printing). PDF is readable on almost every platform with free or open source readers. Open source PDF creators are also available.</p></li>
<li><p>Scalable Vector Graphics (SVG): Graphics format primarily for vector-based images.</p></li>
<li><p>TeX: Popular open-source typesetting program and format. First successful mathematical notation language.</p></li>
</ul>
<p>For the R software there exist many packages that helps to import documents in several formats <span class="citation">(Wickham, Hester, and Francois <a href="#ref-readr2017r">2017</a>)</span>.</p>
</div>
</div>
<div id="sotatoolstidy" class="section level2">
<h2><span class="header-section-number">4.3</span> Tidy</h2>
<p>After that data are imported they have to be processed in such a way that it would be possible to perform the main task of data analysis (transformation, modelling and visualisation). This task of tidying data (usually referred to as data pre-processing) can be very time expensive, so it is important to have clear methods and techniques to perform this task.</p>
<p>Tidy data sets have structure and working with them is easy; they’re easy to manipulate, model and visualize <span class="citation">(Wickham and others <a href="#ref-wickham2014tidy">2014</a>)</span>. Tidy data sets main concept is to arrange data in a way that each variable is a column and each observation (or case) is a row. The characteristics of tidy data can be thus summarised as the points <span class="citation">(Leek <a href="#ref-leek2015elements">2015</a>)</span>:</p>
<ul>
<li>Each variable you measure should be in one column</li>
<li>Each different observation of that variable should be in a different row</li>
<li>If you have multiple tables, they should include a column in the table that allows them to be linked</li>
</ul>
<p>There main advantages of structuring the data in this way is that a consistent data structure make it easier to use the tools (programs) that work with it because they have an underlying uniformity. This lead to an advantage in reproducibility of code.</p>
<p>As stated before tidying data is not a trivial task, and applying this process to text is even harder for documents with respect to structured data <span class="citation">(Silge and Robinson <a href="#ref-silge2016tidytext">2016</a>)</span>. On the other side, is clear that using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use . Treating text as data frames of individual words allows us to manipulate, summarize, and visualize the characteristics of text easily and integrate natural language processing into effective workflows already used.</p>
<p>Tidy text format is designed as being a table with one-token-per-row. A token unit of text that is meaningful for the analysis to be performed (for example letters, words, n-gram, sentences, or paragraphs ). Tokenization is the process of splitting text into tokens. This one-token-per-row structure is in different from the ways documents are often stored in current analyses, mainly strings or document-term matrix. The term document matrix has each corpus word represented as a row with documents as columns. The document term matrix is the transposition of the TDM so each document is a row and each word is a column. The term document matrix or document term matrix is the foundation of bag of words text mining. The bag-of-words model is a simplifying representation of documents: a text is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity <span class="citation">(McTear, Callejas, and Griol <a href="#ref-mctear2016conversational">2016</a>)</span>.</p>
</div>
<div id="sotatoolstransform" class="section level2">
<h2><span class="header-section-number">4.4</span> Transform</h2>
<p>Transforming in the context of Natural Language Processing is what in computational linguistic is called text normalization. Normalizing text means converting it to a more convenient, standard form. Most of the task of technical document analysis in fact relies on first separating out or tokenizing sentences and words, strip suffixes from the end of the word, determining the root of a word or transform the text using regular expressions.</p>
<div id="sotatoolstransformsentencesplit" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Sentence Splitting</h3>
<p>The analysis of technical documents require as first process, that the input text is segmented in sentences. Since documents do not encode this information in a non ambiguous manner (using dots) due to common abbreviations (e.g.: “Mr., Dr.”), a sentence splitting process that does not rely only on a trivial <em>dot based</em> rule is required. This issue in the technical documents domain is even more problematic due to the presence of formulas, numbers, chemical entity names and bibliographic references. Furthermore, since sentence splitting is one of the first processes of an NLP pipeline, errors in this early stage are propagated in the following steps causing a strong decrease for what concerns their accuracy. One of the most advanced techniques are machine learning techniques: given a training corpus of properly segmented sentences and a learning algorithm, a statistical model is built. By reusing the statistical model, the sentence splitter is able to split sentences on texts not used in the training phase. ItalianNLP lab systems uses this approach <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>; Attardi and Dell’Orletta <a href="#ref-attardi2009reverse">2009</a>; Attardi et al. <a href="#ref-attardi2009accurate">2009</a>)</span>. For this reason this algorithm is used for the most of the application presented in this Thesis.</p>
</div>
<div id="tokenization" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Tokenization</h3>
<p>Since documents are unstructured information, these has to be divided into linguistic units. The definition of linguistic units is non-trivial, and more advanced techniques can be used (such as n-gram extraction) but most of the times these are words, punctuation and numbers. English words are often separated from each other by white space, but white space is not always sufficient. Solving this problems and splitting words in well-defined tokens defined as tokenization. In most of the application described in the present Thesis, the tokenizer developed by the ItalianNLP lab was integrated <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>; Attardi and Dell’Orletta <a href="#ref-attardi2009reverse">2009</a>; Attardi et al. <a href="#ref-attardi2009accurate">2009</a>)</span>. This tokenizer is regular expression based: each token must match one of the regular expression defined in a configuration file. Among the others, rules are defined to tokenize words, acronyms, numbers, dates and equations.</p>
</div>
<div id="sotatoolstransformstemming" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Stemming</h3>
<p>Stemming is a simpler but cruder methodology for chopping off of affixes. The goal of stemming is reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem of a word and its morphological root do not need to be identical; it is sufficient that related words map to the same stem, even if this stem is not a valid root. One of the most widely used stemming is the simple and efficient Porter algorithm <span class="citation">(Porter <a href="#ref-porter1980algorithm">1980</a>)</span>.</p>
</div>
<div id="sotatoolstransformlemmatisation" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Lemmatisation</h3>
<p>Lemmatization is the task of determining the root of a words. The output allow to find that two words have the same root, despite their surface differences. For example, the verbs <em>am</em>, <em>are</em>, and <em>is</em> have the shared lemma <em>be</em>; the nouns <em>cat</em> and <em>cats</em> both have the lemma <em>cat</em>. Representing a word by its lemma is important for many natural language processing tasks. Lemmatisation in fact diminish the problem of sparsity of document-word matrix. Furthermore lemmatisaion is important for document retrieval <a href="sotatools.html#sotatoolsimportretrieval">4.2.1</a> web search, since the goal is to find documents mentioning motors if the search is for motor. The most recent methods for lemmatization involve complete morphological parsing of the word <span class="citation">(Hankamer <a href="#ref-hankamer1989morphological">1989</a>)</span>.</p>
</div>
<div id="sotatoolstransformwi" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Words importance metrics</h3>
<p>Once that a document has been tokenized and the tokens has been transformed, an analyst usually wants to measure how important a word is to a document in a collection or corpus. Some of the metrics adopted are:</p>
<ul>
<li><em>Term Frequency</em>: the number of times that a term occurs in document.</li>
<li><em>Boolean frequency</em>: 1 if the term occurs in the document and 0 otherwise;</li>
<li><em>Term frequency adjusted for document length</em>: is raw count normalized for the number of words contained in the document</li>
<li><em>Logarithmically scaled frequency</em>: is raw count normalized for the natural logarithm of one plus the number of words contained in the document</li>
<li><em>Inverse document frequency</em>: is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. It is a measure of how much information the word provides, that is, whether the term is common or rare across all documents.</li>
<li><em>Term frequency–Inverse document frequency</em>: the product between <em>term frequency and inverse document frequency</em>. A high weight in tf–idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.</li>
</ul>
</div>
<div id="sotatoolstransformpos" class="section level3">
<h3><span class="header-section-number">4.4.6</span> Part-of-Speech Tagging</h3>
<p>The part of speech plays an central role in technical document analysis since it provides very useful information concerning the morphological role of a word and its morphosyntactic context: for example, if a token is a determiner, the next token is a noun or an adjective with very high confidence. Part of speech tags are used for many information extraction tools such as named entity taggers (see section <a href="sotatools.html#sotatoolsmodelner">4.5.5</a>) in order to identify named entities. In typical named entity task these are people and locations since tokens representing named entities follow common morphological patterns (e.g. they start with a capital letter). For the application to technical documents, technical entities (like the possible failures of a manufact) becomes more relevant. In this context a correct part-of-speech tagger becomes even more important since morphosyntactical rules can not be used. In addition part of speech tags can be used to mitigate problems related to polysemy since words often have different meaning with respect to their part of speech (e.g. “track”, “guide”). This information is extremely valuable in patent analysis, and some patent tailored part-of-speech tagger has been designed (see section <a href="sotadocuments.html#sotadocumentspatents">5.1</a>). The literature on pos-tagger is huge, and goes behind the scope of the present thesis to make a complete review. In most of the application presented in this work, was employed the ILC postagger <span class="citation">(Attardi <a href="#ref-attardi2006experiments">2006</a>)</span>. This postagger uses a supervised training algorithm: given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus.</p>
</div>
</div>
<div id="sotatoolsmodel" class="section level2">
<h2><span class="header-section-number">4.5</span> Model</h2>
<p>The goal of a model is to provide a simple low-dimensional summary of a dataset <span class="citation">(Wickham and Grolemund <a href="#ref-wickham2016r">2016</a>)</span>. Ideally, the model will capture patterns generated by the phenomenon of interest (true signals), and ignore random variations (noise). A good model at the same time is able to capture the weak signals that cab be easily confounded with noise. These information is particularly valuable in the context of technical document analysis, where great technical insight could come weak quasi-invisible signals. <span class="citation">(James et al. <a href="#ref-james2013introduction">2013</a>)</span></p>
<p>Probabilistic models are widely used in text mining nowadays, and applications range from topic modeling, language modeling, document classification and clustering to information extraction. The present section contains a review of the most used methods used to model textual information.</p>
<div id="sotatoolstransformngrams" class="section level3">
<h3><span class="header-section-number">4.5.1</span> N-Grams</h3>
<p>An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like “credit card”, “3d printing”, or ”printing machine”, and a 3-gram (or trigram) is a three-word sequence of words like “3d printing machine”. Statistical model can be used to extract the n-grams contained in a document. A first approach has the of predicting the next item in a sequence in the form of a (n − 1)–order Markov model<span class="citation">(Lafferty and Zhai <a href="#ref-lafferty2001document">2001</a>)</span>. The algorithm begin with the task of computing P(w|h), the probability of a word w given a word h.The way to estimate this probability is using relative frequency counts. To do that the algorithms count the number of times h is followed by the w. With a large enough corpus it is possible to build valuable models, able to extract n-grams <span class="citation">(Bellegarda <a href="#ref-bellegarda2004statistical">2004</a>)</span>. While this method of estimating probabilities directly from counts works for many natural language applications, in many cases a huge dimension of the corpus does make the model useful, and this is particularly true for technical documents <span class="citation">(Brants et al. <a href="#ref-brants2012large">2012</a>)</span>. This is because technical language has a strong ratio of evolution; as new artifact are invented, new chunks are created all the time, and has no sense to continuously count every word co-occurrence to update our model<span class="citation">(Gibson, Gibson, and Ingold <a href="#ref-gibson1994tools">1994</a>)</span>. A more useful method for chunk extraction fro technical document uses part-of-speech-tagging and regular expression. Once a document is pos-taggerd each word is associated whit a particular part of speech: each sentence is represented as a sequence of part-of-spech. Once this representation is ready, it is possible to extract only certain sequences of part-of-speeches, the ones that whit an high level of confidence are n-grams.</p>
</div>
<div id="sotatoolsmodeldocclass" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Document Classification</h3>
<p>Classification is a general process that has the goal of taking an object, extract features, and assign to the observation one of a set of discrete classes. This process is largely used for documents <span class="citation">(Borko and Bernick <a href="#ref-borko1963automatic">1963</a>)</span> and there exist many methods for document classification <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>.</p>
<p>Regardless of technological sector, most organizations today are facing the problem of overload of information. When it comes to classify huge amount of documents or to separate the useful documents from the irrelevant, document classification techniques can reduce the process cost and time.</p>
<p>The simplest method for classifying text is to use expert defined rules. These systems are called expert systems or knowledge engineering approach. Expert rule-based systems are programs that consist of rules in the IF form condition THEN action (if condition, then action). Given a series of facts, expert systems, thanks to the rules they are made of, manage to deduce new facts. The expert systems therefore differ from other similar programs, since, by referring to technologies developed according to artificial intelligence, they are always able to exhibit the logical steps that underlie their decisions: a purpose that, for example, is not feasible from the human mind or black box-systems. There are many type of documents for which expert based classifiers constitute a state-of-the-art system, or at least part of it. Anyway, rules can be useless in situations such as: - data change over time - the rules are too many and interrelated</p>
<p>Most systems of documents classification are instead done via supervised learning: a data set of input observations is available and each observation is associated with some correct output (training set). The goal of the algorithm is to build a static model able to learn how to map from a new observation (test set) to a correct output. The advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains.</p>
<p>In the supervised document classification process, is used a training set of N documents that have each been typically hand-labeled with a class: (d1, c1),….,(dN, cN). I say typically, because other less expensive methods could be designed, as it will be shown for the task of Named Entity Recognition (another supervised learning task, that classifies words instead of documents <a href="sotatools.html#sotatoolsmodelner">4.5.5</a>). The goal of the supervised document classification task is to learn a statistical model capable of assign a new document d to its correct class c ∈ C. There exist a class of these classifier, probabilistic classifiers, that additionally will tell us the probability of the observation being in the class.</p>
<p>Many kinds of machine learning algorithms are used to build classifiers <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>, such as:</p>
<ul>
<li><p><em>Decision Tree Classifiers</em>: Decision tree documents classifier are systems that has as output a classification tree <span class="citation">(Sebastiani <a href="#ref-sebastiani2002machine">2002</a>)</span>. In this tree internal nodes are terms contained in the corpus under analysis, branches departing are labeled by the weight (see section <a href="sotatools.html#sotatoolstidy">4.3</a>) that the term has in the test document, and leafs are labeled by categories. There exists many methods to automatically learn trees from data. A tree can be build by splitting the data source into subsets based on an test feature. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions.</p></li>
<li><p><em>Rule Based Classifiers</em>: Rule-based classifiers are systems in which the patterns which are most likely to be related to the different classes are extracted from a set of test documents. The set of rules corresponds to the left hand side to a word pattern, and the right-hand side to a class label. These rules are used for the purposes of classification. In its most general form, the left hand side of the rule is a Boolean condition, which is expressed in Disjunctive Normal Form (DNF). However, in most cases, the condition on the left hand side is much simpler and represents a set of terms, all of which must be present in the document for the condition to be satisfied <span class="citation">(Yang, Li, and Wang <a href="#ref-yang2004building">2004</a>)</span>.</p></li>
<li><p><em>Support Vector Machines (SVM) Classifiers</em>: SVM Classifiers attempt to partition the data space with the use of linear or non-linear delineations between the different classes. The main principle of SVM algorithm is to determine separators in the feature space which can best separate the different classes <span class="citation">(Joachims <a href="#ref-joachims1998text">1998</a>; Manevitz and Yousef <a href="#ref-manevitz2001one">2001</a>)</span>.</p></li>
<li><p><em>Baeysian Classifiers</em>: Bayesian classifiers build a probabilistic classifier based on modeling the underlying word features in different classes. The idea is then to classify documents using the posterior probability of the documents belonging to the different classes on the basis of the word presence in the documents <span class="citation">(Pop <a href="#ref-pop2006approach">2006</a>)</span>.</p></li>
<li><p><em>Neural Netword Classifiers</em>: The basic unit in a neural network is a neuron. Each neuron receives a set of inputs, which are denoted by the vector <em>Xi</em>, which are the values of the feature vector for a certain instance. Each neuron is also associated with a set of weights, which are used in order to compute a function of its inputs. Neural Networks Classifier are able, thank to a process called learning phase, to adjust their weights in such a way that the function is able to effectively classify new instances. Neural networks are nowadays one of the best method for documents classification, and are used in a wide variety of applications <span class="citation">(Manevitz and Yousef <a href="#ref-manevitz2007one">2007</a>)</span>. Great performances has also been reached by deep neural networks, which are neural networks whit a large number o neurons arranged in multiple layers <span class="citation">(Lai et al. <a href="#ref-lai2015recurrent">2015</a>; Kim <a href="#ref-kim2014convolutional">2014</a>)</span>.</p></li>
</ul>
</div>
<div id="sotatoolsmodelsentanal" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Sentiment Analsysis</h3>
<p>Sentiment analysis techniques are algorithms able to measure from text, people’s opinions and emotions toward events, topics, products and their attributes <span class="citation">(Pang, Lee, and others <a href="#ref-pang2008opinion">2008</a>)</span>. For example, businesses (particularly marketeers) are interested in finding costumers opinions about their products and services.</p>
<p>Thanks to the growth of social media (forums, blogs and social networks), individuals and organizations are producing a huge quantity of their written opinion. This has make it possible to scholars to study this phenomena and to develop many different and effective sentiment analysis techniques <span class="citation">(Liu and Zhang <a href="#ref-liu2012survey">2012</a>)</span>. In the past decade, a considerable amount of research has been done by scholars and there are also numerous commercial companies that provide opinion mining services. However, measuring sentiment in documents and distilling the information contained in them remains a challenging task because of the diversity of documents from which is possible to extract sentiment.</p>
<p>The approaches to perform sentiment analysis are many. Among all, the most interesting for technical documents analysis are:</p>
<ul>
<li><p><em>Dictionary Base Approaches</em> : This approach has the aim of collecting words that are clues for positive or negative sentiment. In literature these words are called opinion words, opinion-bearing words or sentiment words. Examples of positive opinion words are: good, nice and amazing. Examples of negative opinion words are bad, poor, and terrible. Collectively, they are called the opinion lexicon. The most simple and widely used techniques to produce a dictionary of opinion words is based on bootstrapping using a small set of seed opinion words and an online dictionary such as WordNet <span class="citation">(Miller <a href="#ref-miller1995wordnet">1995</a>)</span>. The works that used this approach <span class="citation">(Hu and Liu <a href="#ref-hu2004mining">2004</a>; Kim and Hovy <a href="#ref-kim2004determining">2004</a>)</span>, adopts a process that consist in two phases: first collect set of opinion words manually, then grow this set by searching in the WordNet for their synonyms and antonyms. The process stops when no more new words are found. After that a manual inspection can be carried out to remove and/or correct errors. Scholars has developed several opinion lexicons <span class="citation">(Ding, Liu, and Yu <a href="#ref-ding2008holistic">2008</a>; Baccianella, Esuli, and Sebastiani <a href="#ref-baccianella2010sentiwordnet">2010</a>; Hu and Liu <a href="#ref-hu2004mining">2004</a>; Philip et al. <a href="#ref-philip1966general">1966</a>; Wiebe, Bruce, and O’Hara <a href="#ref-wiebe1999development">1999</a>)</span> The lexicon based approach has the characteristic of being strongly context specific. This is an advantage when the goal is to design a method able to extract sentiment in a specific context <span class="citation">(Chiarello et al. <a href="#ref-chiarello2017product">2017</a>)</span>, but is a major shortcoming if the goal is to design a general purpose method.</p></li>
<li><p><em>Supervised Learning Approaches</em>: Sentiment analysis can be formulated as a document classification problem with three classes: positive, negative and neutral<span class="citation">(Mullen and Collier <a href="#ref-mullen2004sentiment">2004</a>)</span>. Training and test sets of documents are typically collected from product reviews, movies reviews or are created by scratch using manual annotation. Any learning algorithm can be applied to sentiment classification (naive Bayesian classification, and support vector machines <span class="citation">(Prabowo and Thelwall <a href="#ref-prabowo2009sentiment">2009</a>)</span>). The crucial phase for Supervised Learning sentiment analysis is the features presentation of the data. It was shown <span class="citation">(Pang, Lee, and Vaithyanathan <a href="#ref-pang2002thumbs">2002</a>)</span> that using uni-grams (a bag of individual words) as features in classification performed well with either naive Bayesian or SVM. Subsequent research used many more features and techniques in learning <span class="citation">(Pang, Lee, and others <a href="#ref-pang2008opinion">2008</a>)</span>.</p></li>
</ul>
</div>
<div id="sotatoolsmodelnetanal" class="section level3">
<h3><span class="header-section-number">4.5.4</span> Text Clustering</h3>
<p>The goal of clustering methods is to find groups of similar objects in the data thanks to the measure of a similarity function <span class="citation">(Jain and Dubes <a href="#ref-jain1988algorithms">1988</a>; Kaufman and Rousseeuw <a href="#ref-kaufman2009finding">2009</a>)</span>. Clustering techniques has been widely applied in the text domain, where the objects of the clustering can be documents (at different level of granularity) or terms. In the context of technical documents analysis Clustering is especially useful documents retrieval <span class="citation">(Anick and Vaithyanathan <a href="#ref-anick1997exploiting">1997</a>; Cutting, Karger, and Pedersen <a href="#ref-cutting1993constant">1993</a>)</span>. Clustering problems has been and are studied widely outside the text domain. Methods for clustering have been developed focusing on quantitative/non-textual data <span class="citation">(Guha, Rastogi, and Shim <a href="#ref-guha1998cure">1998</a>; Han, Kamber, and Tung <a href="#ref-han2001spatial">2001</a>; Zhang, Ramakrishnan, and Livny <a href="#ref-zhang1996birch">1996</a>)</span>.</p>
<p>In the context of text analysis, the problem of clustering finds applicability for a number of tasks, such as Document Organization and Browsing <span class="citation">(Cutting et al. <a href="#ref-cutting2017scatter">2017</a>)</span>, Corpus Stigmatization using documents maps <span class="citation">(Schütze and Silverstein <a href="#ref-schutze1997projections">1997</a>)</span> or word clusters <span class="citation">(Baker and McCallum <a href="#ref-baker1998distributional">1998</a>; Bekkerman et al. <a href="#ref-bekkerman2001feature">2001</a>)</span>. It is useful also to use a Soft clustering approach, that associates each document with multiple clusters with a given probability.</p>
<p>However, standard techniques for cluster analysis (k-means or hierarchical clustering) do not typically work well for clustering textual data in general or more specific technical documents. This is because of the unique characteristics of textual data which implies the design of specialized algorithms for the task.</p>
<p>The distinguishing characteristics of the text representation are the following <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>:</p>
<ul>
<li><p>There is a problem of course of dimensionality. The dimensionality of the bug-of-words representation is very large and the underlying data is sparse. In other words, the lexicon from which the documents are drawn may be of the order of millions, but a given document may contain only a few hundred words.This problem is even more serious for technical documents in which the lexicon is even more large.</p></li>
<li><p>The words are correlated with one another and thus the number of concepts (or principal components) in the data is much smaller than the feature space. This necessitates the careful design of algorithms which can account for word correlations in the clustering process.</p></li>
<li><p>The number of words (or non-zero entries) in the different documents may vary widely. Therefore, it is important to normalize the document representations appropriately during the clustering task.</p></li>
</ul>
<p>The problems of sparsity and high dimensionality necessitate the design of specific algorithms text processing. The topic has been heavily studied in the information retrieval literature where many techniques have been proposed <span class="citation">(Ricardo and Berthier <a href="#ref-ricardo2011modern">2011</a>)</span>.</p>
</div>
<div id="sotatoolsmodelner" class="section level3">
<h3><span class="header-section-number">4.5.5</span> Named Entity Recognition</h3>
<p>Named Entity Recognition is the task of identifying entity names like people, organizations, places, temporal expressions or numerical expressions. An example of an annotated sentence for a NER extraction system tailored for user entity extraction from patents, is the following:</p>
<p><em>Traditionally, <user> guitar players <user/> or <user> players <user/> of other stringed instruments may perform in any of a number of various positions, from seated, with the stringed instrument supported on the leg of the <user>performer<user/>, to standing or walking, with the stringed instrument suspended from a strap.</em></p>
<p>Methods and algorithms to deal with the entity extraction task are different, but the most effective are the ones based on supervised methods. Supervised methods tackle this task by extracting relevant statistics from an annotated corpus. These statistics are collected from the computation of features values, which are strong indicators for the identification of entities in the analyzed text. Features used in NLP for NER purposes are divided in two main categories: - Linguistically motivated features, such as n-gram of words (sequences of n words), lemma and part of speech - External resources features as, for example, external lists of entities that are candidates to be classified in the extraction process.</p>
<p>The annotation methods of a training corpus can be of two different kinds: human based, which is time expensive, but usually effective in the classification phase; automatically based, which can lead to annotation errors due to language ambiguity. For instance driver can be classified both as a user (the operator of a motor vehicle), or not a user (a program that determines how a computer will communicate with a peripheral device). Different training algorithms, such as Hidden Markov Models <span class="citation">(Eddy <a href="#ref-eddy1996hidden">1996</a><a href="#ref-eddy1996hidden">a</a>)</span>, Conditional Random Fields (CRF) <span class="citation">(Lafferty, McCallum, and Pereira <a href="#ref-lafferty2001conditional">2001</a><a href="#ref-lafferty2001conditional">a</a>)</span> Support Vector Machines (SVM) <span class="citation">(Hearst, Dumais, Osuna, et al. <a href="#ref-hearst1998support">1998</a>)</span>, or Bidirectional Long Short Term Mermory-CRF Neural Networks <span class="citation">(Lample et al. <a href="#ref-lample2016neural">2016</a>; Misawa et al. <a href="#ref-misawa2017character">2017</a> )</span> are used to build a statistical model based on features that are extracted from the analyzed documents in the training phase.</p>
</div>
<div id="sotatoolsmodeltopicmodel" class="section level3">
<h3><span class="header-section-number">4.5.6</span> Topic Modelling</h3>
<p>Topic modeling is a form of dimension reduction that uses probabilistic models to find the co-occurrence patterns of terms that correspond to semantic topics in a collection of documents <span class="citation">(Crain et al. <a href="#ref-crain2012dimensionality">2012</a>)</span>. To understand topic modelling it is useful to understand its differences with clustering <a href="sotatools.html#sotatoolsmodelnetanal">4.5.4</a> and the problem they both solves: the course of dimensionality. Both these techniques has in fact the goal of representing documents in such a way that they reveals their internal structure and interrelations. Clustering measures the similarity (or dissimilarity) between documents to place documents into groups. Representing each document by considering the belonging to a group, clustering induces a low-dimensional representation for documents. However, it is often difficult to characterize a cluster in terms of meaningful features because the clustering is independent of the document representation, given the computed similarity. Topic modeling integrates soft clustering (assigning each element to a cluster with a given probability and not with a Boolean variable) with dimension reduction. Each document is associated with a number of latent topics: a topic can be seed as both document clusters and compact group of words identified from a corpus. Each document is assigned to the topics with different weights: this feature can be seen both as the degree of membership in the clusters, as well as the coordinates of the document in the reduced dimension space. The result is an understandable representation of documents that is useful for analyzing the themes in documents. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model <span class="citation">(Blei, Ng, and Jordan <a href="#ref-blei2003latent">2003</a>)</span>. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.</p>
</div>
</div>
<div id="sotatoolsvisualize" class="section level2">
<h2><span class="header-section-number">4.6</span> Visualize</h2>
<p>Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests: the process of developing the outputs (the paper, the report, the dashboard, or other deliverable) is less frequently analyzed. This problem influences also text mining <span class="citation">(Parker <a href="#ref-parker2017opinionated">2017</a>)</span>. One of the most studied problems of output production is data visualisation. Data visualisation involves the creation and study of the visual representation of data <span class="citation">(Friendly and Denis <a href="#ref-friendly2001milestones">2001</a>)</span>. Data visualization uses statistical graphics, plots, information graphics and other tools to communicate information in a clearl and efficient way. The main process of data visualisation is the visual encoding of numbers. Numerical data may be encoded in many ways, using a wide range of shapes: the main used are dots, lines, and bars <span class="citation">(Wickham <a href="#ref-wickham2016ggplot2">2016</a>)</span>. The main goal of visualizations is to help users (students, researchers, companies and many others) analyze and reason about evidences hidden in data. It is possible thanks to the ability of visualisation to make complex data more accessible, understandable and usable. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.</p>
<p>Da visualisation has become in the last year a well enstablished discipline thanks to the increased amounts of data created by Internet activity and an expanding number of sensors in the environment are referred to as “big data” or Internet of things. It is important to undeline how the way this data is comunicated present ethical and analytical challenges for data visualization practitioner <span class="citation">(Bikakis <a href="#ref-bikakis2018big">2018</a>)</span>. The field of data science and practitioners called data scientists help address this challenge <span class="citation">(Loukides <a href="#ref-loukides2011data">2011</a>)</span>.</p>
<p>Users of information displays are executing (consciously or not) particular analytical tasks such as making comparisons or determining causality <span class="citation">(Tufte, Goeler, and Benson <a href="#ref-tufte1990envisioning">1990</a>)</span>. The design principle of the information graphic should thus support the analytical task, showing the comparison or causality <span class="citation">(Tufte <a href="#ref-tufte2006beautiful">2006</a>)</span>.</p>
<p>Graphical displays and principles for effective graphical display is defined as the ability to communicate complex statistical and quantitative ideas with clarity, precision and efficiency <span class="citation">(Mulrow <a href="#ref-mulrow2002visual">2002</a>)</span>. For this reason graphical displays should:</p>
<ul>
<li>show the data</li>
<li>induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production or something else</li>
<li>avoid distorting what the data has to say</li>
<li>present many numbers in a small space</li>
<li>make large data sets coherent</li>
<li>encourage the eye to compare different pieces of data</li>
<li>reveal the data at several levels of detail, from a broad overview to the fine structure</li>
<li>serve a reasonably clear purpose: description, exploration, tabulation or decoration</li>
<li>be closely integrated with the statistical and verbal descriptions of a data set</li>
</ul>
<p>In litterature are identified eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message <span class="citation">(Few <a href="#ref-few2012show">2012</a>)</span>:</p>
<ul>
<li>Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.</li>
<li>Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.</li>
<li>Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.</li>
<li>Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.</li>
<li>Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis. A boxplot helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc.</li>
<li>Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.</li>
<li>Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.</li>
<li>Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.</li>
</ul>
<p>Data visualisation practionires has to consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis.</p>
<div id="the-grammar-of-graphics" class="section level3">
<h3><span class="header-section-number">4.6.1</span> The Grammar of Graphics</h3>
<p>Even if trial and error is and will remain an important part of data visualisation, some works as tried to give to data visualisation practitioners a well structured framework able to guide the process of data visualisation. Among the many frameworks, the most used is the <em>Grammar of Graphics</em> <span class="citation">(Wilkinson <a href="#ref-wilkinson2006grammar">2006</a>)</span> and its implementation <span class="citation">(Wickham, Chang, and others <a href="#ref-wickham2008ggplot2">2008</a>)</span>. The grammar of graphics is a coherent system for describing and building graphs. Like other kind of grammars, it describes to basic rules to use the element of data visualization with the goal of comunicating some content. The main concept in the grammar of graphics is that graphs are made by multiple layers. Layers are responsible for creating the objects that we perceive on the plot. A layer is composed of four parts:</p>
<ul>
<li><em>Data and aesthetic mapping</em>: Data are independent from the other components: we can construct a graphic that can be applied to multiple datasets. Along with the data, we need a specification of which variables are mapped to which aesthetics.</li>
<li><em>Statistical transformation</em>: A statistical transformation transforms the data, typically by summarizing them in some manner.</li>
<li><em>Geometric object</em>: Geometric objects control the type of plot that is created. For example, using a point geom will create a scatterplot, whereas using a line geom will create a line plot. Geometric objects can be classified by their dimensionality.</li>
<li><em>Position adjustment</em>: Sometimes there exist th need to tweak the position of the geometric elements on the plot, when otherwise they would obscure each other. This is most common in bar plots, where we stack or dodge (place side-by-side) the bars to avoid overlaps.</li>
</ul>
<p>Multyple layers togheter are used to create complex plots.</p>
<p>Togheter with the layer the designer can control the <em>scale.</em> A scale controls the mapping from data to aesthetic attributes, and one scale for each aesthetic property used in a layer is needed. Scales are common across layers to ensure a consistent mapping from data to aesthetics.</p>
<p>After the decision of the scale, the designer has ti decid the <em>coordinate system</em> for the layer. A coordinate system maps the position of objects onto the plane of the plot. Position is often specified by two coordinates (x, y), but could be any number of coordinates. The Cartesian coordinate system is the most common coordinate system for two dimensions, whereas polar coordinates and various map projections are used less frequently. For higher dimensions, we have parallel coordinates (a projective geometry), mosaic plots (a hierarchical coordinate system), and linear projections onto the plane. Coordinate systems affect all position variables simultaneously and differ from scales in that they also change the appearance of the geometric objects.</p>
<p>Finally, the last element of the grammar are <em>facets</em>. Faceting makes it easy to create small multiples of different subsets of an entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions. The faceting specification describes which variables should be used to split up the data, and how they should be arranged.</p>
</div>
</div>
<div id="sotatoolscomunicate" class="section level2">
<h2><span class="header-section-number">4.7</span> Comunicate</h2>
<p>The last task to perform in the process of knowledge extraction from technical documents is communications. If it means to comunicate the results of an anlysis inside a team or to the world, it doesn not matter how great an analysis is unless it is impossibile to explain it to others <span class="citation">(Wickham and Grolemund <a href="#ref-wickham2016r">2016</a>)</span>. For the purposes of the present thesis, the focus is on the review of technical mechanics of communication especialli in the R <span class="citation">(R Core Team <a href="#ref-R-base">2018</a>)</span> enviroment. One of the most important innovation for the task of communication in data science is R Markdown <span class="citation">(Allaire et al. <a href="#ref-R-rmarkdown">2018</a>)</span>. R Markdown provides an unified authoring framework for data science, combining code, results, and comments. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.</p>
<p>R Markdown files are designed to be used in three ways:</p>
<ul>
<li><p>For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis.</p></li>
<li><p>For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them ( i.e. the code).</p></li>
<li><p>As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.</p></li>
</ul>
<p>Togheter with reports (and usually contained in them) there are visualisation. Making graphics for communication follow all the rules and framework previously revised in section <a href="sotatools.html#sotatoolsvisualize">4.6</a>, but when a graph has to be used to comunicate to a wide audience there are some more rules to follow. The reason why this happen is that the audience likely do not share the background knowledge of the anlysit and do not be deeply invested in the data. To help others quickly build up a good mental model of the data, the analyst need to invest considerable effort in making plots as self-explanatory as possible. For this reason has been developed many tools to help data scientist to make effective comunication graphs<span class="citation">(Wickham <a href="#ref-wickham2016ggplot2">2016</a>; Chang et al. <a href="#ref-shiny2017">2017</a>; Sievert et al. <a href="#ref-plotly2017">2017</a>; Pedersen <a href="#ref-ggprah2018">2018</a>; Bastian, Heymann, and Jacomy <a href="#ref-ICWSM09154">2009</a>)</span> .</p>
</div>
<div id="sotadocumentsunderstand" class="section level2">
<h2><span class="header-section-number">4.8</span> Understand</h2>
<p>The harder challenge in technology intelligence is not how to detect the large trends- they are visible anyway; it is, rather, how to detect weak signals, or information that initially appears with low frequency, in unrelated or unexpected regions of the technology landscape, and associated with large noise <span class="citation">(Apreda et al. <a href="#ref-apreda2016functional">2016</a>)</span>. These signals escape from traditional statistical detection techniques, exactly because it is difficult to distinguish them from pure statistical noise. It is hard to disntinguish weak-signals from noise only using algorythms: understanding the problem to be solved and having domain expertise is essential. For this reason <em>understand</em> is not only part of the process of analysis, but is maybe the most important task with respect to the present thesis.</p>
<p>Unfortunately is difficult to design a process of analsysis in such a way that the source of domain knowledge and they way they impact the analysis is clear. According to the survey carried out by Popper <span class="citation">(Popper <a href="#ref-popper2008foresight">2008</a>)</span>, expert panels are the second most used methodology, after the survey of the literature (see section <a href="sotadocuments.html#sotadocumentspapers">5.2</a> for automatic approach to this task). It is non-sense to gain technical knowledge that is valuable to carry on a text mining analysis by relying exclusively on algorythms, without the support of human experts that are acknowledgeable about the knowledge field, technologies, their inner functioning, and the range of practical problems they are intended to solve. Due to the ill-structured nature of knowledge field expertise systematisation in data science, it requires an extensive problem-formulation phase, which cannot be done by others than human experts <span class="citation">(Bracken and Oughton <a href="#ref-bracken2013making">2013</a>)</span>.</p>
<p>It is usually necessary to involve experts into panels or focus groups, in which they have face-to-face interaction (FTF). It is well known that experts have a preference for methods involving personal communication, such as FTF, but this method delivers the least reliable results, as compared to other methods such as prediction markets, nominal groups and, above all, the Delphi technique <span class="citation">(Woudenberg <a href="#ref-woudenberg1991evaluation">1991</a>; Graefe and Armstrong <a href="#ref-graefe2011comparing">2011</a>)</span>.</p>
<p>Yet the use of experts creates a large number of problems. Even when experts are trained in data science, professionally engaged and personally committed to high standards of ethical integrity, they may introduce cognitive and motivational biases in the proces<span class="citation">(Kahneman and Egan <a href="#ref-kahneman2011thinking">2011</a>)</span>. A crucial point in the literature is that not only lay people but also experts are subject to <em>cognitive biases</em>, and that cognitive abilities may moderate but not eliminate these effects <span class="citation">(Stanovich and West <a href="#ref-stanovich2008relative">2008</a>)</span>. Therefore the issue cannot be solved by selecting better experts <span class="citation">(Taleb <a href="#ref-taleb2007black">2007</a>)</span>.</p>
<p>In addition, experts often work in groups or panels, rather than in isolation or following the Delphi procedure, in which the revision of estimates is done separately and under the protection of anonymity <span class="citation">(Meijering and Tobi <a href="#ref-meijering2016effect">2016</a>; Makkonen, Hujala, and Uusivuori <a href="#ref-makkonen2016policy">2016</a>)</span>. The group decision making process may add other peculiar distortions to the knowledge extraction exercise, as shown by the large literature on groupthink <span class="citation">(Janis <a href="#ref-janis1972victims">1972</a>; Esser <a href="#ref-esser1998alive">1998</a>)</span>. These biases are difficult to overcome and require a dedicated attention. As the Expert Group of the European Commission has recommended: “in order to minimize cognitive bias in analysis, selection and interpretation of information, explicit tools and methodologies should be used” <span class="citation">(Tuomi <a href="#ref-tuomi2013next">2013</a>)</span>.</p>
<p>The issue of biases in data science technological analysis has been raised only recently in a few seminal papers, but it has not yet created a consistent literature, comparable to the large and robust literature in the field of forecasting and in many other areas of social sciences.</p>
<div id="sotadocumentsunderstandbyas" class="section level3">
<h3><span class="header-section-number">4.8.1</span> The problem of byases</h3>
<p>The literature in the heuristic and biases tradition is now in the order of several thousand articles. A deliberately crude search for “heuristic and biases” delivers more than 208,000 items in Google Scholar, a more sober 5,298 items in JSTOR, and 2,463 items in Scopus (accessed January 19, 2018). Even looking at specialized branches of the field we find references in the hundreds: in the EBSCO Business Source database the items are 453, in Pub Med they are 281. There is not a unique agreement on the terminology and the number. To make an example, Arnott <span class="citation">(Arnott <a href="#ref-arnott1998decision">1998</a>)</span> mentions 37 heuristics and biases; a quite different list of 13 heuristics is discussed by Mousavi and Gigerenzer <span class="citation">(Mousavi and Gigerenzer <a href="#ref-mousavi2014risk">2014</a>)</span>. There is a minority opinion that has contested the overall Kahneman-Tversky <span class="citation">(Kahneman and Egan <a href="#ref-kahneman2011thinking">2011</a>)</span> paradigm, apparently without substituting it <span class="citation">(Gigerenzer <a href="#ref-gigerenzer1991make">1991</a>)</span>. There are several meta-analyses and systematic reviews that examine specific biases, which will be included in the reference list and in the discussion below.</p>
<p>What follows is a selection of topics that might be important in data science, and in particular for knowledge extraction from technical documents. In the following we start from the biases suggested by Goodwin <span class="citation">(Goodwin <a href="#ref-goodwin2015history">2015</a>)</span> and we add a few ones, which we believe are relevant for the field. The list is not necessarily exhaustive but is focused on those biases that are more important for anticipating technological trends. We first describe the biases in general terms.</p>
<div id="framing-effect" class="section level4 unnumbered">
<h4>Framing effect</h4>
<p>According to a famous experiment by Tversky and Kahneman <span class="citation">(Tversky and Kahneman <a href="#ref-tversky1981framing">1981</a>)</span> people react differently to a decision problem according to the way it is “framed” in the task description, ignoring the content. In particular, framing a problem in terms of losses induces more risk-taking than framing in terms of gains. Risk or loss aversion occurs when people prefer a small reward with greater certainty than a large reward with less certainty. Framing a problem in terms of gains, or improvement, activates a different response <span class="citation">(Tversky and Kahneman <a href="#ref-tversky1981framing">1981</a>)</span>. This effect has been confirmed in a large number of studies. The literature review by Levin, Schneider and Gaeth <span class="citation">(Levin, Schneider, and Gaeth <a href="#ref-levin1998all">1998</a>)</span> suggests to distinguish the effects of framing on risky choices, on the evaluation of attributes of alternatives, and of goal-related attributes. Experts are subject to framing effects not differently than novices <span class="citation">(Loke and Tan <a href="#ref-loke1992effects">1992</a>)</span> while in high level cognitive tasks the differences are remarkable <span class="citation">(Larkin et al. <a href="#ref-larkin1980expert">1980</a>)</span>.</p>
<p>Several studies suggest that the framing can be mitigated or reduced to zero by warning on the effect <span class="citation">(Cheng and Wu <a href="#ref-cheng2010debiasing">2010</a>)</span>, using Decision Support Systems [bhandari2008debiasing], creating heterogeneous groups <span class="citation">(Yaniv <a href="#ref-yaniv2011group">2011</a>)</span>, exposing people to dissenting minority views <span class="citation">(Nemeth and Chiles <a href="#ref-nemeth1988modelling">1988</a>)</span>, or using deliberately devil’s advocate or dialectical inquiry methods <span class="citation">(Lord, Lepper, and Preston <a href="#ref-lord1984considering">1984</a>)</span>. However, other studies underline the importance of real conflict, or minority views, as opposed to artificially created situations (such as the devil’s advocate method, based on role playing) <span class="citation">(Goodwin and Wright <a href="#ref-goodwin2010limits">2010</a>)</span>. Exposure to dissent and disagreement reduces the confidence in one’s own views, but also reduces the level of conformity and improves the accuracy <span class="citation">(Nemeth, Brown, and Rogers <a href="#ref-nemeth2001devil">2001</a>)</span>. At the same time, it is critical that the dissent is centered around factual data, so that the discussion does not degenerate into issues of personalities <span class="citation">(Keay <a href="#ref-keay2012authorising">2012</a>)</span>. It is also critical that the judgment is repeated, so that people come to weight information according to its accuracy, considering discordant opinions and not discounting them <span class="citation">(Harries, Yaniv, and Harvey <a href="#ref-harries2004combining">2004</a>)</span>.</p>
<p>According to Burgman <span class="citation">(Burgman <a href="#ref-burgman2015trusting">2015</a>)</span>, “experts groups should be as diverse as possible, and systems for engagement should encourage people to listen and integrate information and reasoning from as many sources as possible, and to explore competing explanations”.</p>
<p>different expert orientation than a framing in which gains and losses are compared and weighted. A related problem is that the framing of the problem may hinder the ability to identify rare events, or low predictability phenomena. A combination between quantitative modelling and scenario analysis is suggested to avoid blindness <span class="citation">(Makridakis, Hogarth, and Gaba <a href="#ref-makridakis2009forecasting">2009</a>)</span>.</p>
</div>
<div id="anchoring" class="section level4 unnumbered">
<h4>Anchoring</h4>
<p>In producing numeric estimates of unknown quantities, people are heavily influenced by the information provided in the task description, even when it is not consistent or realistic <span class="citation">(Gigerenzer <a href="#ref-gigerenzer2015calculated">2015</a>)</span>. In other words, according to Kahneman and Tversky <span class="citation">(Tversky and Kahneman <a href="#ref-tversky1974judgment">1974</a>)</span>, people “anchor” their estimate to the number provided in the description and incorporate new information by “adjusting” the estimate above and below the anchor. In this way people do not make efficient use of information available and may be distorted in their judgment <span class="citation">(Thorsteinson et al. <a href="#ref-thorsteinson2008anchoring">2008</a>)</span>. Furnham and Boo <span class="citation">(Furnham and Boo <a href="#ref-furnham2011literature">2011</a>)</span> offers a literature review. The effect seems to be originated from the confirmatory testing of hypothesis that selectively activates anchor-consistent information in memory <span class="citation">(Block and Harper <a href="#ref-block1991overconfidence">1991</a>; Chapman and Johnson <a href="#ref-chapman1999anchoring">1999</a>)</span>.</p>
<p>In a classical experiment <span class="citation">(Slovic, Monahan, and MacGregor <a href="#ref-slovic2000violence">2000</a>)</span> the authors asked thousands of forensic experts, trained formally in psychology and psychiatry, to estimate the probability that a given person with mental disorder would reiterate an offense after release from hospital. Two random assignments were used, one in which there was a suggestion to use a 1-40 scale to estimate probability, the other to use a 1-100 scale. Those using the 1-100 scale estimated that the probability to re-offend was approximately double than the one estimated by the other group. Another striking example was found by Ariely et al. <span class="citation">(Ariely, Loewenstein, and Prelec <a href="#ref-ariely2003coherent">2003</a>)</span>: the willingness to pay for consumption goods is influenced by an arbitrary anchor created by asking the subjects to remember the last two digits of their Social Security Number.</p>
<p>The magnitude of the bias is a function of personality and motivational orientation <span class="citation">(Eroglu and Croxton <a href="#ref-eroglu2010biases">2010</a>)</span> and of the social origin of the anchor value <span class="citation">(Meub and Proeger <a href="#ref-meub2015anchoring">2015</a>)</span>, but not of cognitive abilities, so that experts are also subject to its effect <span class="citation">(Mussweiler and Strack <a href="#ref-mussweiler2000numeric">2000</a>; Oechssler, Roider, and Schmitz <a href="#ref-oechssler2009cognitive">2009</a>; Bergman et al. <a href="#ref-bergman2010anchoring">2010</a>)</span>. However, the level of knowledge of experts moderates the intensity of the anchoring effect <span class="citation">(Wilson et al. <a href="#ref-wilson1996new">1996</a>; Smith, Windschitl, and Bruchmann <a href="#ref-smith2013knowledge">2013</a>)</span>.</p>
<p>The extent to which the anchoring bias can be mitigated is the object of large research. George, Duffy and Auja <span class="citation">(George, Duffy, and Ahuja <a href="#ref-george2000countering">2000</a>)</span> and Legerstee and Franses <span class="citation">(Legerstee and Franses <a href="#ref-legerstee2014experts">2014</a>)</span> report that after a training on the feedback from automated statistical program the accuracy of experts improves substantially, while Mussweiler et al. <span class="citation">(Mussweiler <a href="#ref-mussweiler2002malleability">2002</a>)</span> and Furnham and Boo <span class="citation">(Furnham and Boo <a href="#ref-furnham2011literature">2011</a>)</span> suggest that using compensating strategies, such as consider-the-opposite strategy, may reduce the anchoring bias (see also Mussweiler and Strack <span class="citation">(Mussweiler and Strack <a href="#ref-mussweiler2001semantics">2001</a>)</span>). Providing experts with a feedback that uses metric knowledge (but not mapping knowledge) reduces the anchoring bias <span class="citation">(Smith and Windschitl <a href="#ref-smith2015resisting">2015</a>)</span>. Experts may receive incentives for the accuracy of their predictions, so that a loss function is defined and implemented <span class="citation">(Lawrence and O’Connor <a href="#ref-lawrence2005judgmental">2005</a>)</span>. This does not eliminate biases in the adjustment: Franses, Legerstee and Paap <span class="citation">(Franses, Legerstee, and Paap <a href="#ref-franses2011estimating">2011</a>)</span> show that the loss functions of experts is typically asymmetric, so that underprediction is penalized more.</p>
</div>
<div id="overconfidence" class="section level4 unnumbered">
<h4>Overconfidence</h4>
<p>A pervasive effect observed in many settings is the inability of individuals to evaluate correctly the degree of knowledge they have about issues or facts on which their judgment is requested <span class="citation">(Shanteau and others <a href="#ref-shanteau1992competence">1992</a>)</span>. Moore and Healey <span class="citation">(Moore and Healy <a href="#ref-moore2008trouble">2008</a>)</span> review a large literature, starting from a pioneering observation by Oskamp <span class="citation">(Oskamp <a href="#ref-oskamp1965overconfidence">1965</a>)</span>. The dominant effect is that people overestimate their own ability to perform accurate judgments (precision or accuracy), that is, they exhibit overconfidence in the form of overprecision, or unwarranted certainty <span class="citation">(Koriat, Lichtenstein, and Fischhoff <a href="#ref-koriat1980reasons">1980</a>)</span>. They also overestimate their absolute ability or performance (overestimation) and their relative comparison within a group (overplacement) <span class="citation">(Kruger and Dunning <a href="#ref-kruger1999unskilled">1999</a>)</span>.</p>
<p>According to Plous “no problem in judgment and decision making is more prevalent and more potentially catastrophic than overconfidence” <span class="citation">(Plous <a href="#ref-plous1993psychology">1993</a>)</span>. In technical terms, people are not good in defining appropriately the confidence interval they assign to their own estimates <span class="citation">(Lichtenstein, Fischhoff, and Phillips <a href="#ref-lichtenstein1977calibration">1977</a>)</span>. If confidence is defined at 90%, it means that about 90% of the probability intervals provided by experts will include the observed (true) values. In a classical experiment, on the contrary, Russo and Schoemaker <span class="citation">(Russo and Schoemaker <a href="#ref-russo1992managing">1992</a>)</span> found that only betwen 40% an 60% of the intervals suggested by managers included the true value of several economic variables, while respondents claimed they were 90% confident about the estimate. The overconfidence leads to illusion of competence <span class="citation">(CAstel, MCCAbe, and Roediger <a href="#ref-castel2007illusions">2007</a>)</span>.</p>
<p>Very importantly for our discussion, experts produce on average better estimates than non experts, but they are subject to overconfidence in the same way: “Experts are good at reporting relatively narrow intervals centered on true values, but they are no better than novices at reporting well calibrated, high-confidence interval” <span class="citation">(McKenzie, Liersch, and Yaniv <a href="#ref-mckenzie2008overconfidence">2008</a>)</span>.</p>
<p>It has also been suggested that the willingness of people to revise their judgment in the light of feedback depends on their power and egocentrism. People who self-perceive themselves as powerful tend to be more self-confident and do not adjust their own judgments following external feedback <span class="citation">(Bonaccio and Dalal <a href="#ref-bonaccio2006advice">2006</a>)</span>.</p>
</div>
<div id="hindsight-bias" class="section level4 unnumbered">
<h4>Hindsight bias</h4>
<p>Another distortion that makes it difficult to learn from past events and external knowledge is the hindsight bias. The hindsight bias, initially discussed by Fischhoff <span class="citation">(Fischhoff <a href="#ref-fischhoff2003hindsight">2003</a>)</span>, is the unjustified increase in the perceived probability of an event due to the knowledge of how the event actually took place, or outcome knowledge <span class="citation">(Hawkins and Hastie <a href="#ref-hawkins1990hindsight">1990</a>)</span>. People who know the outcome of an event find it difficult to report objectively the predictions they made before the knowledge of the outcome.</p>
<p>For example, after the outcome of a disease, physicians may become unable to retrieve the reasons for their diagnosis, which at the time of execution was based on a prediction judgment <span class="citation">(Arkes and Harkness <a href="#ref-arkes1980effect">1980</a>; Arkes et al. <a href="#ref-arkes1981hindsight">1981</a>; Arkes and Freedman <a href="#ref-arkes1984demonstration">1984</a>)</span>. As another example, Herrmann and Choi <span class="citation">(Herrmann and Choi <a href="#ref-herrmann2007prediction">2007</a>)</span> examined the judgments of senior experts in foreign policy in Korea after the international political events were manifest and asked them to reconstruct their predictions, finding that “experts tend to forget what they used to believe and conclude in hindsight that they understand the causal forces driving developments” <span class="citation">(Herrmann and Choi <a href="#ref-herrmann2007prediction">2007</a>)</span>. Christensen-Szahansky and Willham <span class="citation">(Christensen-Szalanski and Willham <a href="#ref-christensen1991hindsight">1991</a>)</span> offers a meta-analysis of 128 studies on the effect and Guilbault et al <span class="citation">(Guilbault et al. <a href="#ref-guilbault2004meta">2004</a>)</span> update it.</p>
<p>This effect is also labeled “curse of knowledge”, or “curse of expertise”, that is, the tendency to be biased by one’s current knowledge in the evaluation of the knowledge of others (that is, the same person in an earlier perspective, or someone’s else) or the difficulty in discounting one’s privileged knowledge in judging what other people know or should know <span class="citation">(Koriat and Bjork <a href="#ref-koriat2006illusions">2006</a>)</span>.</p>
<p>In other words, outcome knowledge compromises the ability of respondents to appreciate one’s own prior knowledge and another person’s naïve knowledge <span class="citation">(Bernstein et al. <a href="#ref-bernstein2012auditory">2012</a>)</span> and therefore produces an exaggerated perception of the a priori predictability, or obviousness, of outcomes after they become known <span class="citation">(Fessel, Epstude, and Roese <a href="#ref-fessel2009hindsight">2009</a>; Lassiter <a href="#ref-lassiter2010videotaped">2010</a>)</span>, leading to the inability to learn from the past and the underestimation of the informativeness of facts <span class="citation">(Mazursky and Ofir <a href="#ref-mazursky1996knew">1996</a>)</span>.</p>
<p>Interestingly, expertise exacerbates the hindsight bias <span class="citation">(Knoll and Arkes <a href="#ref-knoll2017effects">2017</a>)</span>. Experts incorporate more easily new knowledge but tend to “go beyond the information given”. This means that they erroneously infer information that was not actually presented or consider old the newly presented information <span class="citation">(Arkes and Freedman <a href="#ref-arkes1984demonstration">1984</a>)</span>. They tend to take for granted elements that are in their knowledge, failing to examine whether the same level of knowledge is available to others. As shown by Tetlock and Lebow <span class="citation">(Tetlock and Lebow <a href="#ref-tetlock2001poking">2001</a>)</span>, experts that have a preference for explanatory closure and are theory-driven, tend to exaggerate the degree to which “they knew it all along”.</p>
</div>
<div id="desirability-bias" class="section level4 unnumbered">
<h4>Desirability bias</h4>
<p>One of the fundamental assumptions of rational decision theory is that people are able to separate the judgment from the motivation, that is, the estimation of the frequency or probability of an event, from the desirability of the event. Beliefs and desires should be a priori independent. A large and consistent literature shows that this is not the case: people systematically inflate the judged probability of desirable events, and diminish that of undesirable events. This is an example of motivational bias, or the influence of motivational factors, such as preferences or desires, on cognitive activities. This distortion is, in principle, of great importance if experts are asked to judge the future developments of techologies which they consider, for any possible reason, desirable. This seems to be plausible in data science, in which experts come from industry, academia, or consulting.</p>
<p>Initially discovered in the ‘50s, the desirability bias has been the object of an extensive empirical research. It is considered a case of motivated reasoning <span class="citation">(Kunda <a href="#ref-kunda1990case">1990</a>)</span>, or interdependence between cognitive and motivational factors, and is also labelled wishful thinking. People distort their evaluation of probability in the direction of their preferred decision alternative <span class="citation">(DeKay, Patiño-Echeverri, and Fischbeck <a href="#ref-dekay2009distortion">2009</a>)</span>. It has been found not only in lay people <span class="citation">(Lench and Bench <a href="#ref-lench2012automatic">2012</a>)</span>, but in experts in a variety of fields, from professional investors to price forecasters , from political election voters , to sport fans . Interestingly, people exhibit the desirability bias not only when they have preferences before the choice (disposition), but also when the preferences are acquired during the decision process <span class="citation">(Russo, Medvec, and Meloy <a href="#ref-russo1996distortion">1996</a>)</span>.</p>
<p>The explanations for this effect are multiple: people distort their judgment because the future prospects have an impact on their personal and affective future <span class="citation">(Ayton, Hunt, and Wright <a href="#ref-ayton1989psychological">1989</a>; Wright, Lawrence, and Collopy <a href="#ref-wright1996role">1996</a>; Lench and Ditto <a href="#ref-lench2008automatic">2008</a>)</span>, or because there is high uncertainty and large amount of guessing on future prospects <span class="citation">(Windschitl et al. <a href="#ref-windschitl2010desirability">2010</a>)</span>, or because the prospect horizon is very far <span class="citation">(Vosgerau <a href="#ref-vosgerau2010prevalent">2010</a>)</span>. A related distortion, called advocacy bias, refers to the consciuos over-estimation of the prospects for technologies of products, when there is competition for resources. Here experts may inflate the probability of success in order to actively champion some technologies.</p>
<p>Of particular interest is the situation in which overoptimism, induced by the desirability bias, is associated to overconfidence, or the misplaced belief in the accuracy of one’s own judgments. Several studies show that overconfidence is frequently found in people with top managerial responsibilities in companies <span class="citation">(Hribar and Yang <a href="#ref-hribar2016ceo">2016</a>)</span>. It is not known to what extent people involved in data science are also subject to this effect, but this is clearly an area for further research.</p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wickham2016r">
<p>Wickham, Hadley, and Garrett Grolemund. 2016. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-r2008">
<p>R Development Core Team. 2008. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="http://www.R-project.org" class="uri">http://www.R-project.org</a>.</p>
</div>
<div id="ref-py95">
<p>Rossum, Guido. 1995. “Python Reference Manual.” Amsterdam, The Netherlands, The Netherlands: CWI (Centre for Mathematics; Computer Science).</p>
</div>
<div id="ref-yuwono1996search">
<p>Yuwono, Budi, and Dik Lun Lee. 1996. “Search and Ranking Algorithms for Locating Resources on the World Wide Web.” In <em>Data Engineering, 1996. Proceedings of the Twelfth International Conference on</em>, 164–71. IEEE.</p>
</div>
<div id="ref-readr2017r">
<p>Wickham, Hadley, Jim Hester, and Romain Francois. 2017. <em>Readr: Read Rectangular Text Data</em>. <a href="https://CRAN.R-project.org/package=readr" class="uri">https://CRAN.R-project.org/package=readr</a>.</p>
</div>
<div id="ref-wickham2014tidy">
<p>Wickham, Hadley, and others. 2014. “Tidy Data.” <em>Journal of Statistical Software</em> 59 (10). Foundation for Open Access Statistics: 1–23.</p>
</div>
<div id="ref-leek2015elements">
<p>Leek, Jeff. 2015. “The Elements of Data Analytic Style.” <em>J. Leek.—Amazon Digital Services, Inc</em>.</p>
</div>
<div id="ref-silge2016tidytext">
<p>Silge, Julia, and David Robinson. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” <em>The Journal of Open Source Software</em> 1 (3): 37.</p>
</div>
<div id="ref-mctear2016conversational">
<p>McTear, Michael, Zoraida Callejas, and David Griol. 2016. <em>The Conversational Interface: Talking to Smart Devices</em>. Springer.</p>
</div>
<div id="ref-dell2009ensemble">
<p>Dell’Orletta, Felice. 2009. “Ensemble System for Part-of-Speech Tagging.” <em>Proceedings of EVALITA</em> 9: 1–8.</p>
</div>
<div id="ref-attardi2009reverse">
<p>Attardi, Giuseppe, and Felice Dell’Orletta. 2009. “Reverse Revision and Linear Tree Combination for Dependency Parsing.” In <em>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</em>, 261–64. Association for Computational Linguistics.</p>
</div>
<div id="ref-attardi2009accurate">
<p>Attardi, Giuseppe, Felice Dell’Orletta, Maria Simi, and Joseph Turian. 2009. “Accurate Dependency Parsing with a Stacked Multilayer Perceptron.” <em>Proceedings of EVALITA</em> 9: 1–8.</p>
</div>
<div id="ref-porter1980algorithm">
<p>Porter, Martin F. 1980. “An Algorithm for Suffix Stripping.” <em>Program</em> 14 (3). MCB UP Ltd: 130–37.</p>
</div>
<div id="ref-hankamer1989morphological">
<p>Hankamer, Jorge. 1989. “Morphological Parsing and the Lexicon.” In <em>Lexical Representation and Process</em>, 392–408. MIT Press.</p>
</div>
<div id="ref-attardi2006experiments">
<p>Attardi, Giuseppe. 2006. “Experiments with a Multilanguage Non-Projective Dependency Parser.” In <em>Proceedings of the Tenth Conference on Computational Natural Language Learning</em>, 166–70. Association for Computational Linguistics.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
<div id="ref-lafferty2001document">
<p>Lafferty, John, and Chengxiang Zhai. 2001. “Document Language Models, Query Models, and Risk Minimization for Information Retrieval.” In <em>Proceedings of the 24th Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 111–19. ACM.</p>
</div>
<div id="ref-bellegarda2004statistical">
<p>Bellegarda, Jerome R. 2004. “Statistical Language Model Adaptation: Review and Perspectives.” <em>Speech Communication</em> 42 (1). Elsevier: 93–108.</p>
</div>
<div id="ref-brants2012large">
<p>Brants, Thorsten, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2012. “Large Language Models in Machine Translation.” Google Patents.</p>
</div>
<div id="ref-gibson1994tools">
<p>Gibson, Kathleen R, Kathleen Rita Gibson, and Tim Ingold. 1994. <em>Tools, Language and Cognition in Human Evolution</em>. Cambridge University Press.</p>
</div>
<div id="ref-borko1963automatic">
<p>Borko, Harold, and Myrna Bernick. 1963. “Automatic Document Classification.” <em>Journal of the ACM (JACM)</em> 10 (2). ACM: 151–62.</p>
</div>
<div id="ref-aggarwal2012survey">
<p>Aggarwal, Charu C, and ChengXiang Zhai. 2012. “A Survey of Text Classification Algorithms.” In <em>Mining Text Data</em>, 163–222. Springer.</p>
</div>
<div id="ref-sebastiani2002machine">
<p>Sebastiani, Fabrizio. 2002. “Machine Learning in Automated Text Categorization.” <em>ACM Computing Surveys (CSUR)</em> 34 (1). ACM: 1–47.</p>
</div>
<div id="ref-yang2004building">
<p>Yang, Qiang, Tianyi Li, and Ke Wang. 2004. “Building Association-Rule Based Sequential Classifiers for Web-Document Prediction.” <em>Data Mining and Knowledge Discovery</em> 8 (3). Springer: 253–73.</p>
</div>
<div id="ref-joachims1998text">
<p>Joachims, Thorsten. 1998. “Text Categorization with Support Vector Machines: Learning with Many Relevant Features.” In <em>European Conference on Machine Learning</em>, 137–42. Springer.</p>
</div>
<div id="ref-manevitz2001one">
<p>Manevitz, Larry M, and Malik Yousef. 2001. “One-Class Svms for Document Classification.” <em>Journal of Machine Learning Research</em> 2 (Dec): 139–54.</p>
</div>
<div id="ref-pop2006approach">
<p>Pop, Ioan. 2006. “An Approach of the Naive Bayes Classifier for the Document Classification.” <em>General Mathematics</em> 14 (4). University: 135–38.</p>
</div>
<div id="ref-manevitz2007one">
<p>Manevitz, Larry, and Malik Yousef. 2007. “One-Class Document Classification via Neural Networks.” <em>Neurocomputing</em> 70 (7-9). Elsevier: 1466–81.</p>
</div>
<div id="ref-lai2015recurrent">
<p>Lai, Siwei, Liheng Xu, Kang Liu, and Jun Zhao. 2015. “Recurrent Convolutional Neural Networks for Text Classification.” In <em>AAAI</em>, 333:2267–73.</p>
</div>
<div id="ref-kim2014convolutional">
<p>Kim, Yoon. 2014. “Convolutional Neural Networks for Sentence Classification.” <em>arXiv Preprint arXiv:1408.5882</em>.</p>
</div>
<div id="ref-pang2008opinion">
<p>Pang, Bo, Lillian Lee, and others. 2008. “Opinion Mining and Sentiment Analysis.” <em>Foundations and Trends in Information Retrieval</em> 2 (1–2). Now Publishers, Inc.: 1–135.</p>
</div>
<div id="ref-liu2012survey">
<p>Liu, Bing, and Lei Zhang. 2012. “A Survey of Opinion Mining and Sentiment Analysis.” In <em>Mining Text Data</em>, 415–63. Springer.</p>
</div>
<div id="ref-miller1995wordnet">
<p>Miller, George A. 1995. “WordNet: A Lexical Database for English.” <em>Communications of the ACM</em> 38 (11). ACM: 39–41.</p>
</div>
<div id="ref-hu2004mining">
<p>Hu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer Reviews.” In <em>Proceedings of the Tenth Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 168–77. ACM.</p>
</div>
<div id="ref-kim2004determining">
<p>Kim, Soo-Min, and Eduard Hovy. 2004. “Determining the Sentiment of Opinions.” In <em>Proceedings of the 20th International Conference on Computational Linguistics</em>, 1367. Association for Computational Linguistics.</p>
</div>
<div id="ref-ding2008holistic">
<p>Ding, Xiaowen, Bing Liu, and Philip S Yu. 2008. “A Holistic Lexicon-Based Approach to Opinion Mining.” In <em>Proceedings of the 2008 International Conference on Web Search and Data Mining</em>, 231–40. ACM.</p>
</div>
<div id="ref-baccianella2010sentiwordnet">
<p>Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. 2010. “Sentiwordnet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.” In <em>Lrec</em>, 10:2200–2204. 2010.</p>
</div>
<div id="ref-philip1966general">
<p>Philip, J, Dexter C Dunphy, Marshall S Smith, Daniel M Ogilvie, and others. 1966. “The General Inquirer: A Computer Approach to Content Analysis.” The MIT Press Cambridge.</p>
</div>
<div id="ref-wiebe1999development">
<p>Wiebe, Janyce M, Rebecca F Bruce, and Thomas P O’Hara. 1999. “Development and Use of a Gold-Standard Data Set for Subjectivity Classifications.” In <em>Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics</em>, 246–53. Association for Computational Linguistics.</p>
</div>
<div id="ref-chiarello2017product">
<p>Chiarello, Filippo, Gualtiero Fantoni, Andrea Bonaccorsi, and others. 2017. “Product Description in Terms of Advantages and Drawbacks: Exploiting Patent Information in Novel Ways.” In <em>DS 87-6 Proceedings of the 21st International Conference on Engineering Design (Iced 17) Vol 6: Design Information and Knowledge, Vancouver, Canada, 21-25.08. 2017</em>, 101–10.</p>
</div>
<div id="ref-mullen2004sentiment">
<p>Mullen, Tony, and Nigel Collier. 2004. “Sentiment Analysis Using Support Vector Machines with Diverse Information Sources.” In <em>Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</em>.</p>
</div>
<div id="ref-prabowo2009sentiment">
<p>Prabowo, Rudy, and Mike Thelwall. 2009. “Sentiment Analysis: A Combined Approach.” <em>Journal of Informetrics</em> 3 (2). Elsevier: 143–57.</p>
</div>
<div id="ref-pang2002thumbs">
<p>Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. 2002. “Thumbs up?: Sentiment Classification Using Machine Learning Techniques.” In <em>Proceedings of the Acl-02 Conference on Empirical Methods in Natural Language Processing-Volume 10</em>, 79–86. Association for Computational Linguistics.</p>
</div>
<div id="ref-jain1988algorithms">
<p>Jain, Anil K, and Richard C Dubes. 1988. “Algorithms for Clustering Data.” Prentice-Hall, Inc.</p>
</div>
<div id="ref-kaufman2009finding">
<p>Kaufman, Leonard, and Peter J Rousseeuw. 2009. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. Vol. 344. John Wiley; Sons.</p>
</div>
<div id="ref-anick1997exploiting">
<p>Anick, Peter G, and Shivakumar Vaithyanathan. 1997. “Exploiting Clustering and Phrases for Context-Based Information Retrieval.” In <em>ACM Sigir Forum</em>, 31:314–23. SI. ACM.</p>
</div>
<div id="ref-cutting1993constant">
<p>Cutting, Douglass R, David R Karger, and Jan O Pedersen. 1993. “Constant Interaction-Time Scatter/Gather Browsing of Very Large Document Collections.” In <em>Proceedings of the 16th Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 126–34. ACM.</p>
</div>
<div id="ref-guha1998cure">
<p>Guha, Sudipto, Rajeev Rastogi, and Kyuseok Shim. 1998. “CURE: An Efficient Clustering Algorithm for Large Databases.” In <em>ACM Sigmod Record</em>, 27:73–84. 2. ACM.</p>
</div>
<div id="ref-han2001spatial">
<p>Han, Jiawei, Micheline Kamber, and Anthony KH Tung. 2001. “Spatial Clustering Methods in Data Mining.” <em>Geographic Data Mining and Knowledge Discovery</em>. Cambridge, UK: Cambridge University Press, 188–217.</p>
</div>
<div id="ref-zhang1996birch">
<p>Zhang, Tian, Raghu Ramakrishnan, and Miron Livny. 1996. “BIRCH: An Efficient Data Clustering Method for Very Large Databases.” In <em>ACM Sigmod Record</em>, 25:103–14. 2. ACM.</p>
</div>
<div id="ref-cutting2017scatter">
<p>Cutting, Douglass R, David R Karger, Jan O Pedersen, and John W Tukey. 2017. “Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections.” In <em>ACM Sigir Forum</em>, 51:148–59. 2. ACM.</p>
</div>
<div id="ref-schutze1997projections">
<p>Schütze, Hinrich, and Craig Silverstein. 1997. “Projections for Efficient Document Clustering.” In <em>ACM Sigir Forum</em>, 31:74–81. SI. ACM.</p>
</div>
<div id="ref-baker1998distributional">
<p>Baker, L Douglas, and Andrew Kachites McCallum. 1998. “Distributional Clustering of Words for Text Classification.” In <em>Proceedings of the 21st Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 96–103. ACM.</p>
</div>
<div id="ref-bekkerman2001feature">
<p>Bekkerman, Ron, Ran El-Yaniv, Naftali Tishby, and Yoad Winter. 2001. “On Feature Distributional Clustering for Text Categorization.” In <em>Proceedings of the 24th Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 146–53. ACM.</p>
</div>
<div id="ref-ricardo2011modern">
<p>Ricardo, BY, and RN Berthier. 2011. “Modern Information Retrieval: The Concepts and Technology Behind Search Second Edition.” <em>Addision Wesley</em> 84 (2).</p>
</div>
<div id="ref-eddy1996hidden">
<p>Eddy, Sean R. 1996a. “Hidden Markov Models.” <em>Current Opinion in Structural Biology</em> 6 (3). Elsevier: 361–65.</p>
</div>
<div id="ref-lafferty2001conditional">
<p>Lafferty, John, Andrew McCallum, and Fernando CN Pereira. 2001a. “Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.”</p>
</div>
<div id="ref-hearst1998support">
<p>Hearst, Marti A., Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. 1998. “Support Vector Machines.” <em>IEEE Intelligent Systems and Their Applications</em> 13 (4). IEEE: 18–28.</p>
</div>
<div id="ref-lample2016neural">
<p>Lample, Guillaume, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. “Neural Architectures for Named Entity Recognition.” <em>arXiv Preprint arXiv:1603.01360</em>.</p>
</div>
<div id="ref-misawa2017character">
<p>Misawa, Shotaro, Motoki Taniguchi, Yasuhide Miura, and Tomoko Ohkuma. 2017. “Character-Based Bidirectional Lstm-Crf with Words and Characters for Japanese Named Entity Recognition.” In <em>Proceedings of the First Workshop on Subword and Character Level Models in Nlp</em>, 97–102.</p>
</div>
<div id="ref-crain2012dimensionality">
<p>Crain, Steven P, Ke Zhou, Shuang-Hong Yang, and Hongyuan Zha. 2012. “Dimensionality Reduction and Topic Modeling: From Latent Semantic Indexing to Latent Dirichlet Allocation and Beyond.” In <em>Mining Text Data</em>, 129–61. Springer.</p>
</div>
<div id="ref-blei2003latent">
<p>Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3 (Jan): 993–1022.</p>
</div>
<div id="ref-parker2017opinionated">
<p>Parker, Hilary. 2017. “Opinionated Analysis Development.” <em>PeerJ Preprints</em> 5. PeerJ Inc. San Francisco, USA: e3210v1.</p>
</div>
<div id="ref-friendly2001milestones">
<p>Friendly, Michael, and Daniel J Denis. 2001. “Milestones in the History of Thematic Cartography, Statistical Graphics, and Data Visualization.” <em>URL Http://Www. Datavis. ca/Milestones</em> 32: 13.</p>
</div>
<div id="ref-wickham2016ggplot2">
<p>Wickham, Hadley. 2016. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer.</p>
</div>
<div id="ref-bikakis2018big">
<p>Bikakis, Nikos. 2018. “Big Data Visualization Tools.” <em>arXiv Preprint arXiv:1801.08336</em>.</p>
</div>
<div id="ref-loukides2011data">
<p>Loukides, Mike. 2011. <em>What Is Data Science?</em> “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-tufte1990envisioning">
<p>Tufte, Edward R, Nora Hillman Goeler, and Richard Benson. 1990. <em>Envisioning Information</em>. Vol. 126. Graphics press Cheshire, CT.</p>
</div>
<div id="ref-tufte2006beautiful">
<p>Tufte, Edward R. 2006. <em>Beautiful Evidence</em>. Vol. 1. Graphics Press Cheshire, CT.</p>
</div>
<div id="ref-mulrow2002visual">
<p>Mulrow, Edward J. 2002. “The Visual Display of Quantitative Information.” Taylor &amp; Francis.</p>
</div>
<div id="ref-few2012show">
<p>Few, Stephen. 2012. <em>Show Me the Numbers: Designing Tables and Graphs to Enlighten</em>. Analytics Press.</p>
</div>
<div id="ref-wilkinson2006grammar">
<p>Wilkinson, Leland. 2006. <em>The Grammar of Graphics</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-wickham2008ggplot2">
<p>Wickham, Hadley, Winston Chang, and others. 2008. “Ggplot2: An Implementation of the Grammar of Graphics.” <em>R Package Version 0.7, URL: Http://CRAN. R-Project. Org/Package= Ggplot2</em>.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2018. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-R-rmarkdown">
<p>Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, and Winston Chang. 2018. <em>Rmarkdown: Dynamic Documents for R</em>. <a href="https://CRAN.R-project.org/package=rmarkdown" class="uri">https://CRAN.R-project.org/package=rmarkdown</a>.</p>
</div>
<div id="ref-shiny2017">
<p>Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2017. <em>Shiny: Web Application Framework for R</em>. <a href="https://CRAN.R-project.org/package=shiny" class="uri">https://CRAN.R-project.org/package=shiny</a>.</p>
</div>
<div id="ref-plotly2017">
<p>Sievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2017. <em>Plotly: Create Interactive Web Graphics via ’Plotly.js’</em>. <a href="https://CRAN.R-project.org/package=plotly" class="uri">https://CRAN.R-project.org/package=plotly</a>.</p>
</div>
<div id="ref-ggprah2018">
<p>Pedersen, Thomas Lin. 2018. <em>Ggraph: An Implementation of Grammar of Graphics for Graphs and Networks</em>. <a href="https://CRAN.R-project.org/package=ggraph" class="uri">https://CRAN.R-project.org/package=ggraph</a>.</p>
</div>
<div id="ref-ICWSM09154">
<p>Bastian, Mathieu, Sebastien Heymann, and Mathieu Jacomy. 2009. “Gephi: An Open Source Software for Exploring and Manipulating Networks.”</p>
</div>
<div id="ref-apreda2016functional">
<p>Apreda, Riccardo, Andrea Bonaccorsi, Felice dell’Orletta, and Gualtiero Fantoni. 2016. “Functional Technology Foresight. a Novel Methodology to Identify Emerging Technologies.” <em>European Journal of Futures Research</em> 4 (1). Springer: 13.</p>
</div>
<div id="ref-popper2008foresight">
<p>Popper, Rafael. 2008. “How Are Foresight Methods Selected?” <em>Foresight</em> 10 (6). Emerald Group Publishing Limited: 62–89.</p>
</div>
<div id="ref-bracken2013making">
<p>Bracken, Louise J, and Elizabeth A Oughton. 2013. “Making Sense of Policy Implementation: The Construction and Uses of Expertise and Evidence in Managing Freshwater Environments.” <em>Environmental Science &amp; Policy</em> 30. Elsevier: 10–18.</p>
</div>
<div id="ref-woudenberg1991evaluation">
<p>Woudenberg, Fred. 1991. “An Evaluation of Delphi.” <em>Technological Forecasting and Social Change</em> 40 (2). North-Holland: 131–50.</p>
</div>
<div id="ref-graefe2011comparing">
<p>Graefe, Andreas, and J Scott Armstrong. 2011. “Comparing Face-to-Face Meetings, Nominal Groups, Delphi and Prediction Markets on an Estimation Task.” <em>International Journal of Forecasting</em> 27 (1). Elsevier: 183–95.</p>
</div>
<div id="ref-kahneman2011thinking">
<p>Kahneman, Daniel, and Patrick Egan. 2011. <em>Thinking, Fast and Slow</em>. Vol. 1. Farrar, Straus; Giroux New York.</p>
</div>
<div id="ref-stanovich2008relative">
<p>Stanovich, Keith E, and Richard F West. 2008. “On the Relative Independence of Thinking Biases and Cognitive Ability.” <em>Journal of Personality and Social Psychology</em> 94 (4). American Psychological Association: 672.</p>
</div>
<div id="ref-taleb2007black">
<p>Taleb, Nassim Nicholas. 2007. <em>The Black Swan: The Impact of the Highly Improbable</em>. Vol. 2. Random house.</p>
</div>
<div id="ref-meijering2016effect">
<p>Meijering, Jurian V, and Hilde Tobi. 2016. “The Effect of Controlled Opinion Feedback on Delphi Features: Mixed Messages from a Real-World Delphi Experiment.” <em>Technological Forecasting and Social Change</em> 103. Elsevier: 166–73.</p>
</div>
<div id="ref-makkonen2016policy">
<p>Makkonen, Marika, Teppo Hujala, and Jussi Uusivuori. 2016. “Policy Experts’ Propensity to Change Their Opinion Along Delphi Rounds.” <em>Technological Forecasting and Social Change</em> 109. Elsevier: 61–68.</p>
</div>
<div id="ref-janis1972victims">
<p>Janis, Irving L. 1972. “Victims of Groupthink: A Psychological Study of Foreign-Policy Decisions and Fiascoes.” Houghton Mifflin.</p>
</div>
<div id="ref-esser1998alive">
<p>Esser, James K. 1998. “Alive and Well After 25 Years: A Review of Groupthink Research.” <em>Organizational Behavior and Human Decision Processes</em> 73 (2-3). Elsevier: 116–41.</p>
</div>
<div id="ref-tuomi2013next">
<p>Tuomi, Ikka. 2013. “Next-Generation Foresight in Anticipatory Organizations.” In <em>Background Study for the European Forum on Forward-Looking Activities (Effla), European Commission</em>.</p>
</div>
<div id="ref-arnott1998decision">
<p>Arnott, David. 1998. “Decision Biases and Decision Support Systems Development.” Citeseer.</p>
</div>
<div id="ref-mousavi2014risk">
<p>Mousavi, Shabnam, and Gerd Gigerenzer. 2014. “Risk, Uncertainty, and Heuristics.” <em>Journal of Business Research</em> 67 (8). Elsevier: 1671–8.</p>
</div>
<div id="ref-gigerenzer1991make">
<p>Gigerenzer, Gerd. 1991. “How to Make Cognitive Illusions Disappear: Beyond ‘Heuristics and Biases’.” <em>European Review of Social Psychology</em> 2 (1). Taylor &amp; Francis: 83–115.</p>
</div>
<div id="ref-goodwin2015history">
<p>Goodwin, C James. 2015. <em>A History of Modern Psychology</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-tversky1981framing">
<p>Tversky, Amos, and Daniel Kahneman. 1981. “The Framing of Decisions and the Psychology of Choice.” <em>Science</em> 211 (4481). American Association for the Advancement of Science: 453–58.</p>
</div>
<div id="ref-levin1998all">
<p>Levin, Irwin P, Sandra L Schneider, and Gary J Gaeth. 1998. “All Frames Are Not Created Equal: A Typology and Critical Analysis of Framing Effects.” <em>Organizational Behavior and Human Decision Processes</em> 76 (2). Academic Press: 149–88.</p>
</div>
<div id="ref-loke1992effects">
<p>Loke, Wing Hong, and Kai Foong Tan. 1992. “Effects of Framing and Missing Information in Expert and Novice Judgment.” <em>Bulletin of the Psychonomic Society</em> 30 (3). Springer: 187–90.</p>
</div>
<div id="ref-larkin1980expert">
<p>Larkin, Jill, John McDermott, Dorothea P Simon, and Herbert A Simon. 1980. “Expert and Novice Performance in Solving Physics Problems.” <em>Science</em> 208 (4450). American Association for the Advancement of Science: 1335–42.</p>
</div>
<div id="ref-cheng2010debiasing">
<p>Cheng, Fei-Fei, and Chin-Shan Wu. 2010. “Debiasing the Framing Effect: The Effect of Warning and Involvement.” <em>Decision Support Systems</em> 49 (3). Elsevier: 328–34.</p>
</div>
<div id="ref-yaniv2011group">
<p>Yaniv, Ilan. 2011. “Group Diversity and Decision Quality: Amplification and Attenuation of the Framing Effect.” <em>International Journal of Forecasting</em> 27 (1). Elsevier: 41–49.</p>
</div>
<div id="ref-nemeth1988modelling">
<p>Nemeth, Charlan, and Cynthia Chiles. 1988. “Modelling Courage: The Role of Dissent in Fostering Independence.” <em>European Journal of Social Psychology</em> 18 (3). Wiley Online Library: 275–80.</p>
</div>
<div id="ref-lord1984considering">
<p>Lord, Charles G, Mark R Lepper, and Elizabeth Preston. 1984. “Considering the Opposite: A Corrective Strategy for Social Judgment.” <em>Journal of Personality and Social Psychology</em> 47 (6). American Psychological Association: 1231.</p>
</div>
<div id="ref-goodwin2010limits">
<p>Goodwin, Paul, and George Wright. 2010. “The Limits of Forecasting Methods in Anticipating Rare Events.” <em>Technological Forecasting and Social Change</em> 77 (3). Elsevier: 355–68.</p>
</div>
<div id="ref-nemeth2001devil">
<p>Nemeth, Charlan, Keith Brown, and John Rogers. 2001. “Devil’s Advocate Versus Authentic Dissent: Stimulating Quantity and Quality.” <em>European Journal of Social Psychology</em> 31 (6). Wiley Online Library: 707–20.</p>
</div>
<div id="ref-keay2012authorising">
<p>Keay, Andrew. 2012. “The Authorising of Directors’ Conflicts of Interest: Getting a Balance?” <em>Journal of Corporate Law Studies</em> 12 (1). Taylor &amp; Francis: 129–62.</p>
</div>
<div id="ref-harries2004combining">
<p>Harries, Clare, Ilan Yaniv, and Nigel Harvey. 2004. “Combining Advice: The Weight of a Dissenting Opinion in the Consensus.” <em>Journal of Behavioral Decision Making</em> 17 (5). Wiley Online Library: 333–48.</p>
</div>
<div id="ref-burgman2015trusting">
<p>Burgman, Mark A. 2015. <em>Trusting Judgements: How to Get the Best Out of Experts</em>. Cambridge University Press.</p>
</div>
<div id="ref-makridakis2009forecasting">
<p>Makridakis, Spyros, Robin M Hogarth, and Anil Gaba. 2009. “Forecasting and Uncertainty in the Economic and Business World.” <em>International Journal of Forecasting</em> 25 (4). Elsevier: 794–812.</p>
</div>
<div id="ref-gigerenzer2015calculated">
<p>Gigerenzer, Gerd. 2015. <em>Calculated Risks: How to Know When Numbers Deceive You</em>. Simon; Schuster.</p>
</div>
<div id="ref-tversky1974judgment">
<p>Tversky, Amos, and Daniel Kahneman. 1974. “Judgment Under Uncertainty: Heuristics and Biases.” <em>Science</em> 185 (4157). American association for the advancement of science: 1124–31.</p>
</div>
<div id="ref-thorsteinson2008anchoring">
<p>Thorsteinson, Todd J, Jennifer Breier, Anna Atwell, Catherine Hamilton, and Monica Privette. 2008. “Anchoring Effects on Performance Judgments.” <em>Organizational Behavior and Human Decision Processes</em> 107 (1). Elsevier: 29–40.</p>
</div>
<div id="ref-furnham2011literature">
<p>Furnham, Adrian, and Hua Chu Boo. 2011. “A Literature Review of the Anchoring Effect.” <em>The Journal of Socio-Economics</em> 40 (1). Elsevier: 35–42.</p>
</div>
<div id="ref-block1991overconfidence">
<p>Block, Richard A, and David R Harper. 1991. “Overconfidence in Estimation: Testing the Anchoring-and-Adjustment Hypothesis.” <em>Organizational Behavior and Human Decision Processes</em> 49 (2). Elsevier: 188–207.</p>
</div>
<div id="ref-chapman1999anchoring">
<p>Chapman, Gretchen B, and Eric J Johnson. 1999. “Anchoring, Activation, and the Construction of Values.” <em>Organizational Behavior and Human Decision Processes</em> 79 (2). Academic Press: 115–53.</p>
</div>
<div id="ref-slovic2000violence">
<p>Slovic, Paul, John Monahan, and Donald G MacGregor. 2000. “Violence Risk Assessment and Risk Communication: The Effects of Using Actual Cases, Providing Instruction, and Employing Probability Versus Frequency Formats.” <em>Law and Human Behavior</em> 24 (3). Springer: 271–96.</p>
</div>
<div id="ref-ariely2003coherent">
<p>Ariely, Dan, George Loewenstein, and Drazen Prelec. 2003. “‘Coherent Arbitrariness’: Stable Demand Curves Without Stable Preferences.” <em>The Quarterly Journal of Economics</em> 118 (1). MIT Press: 73–106.</p>
</div>
<div id="ref-eroglu2010biases">
<p>Eroglu, Cuneyt, and Keely L Croxton. 2010. “Biases in Judgmental Adjustments of Statistical Forecasts: The Role of Individual Differences.” <em>International Journal of Forecasting</em> 26 (1). Elsevier: 116–33.</p>
</div>
<div id="ref-meub2015anchoring">
<p>Meub, Lukas, and Till E Proeger. 2015. “Anchoring in Social Context.” <em>Journal of Behavioral and Experimental Economics</em> 55. Elsevier: 29–39.</p>
</div>
<div id="ref-mussweiler2000numeric">
<p>Mussweiler, Thomas, and Fritz Strack. 2000. “Numeric Judgments Under Uncertainty: The Role of Knowledge in Anchoring.” <em>Journal of Experimental Social Psychology</em> 36 (5). Elsevier: 495–518.</p>
</div>
<div id="ref-oechssler2009cognitive">
<p>Oechssler, Jörg, Andreas Roider, and Patrick W Schmitz. 2009. “Cognitive Abilities and Behavioral Biases.” <em>Journal of Economic Behavior &amp; Organization</em> 72 (1). Elsevier: 147–52.</p>
</div>
<div id="ref-bergman2010anchoring">
<p>Bergman, Oscar, Tore Ellingsen, Magnus Johannesson, and Cicek Svensson. 2010. “Anchoring and Cognitive Ability.” <em>Economics Letters</em> 107 (1). Elsevier: 66–68.</p>
</div>
<div id="ref-wilson1996new">
<p>Wilson, Timothy D, Christopher E Houston, Kathryn M Etling, and Nancy Brekke. 1996. “A New Look at Anchoring Effects: Basic Anchoring and Its Antecedents.” <em>Journal of Experimental Psychology: General</em> 125 (4). American Psychological Association: 387.</p>
</div>
<div id="ref-smith2013knowledge">
<p>Smith, Andrew R, Paul D Windschitl, and Kathryn Bruchmann. 2013. “Knowledge Matters: Anchoring Effects Are Moderated by Knowledge Level.” <em>European Journal of Social Psychology</em> 43 (1). Wiley Online Library: 97–108.</p>
</div>
<div id="ref-george2000countering">
<p>George, Joey F, Kevin Duffy, and Manju Ahuja. 2000. “Countering the Anchoring and Adjustment Bias with Decision Support Systems.” <em>Decision Support Systems</em> 29 (2). Elsevier: 195–206.</p>
</div>
<div id="ref-legerstee2014experts">
<p>Legerstee, Rianne, and Philip Hans Franses. 2014. “Do Experts’ Sku Forecasts Improve After Feedback?” <em>Journal of Forecasting</em> 33 (1). Wiley Online Library: 69–79.</p>
</div>
<div id="ref-mussweiler2002malleability">
<p>Mussweiler, Thomas. 2002. “The Malleability of Anchoring Effects.” <em>Experimental Psychology</em> 49 (1). Hogrefe &amp; Huber Publishers: 67.</p>
</div>
<div id="ref-mussweiler2001semantics">
<p>Mussweiler, Thomas, and Fritz Strack. 2001. “The Semantics of Anchoring.” <em>Organizational Behavior and Human Decision Processes</em> 86 (2). Elsevier: 234–55.</p>
</div>
<div id="ref-smith2015resisting">
<p>Smith, Andrew R, and Paul D Windschitl. 2015. “Resisting Anchoring Effects: The Roles of Metric and Mapping Knowledge.” <em>Memory &amp; Cognition</em> 43 (7). Springer: 1071–84.</p>
</div>
<div id="ref-lawrence2005judgmental">
<p>Lawrence, Michael, and Marcus O’Connor. 2005. “Judgmental Forecasting in the Presence of Loss Functions.” <em>International Journal of Forecasting</em> 21 (1). Elsevier: 3–14.</p>
</div>
<div id="ref-franses2011estimating">
<p>Franses, Philip, Rianne Legerstee, and Richard Paap. 2011. “Estimating Loss Functions of Experts.”</p>
</div>
<div id="ref-shanteau1992competence">
<p>Shanteau, James, and others. 1992. “Competence in Experts: The Role of Task Characteristics.” <em>Organizational Behavior and Human Decision Processes</em> 53. ACADEMIC PRESS INC: 252–52.</p>
</div>
<div id="ref-moore2008trouble">
<p>Moore, Don A, and Paul J Healy. 2008. “The Trouble with Overconfidence.” <em>Psychological Review</em> 115 (2). American Psychological Association: 502.</p>
</div>
<div id="ref-oskamp1965overconfidence">
<p>Oskamp, Stuart. 1965. “Overconfidence in Case-Study Judgments.” <em>Journal of Consulting Psychology</em> 29 (3). American Psychological Association: 261.</p>
</div>
<div id="ref-koriat1980reasons">
<p>Koriat, Asher, Sarah Lichtenstein, and Baruch Fischhoff. 1980. “Reasons for Confidence.” <em>Journal of Experimental Psychology: Human Learning and Memory</em> 6 (2). American Psychological Association: 107.</p>
</div>
<div id="ref-kruger1999unskilled">
<p>Kruger, Justin, and David Dunning. 1999. “Unskilled and Unaware of It: How Difficulties in Recognizing One’s Own Incompetence Lead to Inflated Self-Assessments.” <em>Journal of Personality and Social Psychology</em> 77 (6). American Psychological Association: 1121.</p>
</div>
<div id="ref-plous1993psychology">
<p>Plous, Scott. 1993. <em>The Psychology of Judgment and Decision Making.</em> Mcgraw-Hill Book Company.</p>
</div>
<div id="ref-lichtenstein1977calibration">
<p>Lichtenstein, Sarah, Baruch Fischhoff, and Lawrence D Phillips. 1977. “Calibration of Probabilities: The State of the Art.” In <em>Decision Making and Change in Human Affairs</em>, 275–324. Springer.</p>
</div>
<div id="ref-russo1992managing">
<p>Russo, J Edward, and Paul JH Schoemaker. 1992. “Managing Overconfidence.” <em>Sloan Management Review</em> 33 (2). Cambridge: 7–17.</p>
</div>
<div id="ref-castel2007illusions">
<p>CAstel, AlAn D, DAviD P MCCAbe, and Henry L Roediger. 2007. “Illusions of Competence and Overestimation of Associative Memory for Identical Items: Evidence from Judgments of Learning.” <em>Psychonomic Bulletin &amp; Review</em> 14 (1). Springer: 107–11.</p>
</div>
<div id="ref-mckenzie2008overconfidence">
<p>McKenzie, Craig RM, Michael J Liersch, and Ilan Yaniv. 2008. “Overconfidence in Interval Estimates: What Does Expertise Buy You?” <em>Organizational Behavior and Human Decision Processes</em> 107 (2). Elsevier: 179–91.</p>
</div>
<div id="ref-bonaccio2006advice">
<p>Bonaccio, Silvia, and Reeshad S Dalal. 2006. “Advice Taking and Decision-Making: An Integrative Literature Review, and Implications for the Organizational Sciences.” <em>Organizational Behavior and Human Decision Processes</em> 101 (2). Elsevier: 127–51.</p>
</div>
<div id="ref-fischhoff2003hindsight">
<p>Fischhoff, Baruch. 2003. “Hindsight<span class="math inline">\(\ne\)</span> Foresight: The Effect of Outcome Knowledge on Judgment Under Uncertainty.” <em>BMJ Quality &amp; Safety</em> 12 (4). BMJ Publishing Group Ltd: 304–11.</p>
</div>
<div id="ref-hawkins1990hindsight">
<p>Hawkins, Scott A, and Reid Hastie. 1990. “Hindsight: Biased Judgments of Past Events After the Outcomes Are Known.” <em>Psychological Bulletin</em> 107 (3). American Psychological Association: 311.</p>
</div>
<div id="ref-arkes1980effect">
<p>Arkes, Hal R, and Allan R Harkness. 1980. “Effect of Making a Diagnosis on Subsequent Recognition of Symptoms.” <em>Journal of Experimental Psychology: Human Learning and Memory</em> 6 (5). American Psychological Association: 568.</p>
</div>
<div id="ref-arkes1981hindsight">
<p>Arkes, Hal R, Robert L Wortmann, Paul D Saville, and Allan R Harkness. 1981. “Hindsight Bias Among Physicians Weighing the Likelihood of Diagnoses.” <em>Journal of Applied Psychology</em> 66 (2). American Psychological Association: 252.</p>
</div>
<div id="ref-arkes1984demonstration">
<p>Arkes, Hal R, and Marilyn R Freedman. 1984. “A Demonstration of the Costs and Benefits of Expertise in Recognition Memory.” <em>Memory &amp; Cognition</em> 12 (1). Springer: 84–89.</p>
</div>
<div id="ref-herrmann2007prediction">
<p>Herrmann, Richard K, and Jong Kun Choi. 2007. “From Prediction to Learning: Opening Experts’ Minds to Unfolding History.” <em>International Security</em> 31 (4). MIT Press: 132–61.</p>
</div>
<div id="ref-christensen1991hindsight">
<p>Christensen-Szalanski, Jay JJ, and Cynthia Fobian Willham. 1991. “The Hindsight Bias: A Meta-Analysis.” <em>Organizational Behavior and Human Decision Processes</em> 48 (1). Elsevier: 147–68.</p>
</div>
<div id="ref-guilbault2004meta">
<p>Guilbault, Rebecca L, Fred B Bryant, Jennifer Howard Brockway, and Emil J Posavac. 2004. “A Meta-Analysis of Research on Hindsight Bias.” <em>Basic and Applied Social Psychology</em> 26 (2-3). Taylor &amp; Francis: 103–17.</p>
</div>
<div id="ref-koriat2006illusions">
<p>Koriat, Asher, and Robert A Bjork. 2006. “Illusions of Competence During Study Can Be Remedied by Manipulations That Enhance Learners’ Sensitivity to Retrieval Conditions at Test.” <em>Memory &amp; Cognition</em> 34 (5). Springer: 959–72.</p>
</div>
<div id="ref-bernstein2012auditory">
<p>Bernstein, Daniel M, Alexander Maurice Wilson, Nicole LM Pernat, and Louise R Meilleur. 2012. “Auditory Hindsight Bias.” <em>Psychonomic Bulletin &amp; Review</em> 19 (4). Springer: 588–93.</p>
</div>
<div id="ref-fessel2009hindsight">
<p>Fessel, Florian, Kai Epstude, and Neal J Roese. 2009. “Hindsight Bias Redefined: It’s About Time.” <em>Organizational Behavior and Human Decision Processes</em> 110 (1). Elsevier: 56–64.</p>
</div>
<div id="ref-lassiter2010videotaped">
<p>Lassiter, G Daniel. 2010. “Videotaped Interrogations and Confessions: What’s Obvious in Hindsight May Not Be in Foresight.” Springer.</p>
</div>
<div id="ref-mazursky1996knew">
<p>Mazursky, David, and Chezy Ofir. 1996. “‘I Knew It All Along’ Under All Conditions? Or Possibly ‘I Could Not Have Expected It to Happen’ Under Some Conditions?: Comments on Mark and Mellor (1994).” <em>Organizational Behavior and Human Decision Processes</em> 66 (2). Elsevier: 237–40.</p>
</div>
<div id="ref-knoll2017effects">
<p>Knoll, Melissa AZ, and Hal R Arkes. 2017. “The Effects of Expertise on the Hindsight Bias.” <em>Journal of Behavioral Decision Making</em> 30 (2). Wiley Online Library: 389–99.</p>
</div>
<div id="ref-tetlock2001poking">
<p>Tetlock, Philip E, and Richard Ned Lebow. 2001. “Poking Counterfactual Holes in Covering Laws: Cognitive Styles and Historical Reasoning.” <em>American Political Science Review</em> 95 (4). Cambridge University Press: 829–43.</p>
</div>
<div id="ref-kunda1990case">
<p>Kunda, Ziva. 1990. “The Case for Motivated Reasoning.” <em>Psychological Bulletin</em> 108 (3). American Psychological Association: 480.</p>
</div>
<div id="ref-dekay2009distortion">
<p>DeKay, Michael L, Dalia Patiño-Echeverri, and Paul S Fischbeck. 2009. “Distortion of Probability and Outcome Information in Risky Decisions.” <em>Organizational Behavior and Human Decision Processes</em> 109 (1). Elsevier: 79–92.</p>
</div>
<div id="ref-lench2012automatic">
<p>Lench, Heather C, and Shane W Bench. 2012. “Automatic Optimism: Why People Assume Their Futures Will Be Bright.” <em>Social and Personality Psychology Compass</em> 6 (4). Wiley Online Library: 347–60.</p>
</div>
<div id="ref-russo1996distortion">
<p>Russo, J Edward, Victoria Husted Medvec, and Margaret G Meloy. 1996. “The Distortion of Information During Decisions.” <em>Organizational Behavior and Human Decision Processes</em> 66 (1). Elsevier: 102–10.</p>
</div>
<div id="ref-ayton1989psychological">
<p>Ayton, Peter, Anne J Hunt, and George Wright. 1989. “Psychological Conceptions of Randomness.” <em>Journal of Behavioral Decision Making</em> 2 (4). Wiley Online Library: 221–38.</p>
</div>
<div id="ref-wright1996role">
<p>Wright, George, Michael J Lawrence, and Fred Collopy. 1996. “The Role and Validity of Judgment in Forecasting.” Elsevier.</p>
</div>
<div id="ref-lench2008automatic">
<p>Lench, Heather C, and Peter H Ditto. 2008. “Automatic Optimism: Biased Use of Base Rate Information for Positive and Negative Events.” <em>Journal of Experimental Social Psychology</em> 44 (3). Elsevier: 631–39.</p>
</div>
<div id="ref-windschitl2010desirability">
<p>Windschitl, Paul D, Andrew R Smith, Jason P Rose, and Zlatan Krizan. 2010. “The Desirability Bias in Predictions: Going Optimistic Without Leaving Realism.” <em>Organizational Behavior and Human Decision Processes</em> 111 (1). Elsevier: 33–47.</p>
</div>
<div id="ref-vosgerau2010prevalent">
<p>Vosgerau, Joachim. 2010. “How Prevalent Is Wishful Thinking? Misattribution of Arousal Causes Optimism and Pessimism in Subjective Probabilities.” <em>Journal of Experimental Psychology: General</em> 139 (1). American Psychological Association: 32.</p>
</div>
<div id="ref-hribar2016ceo">
<p>Hribar, Paul, and Holly Yang. 2016. “CEO Overconfidence and Management Forecasting.” <em>Contemporary Accounting Research</em> 33 (1). Wiley Online Library: 204–27.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="structure-and-rationale.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sotadocuments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["thesis_source.pdf", "thesis_source.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
