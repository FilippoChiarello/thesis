<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Text Mining Techniques for Knowledge Extraction from Technical Documents</title>
  <meta name="description" content="This document contains the PhD thesis of Filippo Chiarello.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Text Mining Techniques for Knowledge Extraction from Technical Documents" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Text Mining Techniques for Knowledge Extraction from Technical Documents" />
  
  <meta name="twitter:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

<meta name="author" content="Filippo Chiarello">


<meta name="date" content="2018-09-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="methods.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Text Mining Techniques for Knowledge Extraction from Technical Documents</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#goal"><i class="fa fa-check"></i><b>1.1</b> Goal</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#problem"><i class="fa fa-check"></i><b>1.2</b> Problem</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#solutions"><i class="fa fa-check"></i><b>1.3</b> Solutions</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#challenges-understanding-and-programming"><i class="fa fa-check"></i><b>1.4</b> Challenges: Understanding and Programming</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#understanding"><i class="fa fa-check"></i><b>1.4.1</b> Understanding</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#programming"><i class="fa fa-check"></i><b>1.4.2</b> Programming</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#research-questions"><i class="fa fa-check"></i><b>1.5</b> Research Questions</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#stakeholders"><i class="fa fa-check"></i><b>1.6</b> Stakeholders</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#knowledge-intensive-manegement-engineering"><i class="fa fa-check"></i><b>1.7</b> Knowledge Intensive Manegement Engineering</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sota.html"><a href="sota.html"><i class="fa fa-check"></i><b>2</b> State of the Art</a><ul>
<li class="chapter" data-level="2.1" data-path="sota.html"><a href="sota.html#sotatools"><i class="fa fa-check"></i><b>2.1</b> Phases, Tasks, and Techniques</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sota.html"><a href="sota.html#sotatoolsprogram"><i class="fa fa-check"></i><b>2.1.1</b> Program</a></li>
<li class="chapter" data-level="2.1.2" data-path="sota.html"><a href="sota.html#sotatoolsimport"><i class="fa fa-check"></i><b>2.1.2</b> Import</a></li>
<li class="chapter" data-level="2.1.3" data-path="sota.html"><a href="sota.html#sotatoolstidy"><i class="fa fa-check"></i><b>2.1.3</b> Tidy</a></li>
<li class="chapter" data-level="2.1.4" data-path="sota.html"><a href="sota.html#sotatoolstransform"><i class="fa fa-check"></i><b>2.1.4</b> Transform</a></li>
<li class="chapter" data-level="2.1.5" data-path="sota.html"><a href="sota.html#sentence-splitting"><i class="fa fa-check"></i><b>2.1.5</b> Sentence Splitting</a></li>
<li class="chapter" data-level="2.1.6" data-path="sota.html"><a href="sota.html#tokenization"><i class="fa fa-check"></i><b>2.1.6</b> Tokenization</a></li>
<li class="chapter" data-level="2.1.7" data-path="sota.html"><a href="sota.html#sotatoolsmodel"><i class="fa fa-check"></i><b>2.1.7</b> Model</a></li>
<li class="chapter" data-level="2.1.8" data-path="sota.html"><a href="sota.html#sotatoolsvisualize"><i class="fa fa-check"></i><b>2.1.8</b> Visualize</a></li>
<li class="chapter" data-level="2.1.9" data-path="sota.html"><a href="sota.html#sotatoolscomunicate"><i class="fa fa-check"></i><b>2.1.9</b> Comunicate</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="sota.html"><a href="sota.html#sotadocuments"><i class="fa fa-check"></i><b>2.2</b> Documents</a><ul>
<li class="chapter" data-level="2.2.1" data-path="sota.html"><a href="sota.html#sotadocumentsunderstand"><i class="fa fa-check"></i><b>2.2.1</b> Understand</a></li>
<li class="chapter" data-level="2.2.2" data-path="sota.html"><a href="sota.html#sotadocumentspatents"><i class="fa fa-check"></i><b>2.2.2</b> Patents</a></li>
<li class="chapter" data-level="2.2.3" data-path="sota.html"><a href="sota.html#sotadocumentspapers"><i class="fa fa-check"></i><b>2.2.3</b> Papers</a></li>
<li class="chapter" data-level="2.2.4" data-path="sota.html"><a href="sota.html#sotadocumentsprojects"><i class="fa fa-check"></i><b>2.2.4</b> Projects</a></li>
<li class="chapter" data-level="2.2.5" data-path="sota.html"><a href="sota.html#sotadocumentswiki"><i class="fa fa-check"></i><b>2.2.5</b> Wikipedia</a></li>
<li class="chapter" data-level="2.2.6" data-path="sota.html"><a href="sota.html#sotadocumentstwitter"><i class="fa fa-check"></i><b>2.2.6</b> Twitter</a></li>
<li class="chapter" data-level="2.2.7" data-path="sota.html"><a href="sota.html#sotadocumentsjobs"><i class="fa fa-check"></i><b>2.2.7</b> Job Profiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3</b> Methods</a><ul>
<li class="chapter" data-level="3.1" data-path="methods.html"><a href="methods.html#patents"><i class="fa fa-check"></i><b>3.1</b> Patents</a></li>
<li class="chapter" data-level="3.2" data-path="methods.html"><a href="methods.html#papers"><i class="fa fa-check"></i><b>3.2</b> Papers</a></li>
<li class="chapter" data-level="3.3" data-path="methods.html"><a href="methods.html#projects"><i class="fa fa-check"></i><b>3.3</b> Projects</a></li>
<li class="chapter" data-level="3.4" data-path="methods.html"><a href="methods.html#wikipedia"><i class="fa fa-check"></i><b>3.4</b> Wikipedia</a></li>
<li class="chapter" data-level="3.5" data-path="methods.html"><a href="methods.html#twitter"><i class="fa fa-check"></i><b>3.5</b> Twitter</a></li>
<li class="chapter" data-level="3.6" data-path="methods.html"><a href="methods.html#job-profiles"><i class="fa fa-check"></i><b>3.6</b> Job Profiles</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="applications-and-results.html"><a href="applications-and-results.html"><i class="fa fa-check"></i><b>4</b> Applications and Results</a><ul>
<li class="chapter" data-level="4.1" data-path="applications-and-results.html"><a href="applications-and-results.html#patents-1"><i class="fa fa-check"></i><b>4.1</b> Patents</a></li>
<li class="chapter" data-level="4.2" data-path="applications-and-results.html"><a href="applications-and-results.html#papers-1"><i class="fa fa-check"></i><b>4.2</b> Papers</a></li>
<li class="chapter" data-level="4.3" data-path="applications-and-results.html"><a href="applications-and-results.html#projects-1"><i class="fa fa-check"></i><b>4.3</b> Projects</a></li>
<li class="chapter" data-level="4.4" data-path="applications-and-results.html"><a href="applications-and-results.html#wikipedia-1"><i class="fa fa-check"></i><b>4.4</b> Wikipedia</a></li>
<li class="chapter" data-level="4.5" data-path="applications-and-results.html"><a href="applications-and-results.html#twitter-1"><i class="fa fa-check"></i><b>4.5</b> Twitter</a></li>
<li class="chapter" data-level="4.6" data-path="applications-and-results.html"><a href="applications-and-results.html#job-profiles-1"><i class="fa fa-check"></i><b>4.6</b> Job Profiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="future-developments.html"><a href="future-developments.html"><i class="fa fa-check"></i><b>5</b> Future Developments</a><ul>
<li class="chapter" data-level="5.1" data-path="future-developments.html"><a href="future-developments.html#marketing"><i class="fa fa-check"></i><b>5.1</b> Marketing</a></li>
<li class="chapter" data-level="5.2" data-path="future-developments.html"><a href="future-developments.html#research-and-development"><i class="fa fa-check"></i><b>5.2</b> Research and Development</a></li>
<li class="chapter" data-level="5.3" data-path="future-developments.html"><a href="future-developments.html#design"><i class="fa fa-check"></i><b>5.3</b> Design</a></li>
<li class="chapter" data-level="5.4" data-path="future-developments.html"><a href="future-developments.html#human-resources"><i class="fa fa-check"></i><b>5.4</b> Human Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>6</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="7" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i><b>7</b> Glossary</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Text Mining Techniques for Knowledge Extraction from Technical Documents</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sota" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> State of the Art</h1>
<p>The analysis of technical documents require the design of processes that rely both on programming and Natural Language Processing techniques and on the undestanding and knowldege of field experts. While the first techniques are codified and explicit, the second are sometimes implicit and always harder to systematize. In this section i treat these two groups of techniques in the same way to give to the reader a sistematic litterature review on these topics. For this reason the sections of this chapter has the sequent structure:</p>
<ul>
<li>At a first level we have two sections <a href="sota.html#sotatools">2.1</a> and <a href="sota.html#sotadocuments">2.2</a>, reviewing respectivelly the processes of <em>programming and Natural Language Processing</em> and of <em>undestanding and knowldege of field experts application</em>;</li>
<li>Section <a href="sota.html#sotatools">2.1</a> has a subsection for each of the <em>phases</em> showed in figure <a href="#fig:mainworkflow"><strong>??</strong></a>. These subsections goes from <a href="sota.html#sotatoolsprogram">2.1.1</a> to <a href="sota.html#sotatoolscomunicate">2.1.9</a>;</li>
<li>Each subsection from <a href="sota.html#sotatoolsprogram">2.1.1</a> to <a href="sota.html#sotatoolscomunicate">2.1.9</a> contains the relative Natural Language Processing <em>task</em> that are relevant for the analysis of technical documents, for example Document Retrieval <a href="sota.html#sotatoolsimportretrieval">2.1.2.1</a>, Part-Of-Speech-Tagging; <a href="sota.html#sotatoolstransformpos">2.1.6.3</a> or Named Entity Recognition <a href="sota.html#sotatoolsmodelner">2.1.7.5</a>.</li>
<li>Each task subsection describes the relevant <em>techniques</em> to perform that task. I use the word techniques to include mainly algorythms and procedures but also more generic methods or frameworks;</li>
<li>Since the second section <a href="sota.html#sotadocuments">2.2</a> describes less systematics phases, task and techniques this section opens with a first subsection <a href="sota.html#sotadocumentsunderstand">2.2.1</a> that focuses on the studies of the problems of using expert knowledge in an analytical process and which are the techniques to convert this knowledge in a format that is usable in a Natural Language Processing workflow.</li>
<li>Finally, always section <a href="sota.html#sotadocuments">2.2</a> has a subsection for each of the technical <em>documents</em> I analyzed (aggiungi gancio con introduzione). These subsections goes from <a href="sota.html#sotadocumentspatents">2.2.2</a> to <a href="sota.html#sotadocumentsjobs">2.2.7</a>.</li>
</ul>
<div id="sotatools" class="section level2">
<h2><span class="header-section-number">2.1</span> Phases, Tasks, and Techniques</h2>
<p>In this section I make a review of the most important techniques for Natural Language Processing in the context of technical documents analysis. The techniques (mainly algorythms) are grouped in phases (Import, Tidy, Transform, Model, Visualize, Communicate) showed in figure <a href="#fig:mainworkflow"><strong>??</strong></a> and each phases is dived in the NLP tasks that are the most important for the analysis of technical documents. The algorythms i reviewed in this section are summmarised in table tot, where the reader can see the relationship between tasks and techniques.</p>
<div id="sotatoolsprogram" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Program</h3>
<ul>
<li><strong>Articoli Emily</strong></li>
</ul>
</div>
<div id="sotatoolsimport" class="section level3">
<h3><span class="header-section-number">2.1.2</span> Import</h3>
<ul>
<li>I tipi di codifica di testo</li>
<li>Pachetti per import</li>
</ul>
<div id="sotatoolsimportretrieval" class="section level4">
<h4><span class="header-section-number">2.1.2.1</span> Document Retrieval</h4>
<ul>
<li>Letteratura query</li>
</ul>
</div>
</div>
<div id="sotatoolstidy" class="section level3">
<h3><span class="header-section-number">2.1.3</span> Tidy</h3>
<ul>
<li>Hadley</li>
</ul>
<p>weight, tfids</p>
<p>DTM</p>
<p>problems such as sparsity</p>
</div>
<div id="sotatoolstransform" class="section level3">
<h3><span class="header-section-number">2.1.4</span> Transform</h3>
<p>Transforming in the context of Natural Language Processing is what in computational linguistic is called text normalization. Normalizing text means converting it to a more convenient, standard form. Most of the task of technical document analysis in fact relies on first separating out or tokenizing sentences and words, strip suffixes from the end of the word, determining the root of a word or transform the text using regular expressions.</p>
</div>
<div id="sentence-splitting" class="section level3">
<h3><span class="header-section-number">2.1.5</span> Sentence Splitting</h3>
<p>The analysis of technical documents require as first process, that the input text is segmented in sentences. Since documents do not encode this information in a non ambiguous manner (using dots) due to common abbreviations (e.g.: “Mr., Dr.”), a sentence splitting process that does not rely only on a trivial <em>dot based</em> rule is required. This issue in the technical documents domain is even more problematic due to the presence of formulas, numbers, chemical entity names and bibliographic references. Furthermore, since sentece splitting is one of the first processes of an NLP pipeline, errors in this early stage are propagated in the following steps causing a strong decrease for what concerns their accuracy. One of the most advanced techniques are machine learning techniques: given a training corpus of properly segmented sentences and a learning algorithm, a statistical model is built. By reusing the statistical model, the sentence splitter is able to split sentences on texts not used in the training phase. ItalianNLP lab systems uses this approach <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>; Attardi and Dell’Orletta <a href="#ref-attardi2009reverse">2009</a>; Attardi et al. <a href="#ref-attardi2009accurate">2009</a>)</span>. For this reason this algorythm is used for the most of the application presented in this Thesis.</p>
</div>
<div id="tokenization" class="section level3">
<h3><span class="header-section-number">2.1.6</span> Tokenization</h3>
<p>Since documents are unstructured information, these has to be divided into linguistic units. The definition of linguistic units is non-trivial, and more advanced techniques can be used (such as n-gram extraction) but most of the times these are words, punctuation and numbers. English words are often separated from each other by whitespace, but whitespace is not always sufficient. Solving this problems and splitting words in well-defined tokensis defined as tokenization. In most of the application described in the present Thesis, the tokenizer developed by the ItalianNLP lab was integrated <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>; Attardi and Dell’Orletta <a href="#ref-attardi2009reverse">2009</a>; Attardi et al. <a href="#ref-attardi2009accurate">2009</a>)</span>. This tokenizer is regular expression based: each token must match one of the regular expression defined in a configuration file. Among the others, rules are defined to tokenize words, acronyms, numbers, dates and equations.</p>
<div id="sotatoolstransformstemming" class="section level4">
<h4><span class="header-section-number">2.1.6.1</span> Stemming</h4>
<p>Stemming is a simpler but cruder methodology for chopping off of affixes. The goal of stemming is reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem of a word and its morphological root do not need to be identical; it is sufficient that related words map to the same stem, even if this stem is not a valid root. One of the most widely used stemming is the simple and efficient Porter algorithm <span class="citation">(Porter <a href="#ref-porter1980algorithm">1980</a>)</span>.</p>
</div>
<div id="sotatoolstransformlemmatisation" class="section level4">
<h4><span class="header-section-number">2.1.6.2</span> Lemmatisation</h4>
<p>Lemmatization is the task of determining the root of a words. The output allow to find that two words have the same root, despite their surface differences. For example, the verbs <em>am</em>, <em>are</em>, and <em>is</em> have the shared lemma <em>be</em>; the nouns <em>cat</em> and <em>cats</em> both have the lemma <em>cat</em>. Representing a word by its lemma is important for many natural language processing tasks. Lemmatisation in fact diminish the problem of sparsity of document-word matrix. Futhermore lemmatisaion is important for document retrieval <a href="sota.html#sotatoolsimportretrieval">2.1.2.1</a> web search, since we want to find documents mentioning motors if we search for motor. The most recent methods for lemmatization involve complete morphological parsing of the word <span class="citation">(Hankamer <a href="#ref-hankamer1989morphological">1989</a>)</span>.</p>
</div>
<div id="sotatoolstransformpos" class="section level4">
<h4><span class="header-section-number">2.1.6.3</span> Part-of-Speech Tagging</h4>
<p>The part of speech plays an central role in technical document analysis since it provides very useful information concerning the morphological role of a word and its morphosyntactic context: for example, if a token is a determiner, the next token is a noun or an adjective with very high confidence. Part of speech tags are used for many information extraction tools such as named entity taggers (see section <a href="sota.html#sotatoolsmodelner">2.1.7.5</a>) in order to identify named entities. In typical named entity task these are people and locations since tokens representing named entities follow common morphological patterns (e.g. they start with a capital letter). For the application to technical documents, technical entities (like the possibile failures of a manufact) becomes more relevant. In this context a correct part-of-speech tagger becomes even more important since we can not rely on morphosyntactical rules. In addition part of speech tags can be used to mitigate problems related to polysemy since words often have different meaning with respect to their part of speech (e.g. “track”, “guide”). This information is extremelly valuable in patent analysis, and some patent tailored part-of-speech tagger has been designed (see section <a href="sota.html#sotadocumentspatents">2.2.2</a>). The litterature on pos-tagger is huge, and goes behoind the scope of the present thesis to make a complete review. In most of the application presentend in this work, was employed the ILC postagger <span class="citation">(Attardi <a href="#ref-attardi2006experiments">2006</a>)</span>. This postagger uses a supervised training algorithm: given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus.</p>
</div>
<div id="sotatoolstransformregex" class="section level4">
<h4><span class="header-section-number">2.1.6.4</span> Regular Expressions</h4>
<p>Regular expression (regex) is a language for specifying text search strings, an algebraic notation for characterizing a set of strings. This language whidelly used in modern word processor and text processing tools.. They are particularly useful for searching in texts, when we have a pattern to search for.</p>
<p>A pattern could be at A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression. A search can be designed to return every match on a line, if there are more than one, or just the first match. In the following examples we generally underline the exact part of the pattern that matches the regular expression and show only the first match. We’ll show regular expressions delimited by slashes but note that slashes are not part of the regular expressions.</p>
</div>
</div>
<div id="sotatoolsmodel" class="section level3">
<h3><span class="header-section-number">2.1.7</span> Model</h3>
<p>Classi di modelli. Pedro Domingos</p>
<p><span class="citation">(James et al. <a href="#ref-james2013introduction">2013</a>)</span></p>
<div id="sotatoolstransformngrams" class="section level4">
<h4><span class="header-section-number">2.1.7.1</span> N-Grams</h4>
<p>An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like “credit card”, “3d printing”, or ”printing machine”, and a 3-gram (or trigram) is a three-word sequence of words like “3d printing machine”. Statistical model can be used to extract the n-grams contained in a document. A first approach has the of predicting the next item in a sequence in the form of a (n − 1)–order Markov model<span class="citation">(Lafferty and Zhai <a href="#ref-lafferty2001document">2001</a>)</span>. The algorythm begin with the task of computing P(w|h), the probability of a word w given a word h.The way to extimate this probability is using relative frequency counts. To do that the algorythms count the number of times h is followed by the w. With a large enough corpus it is possibile to build valuable models, able to extract n-grams <span class="citation">(Bellegarda <a href="#ref-bellegarda2004statistical">2004</a>)</span>. While this method of estimating probabilities directly from counts works for many natural language applications, in many cases a huge dimension of the corpus does make the model useful, and this is particularly true for technical documents <span class="citation">(Brants et al. <a href="#ref-brants2012large">2012</a>)</span>. This is because technical language has a strong ratio of evolution; as new artifcat are invented, new chunks are created all the time, and has no sense to continuolly count every word co-occurrence to update our model<span class="citation">(K. R. Gibson, Gibson, and Ingold <a href="#ref-gibson1994tools">1994</a>)</span>. A more usefull method for chunk extraction fro technical document uses part-of-speech-tagging and regular expression. Once a document is pos-taggerd each word is associated whit a particular part of speech: each sentence is rapresented as a sequence of part-of-spech. Once we have this rappresentation, it is possible to etract only certain sequences of part-of-speeches, the ones that whit an high level of confidence are n-grams.</p>
<p>[TROVA LAVORI SU QUESTO ARGOMENTO]</p>
</div>
<div id="sotatoolsmodeldocclass" class="section level4">
<h4><span class="header-section-number">2.1.7.2</span> Document Classification</h4>
<p>Classification is a general process that has the goal of taking an object, extract features, and assign to the observation one of a set of discrete classes. This process is largerly used for documents <span class="citation">(Borko and Bernick <a href="#ref-borko1963automatic">1963</a>)</span> and there exist many methods for document classification <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>.</p>
<p>Regardless of technological sector, most organizations today are facing the problem of overload of information. When it comes to classify huge ammount of documents or to separate the useful documents from the irrelevant, document classification techniques can reduce the proecess cost and time.</p>
<p>The simplest method for classifying text is to use expert defined rules. These systems are called expert systems or knowledge engineering approach. Expert rule-based systems are programs that consist of rules in the IF form condition THEN action (if condition, then action). Given a series of facts, expert systems, thanks to the rules they are made of, manage to deduce new facts. The expert systems therefore differ from other similar programs, since, by referring to technologies developed according to artificial intelligence, they are always able to exhibit the logical steps that underlie their decisions: a purpose that, for example, is not feasible from the human mind or black box-systems. There are many type of documents for which expert based classifiers constitute a stateof-the-art system, or at least part of it. Anyway, rules can be useless in situations such as: - data change over time - the rules are too many and interrelated</p>
<p>Most systems of documents classification are instead done via supervised learning: we have a data set of input observations, each associated with some correct output (training set). The goal of the algorithm is to build a statical model able to learn how to map from a new observation (test set) to a correct output. The advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains.</p>
<p>In the supervised document classification process, we have a training set of N documents that have each been tipically hand-labeled with a class: (d1, c1),….,(dN, cN). I say tipically, because other less expensive methods could be designed, as we will show for the task of Named Entity Recongition (another supervised learning task, that classifies words intstead of documents <a href="sota.html#sotatoolsmodelner">2.1.7.5</a>). The goal of the supervised document classification task is to learn a statistical model capable of assign a new document d to its correct class c ∈ C. There exist a class of these classifier, probabilistic classifiers, that additionally will tell us the probability of the observation being in the class.</p>
<p>Many kinds of machine learning algorithms are used to build classifiers <span class="citation">(Aggarwal and Zhai <a href="#ref-aggarwal2012survey">2012</a>)</span>, such as:</p>
<ul>
<li><p><em>Decision Tree Classifiers</em>: Decision tree documents classifier are systems that has as output a classification tree <span class="citation">(Sebastiani <a href="#ref-sebastiani2002machine">2002</a>)</span>. In this tree internal nodes are terms contained in the corpus under analysis, branches departing are labeled by the weight (see section <a href="sota.html#sotatoolstidy">2.1.3</a>) that the term has in the test document, and leafs are labeled by categories. There exists many methods to automatically learn trees from data. A tree can be build by splitting the data source into subsets based on an test feature. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions.</p></li>
<li><p><em>Rule Based Classifiers</em>: Rule-based classifiers are systems in wich the patterns which are most likely to be related to the different classes are extracted from a set of test documents. The set of rules corresponds to the lefthand side to a word pattern, and the right-hand side to a class label. These rules are used for the purposes of classification. In its most general form, the left hand side of the rule is a boolean condition, which is expressed in Disjunctive Normal Form (DNF). However, in most cases, the condition on the left hand side is much simpler and represents a set of terms, all of which must be present in the document for the condition to be satisfied <span class="citation">(Yang, Li, and Wang <a href="#ref-yang2004building">2004</a>)</span>.</p></li>
<li><p><em>Support Vector Machines (SVM) Classifiers</em>: SVM Classifiers attempt to partition the data space with the use of linear or non-linear delineations between the different classes. The main principle of SVM algorythm is to determine separators in the feature space which can best separate the different classes <span class="citation">(Joachims <a href="#ref-joachims1998text">1998</a>,<span class="citation">L. M. Manevitz and Yousef (<a href="#ref-manevitz2001one">2001</a>)</span>)</span>.</p></li>
<li><p><em>Baeysian Classifiers</em>: Bayesian classifiers build a probabilistic classifier based on modeling the underlying word features in different classes. The idea is then to classify documents using the posterior probability of the documents belonging to the different classes on the basis of the word presence in the documents <span class="citation">(Pop <a href="#ref-pop2006approach">2006</a>)</span>.</p></li>
<li><p><em>Neural Netword Classifiers</em>: The basic unit in a neural network is a neuron. Each neuron receives a set of inputs, which are denoted by the vector <em>Xi</em>, which are the values of the feature vector for a certain instance. Each neuron is also associated with a set of weights, which are used in order to compute a function of its inputs. Neural Networks Classifier are able, thank to a process called learning phase, to ajust their weights in such a way that the function is abble to effectively classify new instances. Neural networks are nowdays one of the best method for documents classification, and are used in a wide variety of appliations <span class="citation">(L. Manevitz and Yousef <a href="#ref-manevitz2007one">2007</a>)</span>. Great performances has also been reached by deep neural networks, which are neural networks whit a large number o neurons arranged in multiple layers <span class="citation">(Lai et al. <a href="#ref-lai2015recurrent">2015</a>. <span class="citation">Y. Kim (<a href="#ref-kim2014convolutional">2014</a>)</span>)</span>.</p></li>
</ul>
</div>
<div id="sotatoolsmodelsentanal" class="section level4">
<h4><span class="header-section-number">2.1.7.3</span> Sentiment Analsysis</h4>
<p>Sentiment analysis techniques are algorythms able to measure from text, people’s opinions and emotions toward events, topics, products and their attributes <span class="citation">(Pang, Lee, and others <a href="#ref-pang2008opinion">2008</a>)</span>. For example, businesses (particularly marketeers) are intrested in finding costumers opinions about their products and services.</p>
<p>Thanks to the growth of social media (forums, blogs and social networks), individuals and organizations are producing a huge quantity of their written opinion. This has make it possible to scholars to study this phenomena and to develop many different and effective sentiment analysis techniques <span class="citation">(B. Liu and Zhang <a href="#ref-liu2012survey">2012</a>)</span>. In the past decade, a considerable amount of research has been done by scholars and there are also numerous commercial companies that provide opinion mining services. However, measuring sentiment in documents and distilling the information contained in them remains a challenging task because of the diversity of documents from which is possibile to extract sentiment.</p>
<p>The approaches to perform sentiment analysis are many. Among all, the most intresting for tehcnical documents analysis are:</p>
<ul>
<li><p><em>Dictionary Base Approaches</em> : This approach has the aim of collecting words that are clues for positive or negative sentiment. In litterature these words are called opinion words, opinion-bearing words or sentiment words. Examples of positive opinion words are: good, nice and amazing. Examples of negative opinion words are bad, poor, and terrible. Collectively, they are called the opinion lexicon. The most simple and widely used techniques to produce a dictionary of opinion words is based on bootstrapping using a small set of seed opinion words and an online dictionary such as WordNet <span class="citation">(Miller <a href="#ref-miller1995wordnet">1995</a>)</span>. The works that used this approach <span class="citation">(Hu and Liu <a href="#ref-hu2004mining">2004</a>, <span class="citation">S.-M. Kim and Hovy (<a href="#ref-kim2004determining">2004</a>)</span>)</span>, adopts a process that consist in two phases: first collect set of opinion words manually, then grow this set by searching in the WordNet for their synonyms and antonyms. The process stops when no more new words are found. After that a manual inspection can be carried out to remove and/or correct errors. Scholars has developed several opinion lexicons <span class="citation">(Ding, Liu, and Yu <a href="#ref-ding2008holistic">2008</a>, <span class="citation">Baccianella, Esuli, and Sebastiani (<a href="#ref-baccianella2010sentiwordnet">2010</a>)</span>, <span class="citation">Hu and Liu (<a href="#ref-hu2004mining">2004</a>)</span>, <span class="citation">Philip et al. (<a href="#ref-philip1966general">1966</a>)</span>, <span class="citation">Wiebe, Bruce, and O’Hara (<a href="#ref-wiebe1999development">1999</a>)</span>)</span> The lexicon based approach has the characteristic of beeing strongly context specific. This is an advantage when the goal is to design a method able to extract sentiment in a specific context <span class="citation">(Chiarello et al. <a href="#ref-chiarello2017product">2017</a>)</span>, but is a major shortcoming if the goal is to design a general purpose method.</p></li>
<li><p><em>Supervised Learning Approaches</em>:</p></li>
<li><p><em>Corpus-based Approaches</em></p></li>
</ul>
</div>
<div id="sotatoolsmodelnetanal" class="section level4">
<h4><span class="header-section-number">2.1.7.4</span> Network Analysis</h4>
</div>
<div id="sotatoolsmodelner" class="section level4">
<h4><span class="header-section-number">2.1.7.5</span> Named Entity Recognition</h4>
</div>
<div id="sotatoolsmodeltopicmodel" class="section level4">
<h4><span class="header-section-number">2.1.7.6</span> Topic Modelling</h4>
</div>
</div>
<div id="sotatoolsvisualize" class="section level3">
<h3><span class="header-section-number">2.1.8</span> Visualize</h3>
</div>
<div id="sotatoolscomunicate" class="section level3">
<h3><span class="header-section-number">2.1.9</span> Comunicate</h3>
</div>
</div>
<div id="sotadocuments" class="section level2">
<h2><span class="header-section-number">2.2</span> Documents</h2>
<div id="sotadocumentsunderstand" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Understand</h3>
<p>Expertise (collins)</p>
<p>Sheela Jasanow</p>
<p>Taleb?</p>
<div id="sotadocumentsunderstandbyas" class="section level4">
<h4><span class="header-section-number">2.2.1.1</span> The problem of byases</h4>
<p>Each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs. The average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them. Moreover, it is also known that human analysis of text information is subject to con- siderable biases, e.g., people often pay greater attention to opinions that are consistent with their own preferences. People also have difficulty, owing to their mental and physical limitations, producing consistent results when the amount of information to be processed is large. Automated opinion mining and summarization systems are thus needed, as subjective biases and mental limitations can be overcome with an objective sentiment analysis system.</p>
</div>
<div id="sotadocumentsunderstandlexicons" class="section level4">
<h4><span class="header-section-number">2.2.1.2</span> The Importance of Lexicons for Technical Documents Analysis</h4>
</div>
<div id="expert-systems" class="section level4">
<h4><span class="header-section-number">2.2.1.3</span> Expert Systems</h4>
</div>
</div>
<div id="sotadocumentspatents" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Patents</h3>
</div>
<div id="sotadocumentspapers" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Papers</h3>
<ul>
<li>Parte Barilari.Keyword base, defini i confini di area tecnologica. Hot-topics su paper (guaiè)</li>
<li>Biblio</li>
</ul>
</div>
<div id="sotadocumentsprojects" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Projects</h3>
</div>
<div id="sotadocumentswiki" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Wikipedia</h3>
</div>
<div id="sotadocumentstwitter" class="section level3">
<h3><span class="header-section-number">2.2.6</span> Twitter</h3>
</div>
<div id="sotadocumentsjobs" class="section level3">
<h3><span class="header-section-number">2.2.7</span> Job Profiles</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-dell2009ensemble">
<p>Dell’Orletta, Felice. 2009. “Ensemble System for Part-of-Speech Tagging.” <em>Proceedings of EVALITA</em> 9: 1–8.</p>
</div>
<div id="ref-attardi2009reverse">
<p>Attardi, Giuseppe, and Felice Dell’Orletta. 2009. “Reverse Revision and Linear Tree Combination for Dependency Parsing.” In <em>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</em>, 261–64. Association for Computational Linguistics.</p>
</div>
<div id="ref-attardi2009accurate">
<p>Attardi, Giuseppe, Felice Dell’Orletta, Maria Simi, and Joseph Turian. 2009. “Accurate Dependency Parsing with a Stacked Multilayer Perceptron.” <em>Proceedings of EVALITA</em> 9: 1–8.</p>
</div>
<div id="ref-porter1980algorithm">
<p>Porter, Martin F. 1980. “An Algorithm for Suffix Stripping.” <em>Program</em> 14 (3). MCB UP Ltd: 130–37.</p>
</div>
<div id="ref-hankamer1989morphological">
<p>Hankamer, Jorge. 1989. “Morphological Parsing and the Lexicon.” In <em>Lexical Representation and Process</em>, 392–408. MIT Press.</p>
</div>
<div id="ref-attardi2006experiments">
<p>Attardi, Giuseppe. 2006. “Experiments with a Multilanguage Non-Projective Dependency Parser.” In <em>Proceedings of the Tenth Conference on Computational Natural Language Learning</em>, 166–70. Association for Computational Linguistics.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.</p>
</div>
<div id="ref-lafferty2001document">
<p>Lafferty, John, and Chengxiang Zhai. 2001. “Document Language Models, Query Models, and Risk Minimization for Information Retrieval.” In <em>Proceedings of the 24th Annual International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 111–19. ACM.</p>
</div>
<div id="ref-bellegarda2004statistical">
<p>Bellegarda, Jerome R. 2004. “Statistical Language Model Adaptation: Review and Perspectives.” <em>Speech Communication</em> 42 (1). Elsevier: 93–108.</p>
</div>
<div id="ref-brants2012large">
<p>Brants, Thorsten, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2012. “Large Language Models in Machine Translation.” Google Patents.</p>
</div>
<div id="ref-gibson1994tools">
<p>Gibson, Kathleen R, Kathleen Rita Gibson, and Tim Ingold. 1994. <em>Tools, Language and Cognition in Human Evolution</em>. Cambridge University Press.</p>
</div>
<div id="ref-borko1963automatic">
<p>Borko, Harold, and Myrna Bernick. 1963. “Automatic Document Classification.” <em>Journal of the ACM (JACM)</em> 10 (2). ACM: 151–62.</p>
</div>
<div id="ref-aggarwal2012survey">
<p>Aggarwal, Charu C, and ChengXiang Zhai. 2012. “A Survey of Text Classification Algorithms.” In <em>Mining Text Data</em>, 163–222. Springer.</p>
</div>
<div id="ref-sebastiani2002machine">
<p>Sebastiani, Fabrizio. 2002. “Machine Learning in Automated Text Categorization.” <em>ACM Computing Surveys (CSUR)</em> 34 (1). ACM: 1–47.</p>
</div>
<div id="ref-yang2004building">
<p>Yang, Qiang, Tianyi Li, and Ke Wang. 2004. “Building Association-Rule Based Sequential Classifiers for Web-Document Prediction.” <em>Data Mining and Knowledge Discovery</em> 8 (3). Springer: 253–73.</p>
</div>
<div id="ref-joachims1998text">
<p>Joachims, Thorsten. 1998. “Text Categorization with Support Vector Machines: Learning with Many Relevant Features.” In <em>European Conference on Machine Learning</em>, 137–42. Springer.</p>
</div>
<div id="ref-manevitz2001one">
<p>Manevitz, Larry M, and Malik Yousef. 2001. “One-Class Svms for Document Classification.” <em>Journal of Machine Learning Research</em> 2 (Dec): 139–54.</p>
</div>
<div id="ref-pop2006approach">
<p>Pop, Ioan. 2006. “An Approach of the Naive Bayes Classifier for the Document Classification.” <em>General Mathematics</em> 14 (4). University: 135–38.</p>
</div>
<div id="ref-manevitz2007one">
<p>Manevitz, Larry, and Malik Yousef. 2007. “One-Class Document Classification via Neural Networks.” <em>Neurocomputing</em> 70 (7-9). Elsevier: 1466–81.</p>
</div>
<div id="ref-lai2015recurrent">
<p>Lai, Siwei, Liheng Xu, Kang Liu, and Jun Zhao. 2015. “Recurrent Convolutional Neural Networks for Text Classification.” In <em>AAAI</em>, 333:2267–73.</p>
</div>
<div id="ref-kim2014convolutional">
<p>Kim, Yoon. 2014. “Convolutional Neural Networks for Sentence Classification.” <em>arXiv Preprint arXiv:1408.5882</em>.</p>
</div>
<div id="ref-pang2008opinion">
<p>Pang, Bo, Lillian Lee, and others. 2008. “Opinion Mining and Sentiment Analysis.” <em>Foundations and Trends in Information Retrieval</em> 2 (1–2). Now Publishers, Inc.: 1–135.</p>
</div>
<div id="ref-liu2012survey">
<p>Liu, Bing, and Lei Zhang. 2012. “A Survey of Opinion Mining and Sentiment Analysis.” In <em>Mining Text Data</em>, 415–63. Springer.</p>
</div>
<div id="ref-miller1995wordnet">
<p>Miller, George A. 1995. “WordNet: A Lexical Database for English.” <em>Communications of the ACM</em> 38 (11). ACM: 39–41.</p>
</div>
<div id="ref-hu2004mining">
<p>Hu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer Reviews.” In <em>Proceedings of the Tenth Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 168–77. ACM.</p>
</div>
<div id="ref-kim2004determining">
<p>Kim, Soo-Min, and Eduard Hovy. 2004. “Determining the Sentiment of Opinions.” In <em>Proceedings of the 20th International Conference on Computational Linguistics</em>, 1367. Association for Computational Linguistics.</p>
</div>
<div id="ref-ding2008holistic">
<p>Ding, Xiaowen, Bing Liu, and Philip S Yu. 2008. “A Holistic Lexicon-Based Approach to Opinion Mining.” In <em>Proceedings of the 2008 International Conference on Web Search and Data Mining</em>, 231–40. ACM.</p>
</div>
<div id="ref-baccianella2010sentiwordnet">
<p>Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. 2010. “Sentiwordnet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.” In <em>Lrec</em>, 10:2200–2204. 2010.</p>
</div>
<div id="ref-philip1966general">
<p>Philip, J, Dexter C Dunphy, Marshall S Smith, Daniel M Ogilvie, and others. 1966. “The General Inquirer: A Computer Approach to Content Analysis.” The MIT Press Cambridge.</p>
</div>
<div id="ref-wiebe1999development">
<p>Wiebe, Janyce M, Rebecca F Bruce, and Thomas P O’Hara. 1999. “Development and Use of a Gold-Standard Data Set for Subjectivity Classifications.” In <em>Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics</em>, 246–53. Association for Computational Linguistics.</p>
</div>
<div id="ref-chiarello2017product">
<p>Chiarello, Filippo, Gualtiero Fantoni, Andrea Bonaccorsi, and others. 2017. “Product Description in Terms of Advantages and Drawbacks: Exploiting Patent Information in Novel Ways.” In <em>DS 87-6 Proceedings of the 21st International Conference on Engineering Design (Iced 17) Vol 6: Design Information and Knowledge, Vancouver, Canada, 21-25.08. 2017</em>, 101–10.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["thesis_source.pdf", "thesis_source.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
