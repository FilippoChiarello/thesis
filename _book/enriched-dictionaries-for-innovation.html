<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="This document contains the PhD thesis of Filippo Chiarello.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  <meta name="twitter:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="explpatentnovel.html">
<link rel="next" href="impactresuser.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Mining Technical Knowledge</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="scope-of-the-thesis.html"><a href="scope-of-the-thesis.html"><i class="fa fa-check"></i><b>1</b> Scope of the Thesis</a><ul>
<li class="chapter" data-level="1.1" data-path="scope-of-the-thesis.html"><a href="scope-of-the-thesis.html#sources-of-information"><i class="fa fa-check"></i><b>1.1</b> Sources of Information</a></li>
<li class="chapter" data-level="1.2" data-path="scope-of-the-thesis.html"><a href="scope-of-the-thesis.html#methods-and-focus-of-the-analysis"><i class="fa fa-check"></i><b>1.2</b> Methods and Focus of the Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="approach.html"><a href="approach.html"><i class="fa fa-check"></i><b>2</b> Approach</a><ul>
<li class="chapter" data-level="2.1" data-path="approach.html"><a href="approach.html#introdesres"><i class="fa fa-check"></i><b>2.1</b> Design Science Paradigm</a></li>
<li class="chapter" data-level="2.2" data-path="approach.html"><a href="approach.html#data-science-workflow"><i class="fa fa-check"></i><b>2.2</b> Data Science Workflow</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="structure-and-rationale.html"><a href="structure-and-rationale.html"><i class="fa fa-check"></i>Structure and Rationale</a></li>
<li class="part"><span><b>II State of the Art</b></span></li>
<li class="chapter" data-level="3" data-path="sotatools.html"><a href="sotatools.html"><i class="fa fa-check"></i><b>3</b> Phases, Tasks, and Techniques</a><ul>
<li class="chapter" data-level="3.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolsprogram"><i class="fa fa-check"></i><b>3.1</b> Program</a></li>
<li class="chapter" data-level="3.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimport"><i class="fa fa-check"></i><b>3.2</b> Import</a><ul>
<li class="chapter" data-level="3.2.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimportretrieval"><i class="fa fa-check"></i><b>3.2.1</b> Document Retrieval</a></li>
<li class="chapter" data-level="3.2.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsimportformat"><i class="fa fa-check"></i><b>3.2.2</b> Documents Format</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolstidy"><i class="fa fa-check"></i><b>3.3</b> Tidy</a></li>
<li class="chapter" data-level="3.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransform"><i class="fa fa-check"></i><b>3.4</b> Transform</a><ul>
<li class="chapter" data-level="3.4.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformsentencesplit"><i class="fa fa-check"></i><b>3.4.1</b> Sentence Splitting</a></li>
<li class="chapter" data-level="3.4.2" data-path="sotatools.html"><a href="sotatools.html#tokenization"><i class="fa fa-check"></i><b>3.4.2</b> Tokenization</a></li>
<li class="chapter" data-level="3.4.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformstemming"><i class="fa fa-check"></i><b>3.4.3</b> Stemming</a></li>
<li class="chapter" data-level="3.4.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformlemmatisation"><i class="fa fa-check"></i><b>3.4.4</b> Lemmatisation</a></li>
<li class="chapter" data-level="3.4.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformwi"><i class="fa fa-check"></i><b>3.4.5</b> Words importance metrics</a></li>
<li class="chapter" data-level="3.4.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformpos"><i class="fa fa-check"></i><b>3.4.6</b> Part-of-Speech Tagging</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodel"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sotatools.html"><a href="sotatools.html#sotatoolstransformngrams"><i class="fa fa-check"></i><b>3.5.1</b> N-Grams</a></li>
<li class="chapter" data-level="3.5.2" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodeldocclass"><i class="fa fa-check"></i><b>3.5.2</b> Document Classification</a></li>
<li class="chapter" data-level="3.5.3" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelsentanal"><i class="fa fa-check"></i><b>3.5.3</b> Sentiment Analysis</a></li>
<li class="chapter" data-level="3.5.4" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelnetanal"><i class="fa fa-check"></i><b>3.5.4</b> Text Clustering</a></li>
<li class="chapter" data-level="3.5.5" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodelner"><i class="fa fa-check"></i><b>3.5.5</b> Named Entity Recognition</a></li>
<li class="chapter" data-level="3.5.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolsmodeltopicmodel"><i class="fa fa-check"></i><b>3.5.6</b> Topic Modelling</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="sotatools.html"><a href="sotatools.html#sotatoolsvisualize"><i class="fa fa-check"></i><b>3.6</b> Visualize</a><ul>
<li class="chapter" data-level="3.6.1" data-path="sotatools.html"><a href="sotatools.html#the-grammar-of-graphics"><i class="fa fa-check"></i><b>3.6.1</b> The Grammar of Graphics</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="sotatools.html"><a href="sotatools.html#sotatoolscomunicate"><i class="fa fa-check"></i><b>3.7</b> Comunicate</a></li>
<li class="chapter" data-level="3.8" data-path="sotatools.html"><a href="sotatools.html#sotadocumentsunderstand"><i class="fa fa-check"></i><b>3.8</b> Understand</a><ul>
<li class="chapter" data-level="3.8.1" data-path="sotatools.html"><a href="sotatools.html#sotadocumentsunderstandbyas"><i class="fa fa-check"></i><b>3.8.1</b> The problem of biases</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sotadocuments.html"><a href="sotadocuments.html"><i class="fa fa-check"></i><b>4</b> Documents</a><ul>
<li class="chapter" data-level="4.1" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentspatents"><i class="fa fa-check"></i><b>4.1</b> Patents</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sotadocuments.html"><a href="sotadocuments.html#metadata-approaches"><i class="fa fa-check"></i><b>4.1.1</b> Metadata Approaches</a></li>
<li class="chapter" data-level="4.1.2" data-path="sotadocuments.html"><a href="sotadocuments.html#keywords-approaches"><i class="fa fa-check"></i><b>4.1.2</b> Keywords Approaches</a></li>
<li class="chapter" data-level="4.1.3" data-path="sotadocuments.html"><a href="sotadocuments.html#natural-language-processing-approaches"><i class="fa fa-check"></i><b>4.1.3</b> Natural Language Processing approaches</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentspapers"><i class="fa fa-check"></i><b>4.2</b> Papers</a></li>
<li class="chapter" data-level="4.3" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentswiki"><i class="fa fa-check"></i><b>4.3</b> Wikipedia</a></li>
<li class="chapter" data-level="4.4" data-path="sotadocuments.html"><a href="sotadocuments.html#sotadocumentstwitter"><i class="fa fa-check"></i><b>4.4</b> Social Media</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sotadocuments.html"><a href="sotadocuments.html#economics"><i class="fa fa-check"></i><b>4.4.1</b> Economics</a></li>
<li class="chapter" data-level="4.4.2" data-path="sotadocuments.html"><a href="sotadocuments.html#marketing"><i class="fa fa-check"></i><b>4.4.2</b> Marketing</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Methods and Results</b></span></li>
<li class="chapter" data-level="5" data-path="patents.html"><a href="patents.html"><i class="fa fa-check"></i><b>5</b> Patents</a><ul>
<li class="chapter" data-level="5.1" data-path="patents.html"><a href="patents.html#usersresults"><i class="fa fa-check"></i><b>5.1</b> Users</a><ul>
<li class="chapter" data-level="5.1.1" data-path="patents.html"><a href="patents.html#method"><i class="fa fa-check"></i><b>5.1.1</b> Method</a></li>
<li class="chapter" data-level="5.1.2" data-path="patents.html"><a href="patents.html#results"><i class="fa fa-check"></i><b>5.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="patents.html"><a href="patents.html#advdrwresults"><i class="fa fa-check"></i><b>5.2</b> Advantages and Drawbacks</a><ul>
<li class="chapter" data-level="5.2.1" data-path="patents.html"><a href="patents.html#methodology"><i class="fa fa-check"></i><b>5.2.1</b> Methodology</a></li>
<li class="chapter" data-level="5.2.2" data-path="patents.html"><a href="patents.html#results-1"><i class="fa fa-check"></i><b>5.2.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="patents.html"><a href="patents.html#trademakrs"><i class="fa fa-check"></i><b>5.3</b> Trademakrs</a><ul>
<li class="chapter" data-level="5.3.1" data-path="patents.html"><a href="patents.html#methodology-1"><i class="fa fa-check"></i><b>5.3.1</b> Methodology</a></li>
<li class="chapter" data-level="5.3.2" data-path="patents.html"><a href="patents.html#results-2"><i class="fa fa-check"></i><b>5.3.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="papers.html"><a href="papers.html"><i class="fa fa-check"></i><b>6</b> Papers</a><ul>
<li class="chapter" data-level="6.1" data-path="papers.html"><a href="papers.html#smtextdrivenbottomup"><i class="fa fa-check"></i><b>6.1</b> Sustainable Manufacturing: the 6R Framework</a><ul>
<li class="chapter" data-level="6.1.1" data-path="papers.html"><a href="papers.html#methodology-2"><i class="fa fa-check"></i><b>6.1.1</b> Methodology</a></li>
<li class="chapter" data-level="6.1.2" data-path="papers.html"><a href="papers.html#results-3"><i class="fa fa-check"></i><b>6.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="papers.html"><a href="papers.html#sustainable-manufacturing-an-extended-mapping"><i class="fa fa-check"></i><b>6.2</b> Sustainable Manufacturing: An Extended Mapping</a><ul>
<li class="chapter" data-level="6.2.1" data-path="papers.html"><a href="papers.html#methodology-3"><i class="fa fa-check"></i><b>6.2.1</b> Methodology</a></li>
<li class="chapter" data-level="6.2.2" data-path="papers.html"><a href="papers.html#results-4"><i class="fa fa-check"></i><b>6.2.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="papers.html"><a href="papers.html#blockchainanalysis"><i class="fa fa-check"></i><b>6.3</b> Blockchain</a><ul>
<li class="chapter" data-level="6.3.1" data-path="papers.html"><a href="papers.html#methodology-4"><i class="fa fa-check"></i><b>6.3.1</b> Methodology</a></li>
<li class="chapter" data-level="6.3.2" data-path="papers.html"><a href="papers.html#results-5"><i class="fa fa-check"></i><b>6.3.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="papers.html"><a href="papers.html#precisionagri"><i class="fa fa-check"></i><b>6.4</b> Precision Agriculture</a><ul>
<li class="chapter" data-level="6.4.1" data-path="papers.html"><a href="papers.html#methodology-5"><i class="fa fa-check"></i><b>6.4.1</b> Methodology</a></li>
<li class="chapter" data-level="6.4.2" data-path="papers.html"><a href="papers.html#results-6"><i class="fa fa-check"></i><b>6.4.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="wikipedia.html"><a href="wikipedia.html"><i class="fa fa-check"></i><b>7</b> Wikipedia</a><ul>
<li class="chapter" data-level="7.1" data-path="wikipedia.html"><a href="wikipedia.html#technimetrochap"><i class="fa fa-check"></i><b>7.1</b> Industry 4.0: Extracting and Mapping Technologies</a><ul>
<li class="chapter" data-level="7.1.1" data-path="wikipedia.html"><a href="wikipedia.html#methodology-6"><i class="fa fa-check"></i><b>7.1.1</b> Methodology</a></li>
<li class="chapter" data-level="7.1.2" data-path="wikipedia.html"><a href="wikipedia.html#results-7"><i class="fa fa-check"></i><b>7.1.2</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="wikipedia.html"><a href="wikipedia.html#industry-4.0-a-comparison-with-industrie-4.0"><i class="fa fa-check"></i><b>7.2</b> Industry 4.0: a Comparison with Industrie 4.0</a><ul>
<li class="chapter" data-level="7.2.1" data-path="wikipedia.html"><a href="wikipedia.html#methodology-7"><i class="fa fa-check"></i><b>7.2.1</b> Methodology</a></li>
<li class="chapter" data-level="7.2.2" data-path="wikipedia.html"><a href="wikipedia.html#results-8"><i class="fa fa-check"></i><b>7.2.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="social-media.html"><a href="social-media.html"><i class="fa fa-check"></i><b>8</b> Social Media</a><ul>
<li class="chapter" data-level="8.1" data-path="social-media.html"><a href="social-media.html#techsentanal"><i class="fa fa-check"></i><b>8.1</b> Technical Sentiment Analysis</a><ul>
<li class="chapter" data-level="8.1.1" data-path="social-media.html"><a href="social-media.html#methodology-8"><i class="fa fa-check"></i><b>8.1.1</b> Methodology</a></li>
<li class="chapter" data-level="8.1.2" data-path="social-media.html"><a href="social-media.html#results-9"><i class="fa fa-check"></i><b>8.1.2</b> Results</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Applications of the Results</b></span></li>
<li class="chapter" data-level="9" data-path="explpatentnovel.html"><a href="explpatentnovel.html"><i class="fa fa-check"></i><b>9</b> A Novel Rapresentation of Patents</a><ul>
<li class="chapter" data-level="9.1" data-path="explpatentnovel.html"><a href="explpatentnovel.html#towards-formal-definitions-of-advantages-and-drawbacks"><i class="fa fa-check"></i><b>9.1</b> Towards Formal Definitions of Advantages and Drawbacks</a></li>
<li class="chapter" data-level="9.2" data-path="explpatentnovel.html"><a href="explpatentnovel.html#methodology-9"><i class="fa fa-check"></i><b>9.2</b> Methodology</a><ul>
<li class="chapter" data-level="9.2.1" data-path="explpatentnovel.html"><a href="explpatentnovel.html#advantages-and-drawbacks-extraction"><i class="fa fa-check"></i><b>9.2.1</b> Advantages and drawbacks extraction</a></li>
<li class="chapter" data-level="9.2.2" data-path="explpatentnovel.html"><a href="explpatentnovel.html#advantages-and-drawbacks-adio-classification"><i class="fa fa-check"></i><b>9.2.2</b> Advantages and Drawbacks ADIO classification</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="explpatentnovel.html"><a href="explpatentnovel.html#results-10"><i class="fa fa-check"></i><b>9.3</b> Results</a><ul>
<li class="chapter" data-level="9.3.1" data-path="explpatentnovel.html"><a href="explpatentnovel.html#patent-set"><i class="fa fa-check"></i><b>9.3.1</b> Patent set</a></li>
<li class="chapter" data-level="9.3.2" data-path="explpatentnovel.html"><a href="explpatentnovel.html#extraction-of-advantages-and-drawbacks"><i class="fa fa-check"></i><b>9.3.2</b> Extraction of Advantages and Drawbacks</a></li>
<li class="chapter" data-level="9.3.3" data-path="explpatentnovel.html"><a href="explpatentnovel.html#a-d-i-o-representation"><i class="fa fa-check"></i><b>9.3.3</b> A-D-I-O Representation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="explpatentnovel.html"><a href="explpatentnovel.html#discussion"><i class="fa fa-check"></i><b>9.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html"><i class="fa fa-check"></i><b>10</b> Enriched dictionaries for Innovation</a><ul>
<li class="chapter" data-level="10.1" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#an-overview-of-dictionaries-for-technology-intelligence"><i class="fa fa-check"></i><b>10.1</b> An overview of dictionaries for technology intelligence</a><ul>
<li class="chapter" data-level="10.1.1" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#publicly-available-dictionaries"><i class="fa fa-check"></i><b>10.1.1</b> Publicly available dictionaries</a></li>
<li class="chapter" data-level="10.1.2" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#research-based-dictionaries"><i class="fa fa-check"></i><b>10.1.2</b> Research-based dictionaries</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#the-value-added-of-enriched-dictionaries"><i class="fa fa-check"></i><b>10.2</b> The value added of enriched dictionaries</a></li>
<li class="chapter" data-level="10.3" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#methodology-10"><i class="fa fa-check"></i><b>10.3</b> Methodology</a></li>
<li class="chapter" data-level="10.4" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#results-11"><i class="fa fa-check"></i><b>10.4</b> Results</a></li>
<li class="chapter" data-level="10.5" data-path="enriched-dictionaries-for-innovation.html"><a href="enriched-dictionaries-for-innovation.html#discussions"><i class="fa fa-check"></i><b>10.5</b> Discussions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="impactresuser.html"><a href="impactresuser.html"><i class="fa fa-check"></i><b>11</b> Impact of Research from the Perspective of Users</a><ul>
<li class="chapter" data-level="11.1" data-path="impactresuser.html"><a href="impactresuser.html#methodological-challenges"><i class="fa fa-check"></i><b>11.1</b> Methodological challenges</a><ul>
<li class="chapter" data-level="11.1.1" data-path="impactresuser.html"><a href="impactresuser.html#variability-in-the-identification-of-outcomes-and-users"><i class="fa fa-check"></i><b>11.1.1</b> Variability in the identification of outcomes and users</a></li>
<li class="chapter" data-level="11.1.2" data-path="impactresuser.html"><a href="impactresuser.html#sources-of-information-1"><i class="fa fa-check"></i><b>11.1.2</b> Sources of information</a></li>
<li class="chapter" data-level="11.1.3" data-path="impactresuser.html"><a href="impactresuser.html#text-based-impact-assessment"><i class="fa fa-check"></i><b>11.1.3</b> Text-based impact assessment</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="impactresuser.html"><a href="impactresuser.html#methodology-11"><i class="fa fa-check"></i><b>11.2</b> Methodology</a><ul>
<li class="chapter" data-level="11.2.1" data-path="impactresuser.html"><a href="impactresuser.html#operationalizing-user-groups-using-natural-language-processing-techniques"><i class="fa fa-check"></i><b>11.2.1</b> Operationalizing user groups using Natural Language Processing techniques</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="impactresuser.html"><a href="impactresuser.html#from-text-extraction-to-indicators"><i class="fa fa-check"></i><b>11.3</b> From text extraction to indicators</a><ul>
<li class="chapter" data-level="" data-path="impactresuser.html"><a href="impactresuser.html#frequency"><i class="fa fa-check"></i>Frequency</a></li>
<li class="chapter" data-level="" data-path="impactresuser.html"><a href="impactresuser.html#diversity"><i class="fa fa-check"></i>Diversity</a></li>
<li class="chapter" data-level="" data-path="impactresuser.html"><a href="impactresuser.html#specificity"><i class="fa fa-check"></i>Specificity</a></li>
<li class="chapter" data-level="11.3.1" data-path="impactresuser.html"><a href="impactresuser.html#the-meaning-of-frequency-diversity-and-specificity-indicators-for-the-analysis-of-research-impact"><i class="fa fa-check"></i><b>11.3.1</b> The meaning of Frequency, Diversity and Specificity indicators for the analysis of research impact</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="impactresuser.html"><a href="impactresuser.html#data"><i class="fa fa-check"></i><b>11.4</b> Data</a><ul>
<li class="chapter" data-level="11.4.1" data-path="impactresuser.html"><a href="impactresuser.html#description-of-the-corpus"><i class="fa fa-check"></i><b>11.4.1</b> Description of the corpus</a></li>
<li class="chapter" data-level="11.4.2" data-path="impactresuser.html"><a href="impactresuser.html#preliminary-analysis-of-the-corpus"><i class="fa fa-check"></i><b>11.4.2</b> Preliminary analysis of the corpus</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="impactresuser.html"><a href="impactresuser.html#results-12"><i class="fa fa-check"></i><b>11.5</b> Results</a><ul>
<li class="chapter" data-level="11.5.1" data-path="impactresuser.html"><a href="impactresuser.html#descriptive-analysis"><i class="fa fa-check"></i><b>11.5.1</b> Descriptive analysis</a></li>
<li class="chapter" data-level="11.5.2" data-path="impactresuser.html"><a href="impactresuser.html#findings-by-subject-area"><i class="fa fa-check"></i><b>11.5.2</b> Findings by subject area</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="impactresuser.html"><a href="impactresuser.html#discussion-1"><i class="fa fa-check"></i><b>11.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html"><i class="fa fa-check"></i><b>12</b> Defining Industry 4.0 Professional Archetypes</a><ul>
<li class="chapter" data-level="12.1" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#digital-competences-development"><i class="fa fa-check"></i><b>12.1</b> Digital Competences Development</a></li>
<li class="chapter" data-level="12.2" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#methodology-12"><i class="fa fa-check"></i><b>12.2</b> Methodology</a></li>
<li class="chapter" data-level="12.3" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#the-archetypes"><i class="fa fa-check"></i><b>12.3</b> The Archetypes</a></li>
<li class="chapter" data-level="12.4" data-path="defining-industry-4-0-professional-archetypes.html"><a href="defining-industry-4-0-professional-archetypes.html#discussions-1"><i class="fa fa-check"></i><b>12.4</b> Discussions</a></li>
</ul></li>
<li class="part"><span><b>V Conclusions</b></span></li>
<li class="chapter" data-level="13" data-path="sotadocumentsunderstandlexicons.html"><a href="sotadocumentsunderstandlexicons.html"><i class="fa fa-check"></i><b>13</b> Lexicons Design for Mining Technical Knowledge</a><ul>
<li class="chapter" data-level="13.1" data-path="sotadocumentsunderstandlexicons.html"><a href="sotadocumentsunderstandlexicons.html#the-knowledge-bases-and-technical-lexicons"><i class="fa fa-check"></i><b>13.1</b> The Knowledge Bases and Technical Lexicons</a></li>
<li class="chapter" data-level="13.2" data-path="sotadocumentsunderstandlexicons.html"><a href="sotadocumentsunderstandlexicons.html#the-history-of-lexicons-design"><i class="fa fa-check"></i><b>13.2</b> The history of Lexicons Design</a></li>
<li class="chapter" data-level="13.3" data-path="sotadocumentsunderstandlexicons.html"><a href="sotadocumentsunderstandlexicons.html#a-map-for-lexicons-design"><i class="fa fa-check"></i><b>13.3</b> A Map for Lexicons Design</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="future-developments.html"><a href="future-developments.html"><i class="fa fa-check"></i><b>14</b> Future Developments</a></li>
<li class="chapter" data-level="" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i>Glossary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="enriched-dictionaries-for-innovation" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Enriched dictionaries for Innovation</h1>
<p>The demand for intelligence and foresight of technologies is increasing due to the need of companies and governments to make sense of the rapidly changing technology landscape and to make better decisions. In particular, emerging technologies exhibit not only rapid growth, but also strong conditions of technology and market uncertainty, so that traditional techniques of technology intelligence are challenged <span class="citation">(Rotolo, Hicks, and Martin <a href="#ref-rotolo2015emerging">2015</a>)</span>.</p>
<p>Technology intelligence makes large use of a statistical procedure called clustering. This multivariate technique is commonly used to place entities into relatively homogeneous groups, maximizing the difference with other groups, when entities are not subject to an existing classification. In technology intelligence this is the most interesting situation: if technologies were already fully classified, then they would already be established or mature.</p>
<p>It is important to remark that clustering, in one way or another, is almost a necessity in technology intelligence. The amount of data available on technologies, even on the last generation of technologies, is so large that a preliminary effort of clustering and profiling is generally considered a preliminary step for the analysis.</p>
<p>In technology intelligence the formation of clusters is generally based on words extracted from relevant documents (scientific publications, patents, technical standards). There are two main approaches to the extraction of words from documents: extracting from the metadata associated to the document, or extracting from the full text of the document. Examples of metadata are authors, affiliations, inventors, assignees, keywords, titles. This approach has generated a large literature that uses metadata in order to extract usable knowledge. Within this literature, a notable stream of studies has extracted usable knowledge from documents by clustering the metadata in order to obtain meaningful structures. This method is associated to the use of keywords, as we will see below.</p>
<p>More recently, a different approach has been introduced, based on the processing of the full text of documents. Following the remarkable advancements in computational linguistics, it has become possible to process the entire text of publications, patents, or technical standards to a large scale. Here the clustering exercise does not take place on metadata, but on words, or their combinations, included in the text of documents. The topic modeling is the most used approach. Topic modeling is a tool to extract structures of text from corpora without the help of external sources of knowledge <span class="citation">(Blei, Ng, and Jordan <a href="#ref-blei2003latent">2003</a>)</span>. Once the words have been extracted from metadata or the full text, a clustering strategy must be designed based on an appropriate definition of similarity.</p>
<p>We then suggest the exploration of a novel approach, one that combines domain knowledge with powerful data science techniques, called <em>enriched dictionaries</em>. These are large and highly structured collections of words, associated to formal definitions and internal linkages, that are produced on the basis of domain knowledge of various kinds. In some cases they are publicly available, in other cases they are the result of dedicated and idiosyncratic research efforts. These dictionaries can be used in order to “filter” the semantic content of the full text of documents according to pre-defined structures, generated within the domain knowledge and validated at the state of the art. They can be used, therefore, within the so called supervised text mining approach. This section has two objectives: introducing the methodology of enriched dictionaries, and showing that it allows the joint use of several, and complementary, perspectives: one based on the abstract engineering principles of technologies (functional view), another on the advantages delivered by the technology (advantage view). These dimensions are kept separate in the literature and clustering exercises do not combine them. We show the power of clustering technologies using these views in a combined way.</p>
<div id="an-overview-of-dictionaries-for-technology-intelligence" class="section level2">
<h2><span class="header-section-number">10.1</span> An overview of dictionaries for technology intelligence</h2>
<div id="publicly-available-dictionaries" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Publicly available dictionaries</h3>
<p>One of the advantages of the dictionary approach is the possibility to utilize publicly available dictionaries, that is, available online with unrestricted, free access. We made systematic use of Wordnet, a large dictionary in English language made available and curated by Princeton University. Wordnet is a standard reference in computational linguistics.</p>
<p>Upon the basic Wordnet dictionary, several specialized or dedicated dictionaries have then been developed, which are also available with unrestricted, free access. We made use of the following dictionaries.</p>
<ul>
<li><p>Artifacts: a collection of terms referring to objects, inventions and tools. The dictionary is one of the Name categories available in the Wordnet-based categorization dictionary from Provalis, free to download.<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a> Artifacts were chosen to explore texts in order to find structural indications for the components of an invention.</p></li>
<li><p>Acts: a collection of terms referring to generic actions. This is another Name category from the Wordnet dictionary. Acts were selected in order to find verbal expressions referable to functions carried out by the objects described therein.</p></li>
</ul>
</div>
<div id="research-based-dictionaries" class="section level3">
<h3><span class="header-section-number">10.1.2</span> Research-based dictionaries</h3>
<p>The dictionary methodology offers the flexibility to build up dedicated or specialized dictionaries, that represent the state of the art of a given knowledge domain. We make use of two dedicated dictionaries recently developed by the authors in an academic research context. Needless to say, in these cases the authors must give the readers or users of the dictionary full proof of the completeness of the dictionary, or the compliance with formal criteria for the definition of a dictionary. These proofs have been offered in a number of papers cited below.</p>
<table>
<caption><span id="tab:dictex1">Table 10.1: </span> Sources and main characteristics of dictionaries.</caption>
<colgroup>
<col width="18%" />
<col width="13%" />
<col width="68%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Dictionary</th>
<th align="right">Numberof entries</th>
<th align="left">Examples of chunks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Artifacts</td>
<td align="right">12374</td>
<td align="left">Acetanilid, Actuator, Aerogenerator, Agglomerator</td>
</tr>
<tr class="even">
<td align="left">Acts</td>
<td align="right">5527</td>
<td align="left">Abduction, Abidance, Accenting, Acquiring</td>
</tr>
<tr class="odd">
<td align="left">Functional Verbs</td>
<td align="right">11256</td>
<td align="left">Abrade, Absorb, Abut, Accelerate</td>
</tr>
<tr class="even">
<td align="left">ADV/DIS</td>
<td align="right">29902</td>
<td align="left">Ability, Accelerate, Acceptance, Accessibility, Accident, Accidents, Aggravated, Aggravation</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Functional verbs: list of verbal expressions describing functions, or actions performed by artifacts on given objects <span class="citation">(Fantoni et al. <a href="#ref-fantoni2013automatic">2013</a>; Apreda et al. <a href="#ref-apreda2016functional">2016</a>)</span>.</p></li>
<li><p>Advantages and Disadvantages: list of terms referring to measurable benefits or drawbacks related to an object, chosen for their capacity to represent applications of an invention (see section <a href="patents.html#advdrwresults">5.2</a>).</p></li>
</ul>
<p>Table <a href="enriched-dictionaries-for-innovation.html#tab:dictex1">10.1</a> offers an overview of the dictionaries used for this analysis, with a few examples of terms.</p>
</div>
</div>
<div id="the-value-added-of-enriched-dictionaries" class="section level2">
<h2><span class="header-section-number">10.2</span> The value added of enriched dictionaries</h2>
<p>In order to illustrate the original contribution of the enriched dictionary approach, it is useful to refer to similar approaches, recently introduced in the literature. The starting point of this literature is similar to the one advocated in this paper, i.e. the need to supervise the full text mining with the help of list of words that reflect domain knowledge.</p>
<p>A natural candidate here is the list of words that describe technical functions, or the abstract characterization of the working principles of technologies <span class="citation">(Cascini, Fantechi, and Spinicci <a href="#ref-cascini2004natural">2004</a>; Dewulf <a href="#ref-dewulf2006directed">2006</a>; Cascini, Russo, and Zini <a href="#ref-cascini2007computer">2007</a>; Cascini and Zini <a href="#ref-cascini2008measuring">2008</a>)</span>. The pioneering approach in this field is TRIZ, the patent-based methodology that has identified a number of abstract inventive principles <span class="citation">(Petrov <a href="#ref-petrov2002laws">2002</a>)</span>, whose application to patent texts has permitted the detection of evolutionary trends in specific technologies <span class="citation">(Verhaegen et al. <a href="#ref-verhaegen2009relating">2009</a>; Wang, Chang, and Kao <a href="#ref-wang2010identifying">2010</a>; J. Yoon and Kim <a href="#ref-yoon2011automated">2011</a><a href="#ref-yoon2011automated">a</a>; Park, Ree, and Kim <a href="#ref-park2013identification">2013</a>)</span>. A variant of this approach combines the functional approach with lists of product attributes or properties <span class="citation">(Yoon and Kim <a href="#ref-yoon2012detecting">2012</a>; Yoon, Choi, and Kim <a href="#ref-yoon2011invention">2011</a>; Kim et al. <a href="#ref-kim2010cause">2010</a>)</span>. More recently, a similar approach has been introduced, suggesting that subject-action-object (SAO) linguistic patterns can be derived automatically from the full text of patents <span class="citation">(J. Yoon and Kim <a href="#ref-yoon2011identifying">2011</a><a href="#ref-yoon2011identifying">b</a>; Choi et al. <a href="#ref-choi2012sao">2012</a>; Park, Yoon, and Kim <a href="#ref-park2011identifying">2011</a><a href="#ref-park2011identifying">b</a>)</span>. A SAO textual sequence is considered an indication of the engineering principle that describes an action that is changing an object. Choi et al <span class="citation">(Choi et al. <a href="#ref-choi2012sao">2012</a>)</span> reviews the state of the art of function-based technology databases (in particular, TRIZ and Creax) but suggest that SAO structures are more versatile and flexible in order to apply Natural Language Processing techniques.</p>
<p>Within this line of investigation the notion of technology tree (Tech Tree) has been introduced <span class="citation">(Choi et al. <a href="#ref-choi2012sao">2012</a>)</span>. It combines within a single representation taxonomies of products, technologies, and abstract functions. Similarity matrices are built along these dimensions and aggregated.</p>
<p>While these two approaches have generated a large and interesting literature, their main limitation is the lack of transparency. The queries obtained from the TRIZ inventive principles have not been published and there is no demonstration of their reliability (i.e. replicability under controlled conditions) with respect to all semantic variations of words. Similarly, the SAO queries, although intuitively clear, leave the readers and users with the problem of establishing the semantic content in the engineering sense, given that the notion of “action” may cover, in fact, many different meanings.</p>
</div>
<div id="methodology-10" class="section level2">
<h2><span class="header-section-number">10.3</span> Methodology</h2>
<p>The cited studies have suggested that a mixed approach to the clustering problem could be the best solution to achieve good results. In particular there is a need for placing more domain knowledge into the analysis, while keeping the enormous advantages of text mining and automatic knowledge representation techniques.</p>
<p>This is the starting point of the dictionary approach we propose. It suggests that the full text of technical documents is searched by using a structured list of words, generated from a substantive body of domain knowledge, formally defined, and organized in a hierarchical way.</p>
<p>A dictionary, in substance, is a collection of terms pertaining to a precise context and then selected following a logical pattern. The content of a dictionary may sensibly vary from case to case, comprising terms related to a precise technology, a technical field, a social group, or more generic parts-of-speech such as particular kinds of actions or nouns, and so on.</p>
<p>The main difference between a dictionary and a mere collection of keywords is in their respective scope, which is, for a dictionary, wider and far more complete, including all the synonyms, hypernyms and hyponyms of a term. By definition, a dictionary must include all the definitions and terms referring to the chosen context. The completeness has an important consequence for text mining applications. When a dictionary is used as a tool to filter the content in a collection of texts, the query is formed not only by principal words, but also by secondary words, such as synonyms. These secondary words would be lost if the analysis were based on words and keywords. Therefore dictionaries contribute to overcoming the biases introduced in the analysis by the subjective choice of keywords by experts.</p>
<p>To start with, the dictionary method was combined with topic modelling techniques in order to overcome one of the main limitations of topic modeling, that is, the extraction of irrelevant topics.</p>
<p>The starting assumption is the possibility of representing a document using the main topics cited in it. A topic is, basically, a collection of terms that delineate and describe an argument. Topics may be identified by using unsupervised or supervised methods. The unsupervised application of the method will have the result of giving generic topics, often irrelevant in specialized analysis. This outcome can be mitigated, but not eliminated, by the application of ranking techniques, such as Term Frequency-Inverse Document Frequency (TF-IDF), that allow the identification of coherent topics. Alternatively, analysts may refer to supervised methods, which are however resource-consuming in their design and implementation. The dictionary approach offers an excellent solution to the trade-off between relevance and precision of the search, which plays in favor of supervised methods, and the parsimony of the analysis, which on the contrary militates for the adoption of unsupervised methods.</p>
<p>In addition, if topics are clearly delineated, it becomes possible to identify similarities between documents that refer to technologies with a transversal, or cross-domain, potential for application. Dictionaries can be used for different purposes and help to identify latent structures from a variety of perspectives. We advocate the use of several dictionaries in parallel, on the same collection of texts, as a way to examine technologies with a variety of perspectives. Figure <a href="enriched-dictionaries-for-innovation.html#fig:dictwf">10.1</a> illustrates the main steps of the methodology.</p>
<div class="figure" style="text-align: center"><span id="fig:dictwf"></span>
<img src="_bookdown_files/figures/dictwf.png" alt="Steps of the dictionary approach to the text mining of patents." width="80%" />
<p class="caption">
Figure 10.1: Steps of the dictionary approach to the text mining of patents.
</p>
</div>
<p>The basic idea of the process is that a document can be represented by the set of terms, numbers, graphic symbols and punctuation that compose meaningful sentences. The “technical” objects belonging to this set are named Features. Starting from selected patent sets, a Document-Feature Matrix is created using different dictionaries as filters to select only the desired features; the obtained matrices are then used as the starting point to perform a topics number evaluation algorithm, based on the connections between documents with same or similar features. Topics are formally clusters of features, coupled because of their pertinence to the same concept; measuring the percentage of cohesion of a document to each topic recognized is, then, a method to collect information about the contents of the document itself.</p>
<div id="creation-of-the-document-feature-matrix" class="section level4 unnumbered">
<h4>Creation of the Document-Feature Matrix</h4>
<p>A patent can be represented by a vector exhibiting the number of occurrences, if a given feature is included, and 0 otherwise. Since these features are ordered within dictionaries, all patents in a patent set can be represented with similar vectors. A collection of texts can be therefore represented as a matrix called Document-Feature. A Document Feature Matrix (DFM) is a algebraic matrix with N rows, corresponding to individual documents in the corpus, and M columns representing the features. With this formalization it is possible to measure the occurrences of features in every document contained in the corpus. It is a powerful tool to get a fast quantitative information about a document set, counting the features originated in the dictionaries, with the opportunity of selecting (or removing) some of them.</p>
<p>Each patent in the patent set is filtered using one of the four dictionaries at a time. The resulting list of words was processed in order to eliminate the generic words, or those words that, being diffused across most documents, do not have specific semantic content. This is done by using the stopwords labelled SMART in the text mining literature, in combination with a list of stopwords extracted from the text of patents and including some generic words in the legal language of patents (or “Patentese”), such as claim, invention, right, tool etc.</p>
<p>Due to the large number of entries in dictionaries and the exploratory nature of this work, we restricted the number of features considered for each of the DFMs to 500. The selection was made automatically, by selecting the top 500 terms in terms of share of documents in which they appear. In some cases the selected patent text delivered no matching with the selected features. In this case the patents were eliminated. Thus the final DFM only includes documents with positive entries from the lists of features extracted from the analysis. It must be noted that DFMs are sparse matrices: sparsity is a mathematical parameter, varying from 0 to 1, that indicates how much systems are loosely coupled. In particular, it is a measure of the Z zero-valued elements in a matrix divided by the N x M total number of elements. Thus, a sparse matrix is a matrix in which large part of elements in cells are zeroes. Sparsity it is a good measure of how documents in a corpus differ from each other with respect to their Features.</p>
<p>Documents are iteratively examined in pairs: if two documents share a feature they are considered similar. Various metrics of similarity can be defined and computed. After a measure of similarity is defined, various clustering techniques can be applied as well. In this application we use the cosine similarity, which is largely adopted in the field. Cosine similarity is a measure of similarity between two non-zero vectors, that computes the cosine of the angle between them in an inner product space. Each document in the Document-feature Matrix is characterized by a vector where the value of each dimension corresponds to the number of times the feature appears in the document. Two documents, then, are similar if their cosine similarity is near to 1.</p>
<p>Given two n-dimensional vectors of attributes, x and y, their cosine similarity is calculated by the dot product of them, normalized by the product of the vector lengths. Results of the similarity measures are stored in a N x N matrix, in which documents are both upon rows and columns. It is a squared matrix with ones on its diagonal, indicating identities between patents and themselves, and the upper (or lower, for it is the same) half filled with similarity values for all the document couples. The most intuitive method to analyze which patents are more similar is to visualize them in a graph. To do so, it was used the igraph package , which contains a list of commands for creating and analyzing graphs. We can use our similarity matrices as a base to build the corresponding graphs, using them as they were adjacency matrices.</p>
</div>
<div id="adjacency-matrix" class="section level4 unnumbered">
<h4>Adjacency Matrix</h4>
<p>An Adjacency Matrix is a square matrix that represents a finite graph. In this case, the similarity matrix has in position (i,j) the inverse of the distance between vertices vi and vj . This gives to the graph not only information about whether two vertices are connected, but also a measure of their connection. It was chosen to set a threshold t = 0.8 for similarities in order to avoid representation of loose relations between vertices. To better visualize results, graphs were migrated from R to Gephi, a visualization software that simplifies this kind of operations. Gephi makes possible to assign colors to nodes as an intuitive way to label them; in our graphs four different colors were used to partition patents in their class, then Force Atlas layout algorithm was performed to see how well the graphs were clustered.</p>
</div>
<div id="optimal-number-of-topics" class="section level4 unnumbered">
<h4>Optimal Number of Topics</h4>
<p>Once created, a DFM can be interpreted as a graph showing similarity links between documents. In this context, in fact, similarity can be defined in terms of the percentage of total features that are shared between documents. Features can also be grouped in topics. In turn, similarity between documents can be defined in terms of the percentage of topics shared between documents. Finally, documents that refer to the same topic, hence are similar among them, can be grouped in cluster. Within a cluster it is possible to define a metrics of cohesiveness: the larger the number of topics shared by documents, the larger the cohesiveness of the cluster they belong to.</p>
<p>The optimal number of topics is not defined once and for all, but it depends in subtle ways from the characteristics of the documents in the set and the list of features. Thus the optimal number is the result of an experimental approach. The R package ldatuning calculates several metrics and then delivers an estimate on the optimal number of topics for Latent Dirichlet Allocation (LDA) models. In our application, we chose the CAOJUAN metric to evaluate the number of topics. The decision to use just one metric was made due to computational reasons; we selected the model proposed by Cao Juan, since he demonstrated that LDA models perform in an optimal way when the average cosine distance of topics reaches the minimum. Starting from the assumption that less correlated topics are more independent, the author defined average dispersion as a metric to measure the topic structure, using cosine distance. A smaller average dispersion denotes a more stable topic structure, that is, a topic model which is defined in a better way.</p>
</div>
</div>
<div id="results-11" class="section level2">
<h2><span class="header-section-number">10.4</span> Results</h2>
<p>We now turn to the analysis of the topics resulting from the filtering procedure based on dictionaries, the construction of the DFM, the measurement of cosine similarity. We will use four measures to highlight the content of the patent sets:</p>
<ul>
<li>Average cosine distance of topics</li>
<li>Average number of topics</li>
<li>Average sparsity of DFM</li>
</ul>
<p>The dictionaries were divided in two macro-classes, due to their nature: Non-Technical Dictionaries (Acts and Artifacts) versus Technical Dictionaries (Functional Verbs and Advantages &amp; Disadvantages). The Blacklist set, filtered with canonical and “Patentese” stopwords, will be useful for a more complete understanding of the findings.</p>
<p>In order to test the dictionary approach to clustering, we trained the algorithm on a standard patent set, formed by patents belonging to four IPC classes whose technologies are largely known. Table <a href="#tab:dicpatents"><strong>??</strong></a> illustrates the International Patent Classification (IPC) classes included in the analysis (a random sample of 2.000 patents each).</p>

<p>The technologies examined are largely mature. As it is shown in Figure <a href="enriched-dictionaries-for-innovation.html#fig:dicttrendsipc">10.2</a>, in some of them the patenting activity started as early as XIX century. The maturity of these technologies is a good starting condition for the training set: similarities and dissimilarities of the patents should be clearly visible.</p>
<p>By placing them together in the same patent set we deliberately include very different technologies. We ask the algorithm based on dictionaries to be able to discriminate technologies in a patent set which is, by construction, highly heterogeneous.</p>
<div class="figure" style="text-align: center"><span id="fig:dicttrendsipc"></span>
<img src="_bookdown_files/figures/dicttrendsipc.png" alt="Technological history of the technologies in the selected patent sets." width="80%" />
<p class="caption">
Figure 10.2: Technological history of the technologies in the selected patent sets.
</p>
</div>
<p>Table <a href="#tab:dictmetrix"><strong>??</strong></a> offers a summary of measures derived from the application of dictionaries to the four patent sets combined together.</p>

<p>Two findings are visible in Table <a href="#tab:dictmetrix"><strong>??</strong></a>. First, there is large difference in the cohesiveness of the clustering based on Technical and Non-technical dictionaries. The average cosine distance calculated with the Artifacts and Acts dictionaries is ten times smaller than the average cosine distance for Advantages &amp; disadvantages and the Functional dictionary. Second, Non-technical dictionaries give origin to a much smaller number of topics.</p>
<p>Let us examine the differences between dictionaries more in detail. We start by examining the performance of each dictionary in extracting and clustering topics.</p>
<div id="non-technical-dictionaries" class="section level4 unnumbered">
<h4>Non-technical Dictionaries</h4>
<p>Table <a href="#tab:dictactsresults"><strong>??</strong></a> shows the results of the application of the Acts dictionary to the four separate patent sets, while Table <a href="#tab:dictartsresults"><strong>??</strong></a> shows the same for the Artifacts dictionary.</p>






<p>These two dictionaries were retrieved from the Wordnet categorization dictionary, which is a tool for linguistic analysis; thus, the terms comprised in them are of common use, and their suitability for the description of patents is not guaranteed. Taken together the Non-technical dictionaries deliver two main results. First, there is a difference in the optimal number of topics between simple technologies (Burners and Toothbrushes) and more complex and articulated technologies (Prosthesis and Funeral devices). In the case of the Acts dictionary, the optimal number of topics is 2-6 for the former technologies and 18-16 for the latter. Second, Non-technical dictionaries deliver clusters with an extremely low cosine distance, that is, with strong internal cohesiveness. In other words, Non-technical dictionaries produce an excellent clustering of documents. This suggests that they can be used to obtain a small number of non-technical topics, that allow a general identification of the content of documents, but not the identification of technical similarities and differences.</p>
<p>To provide an example of the potential of the dictionary methodology we show in Tables <a href="#tab:acttopicresults"><strong>??</strong></a> and <a href="#tab:arttopicresults"><strong>??</strong></a> the top ten words found in a selection of the topics extracted with the Non-technical dictionaries in the patent set Prosthesis. We need to test whether Non-technical dictionaries deliver topics that are satisfactory with respect to their potential to describe technologies.</p>


<p>In the case of the Artifacts dictionary, Topic 5 gives us information on components related to the body part interested by the prosthesis, while Topic 14 tells us about electrical tools and sensors that are involved in the invention. Also in this case, the two groups are useful to gather information about the basic composition of the set, but not enough on focus to have a satisfactory definition of the technologies.</p>
</div>
<div id="technical-dictionaries" class="section level4 unnumbered">
<h4>Technical Dictionaries</h4>
<p>Table <a href="#tab:dictadvsresults"><strong>??</strong></a> shows the results of the application of the Advantages &amp; Disadvantages dictionary to the four separate patent sets, while Table <a href="#tab:dictfunctsresults"><strong>??</strong></a> shows the same for the Functional dictionary. The technical dictionaries applied contain a large number of features, generating a large DFM. An overall look at Tables <a href="#tab:dictadvsresults"><strong>??</strong></a> and <a href="#tab:dictfunctsresults"><strong>??</strong></a> shows several interesting findings. First of all, the technical dictionaries allow the identification of a much larger number of topics with respect to non-technical dictionaries: the average number is 32 for the Advantages &amp; Disadvantages, and 37 for the Functional dictionary. This means that technical dictionaries show a higher power of discrimination of semantic content. This is an important finding for Technology intelligence, since a larger number of internally coherent topics is an extremely useful starting point for the interpretation of their content.</p>






<p>Second, in the case of the Functional dictionary the Document-Feature Matrix (DFMs) is less sparse: the average sparsity index is 73,8%, as opposed to around 98% for the non-technical dictionaries and 94,3% for the Advantages &amp; Disadvantages dictionary. Third, the total number of documents used, i.e. the documents in which there is at least one matching between the words and the features of the dictionary, is larger in the case of technical dictionaries (in particular, it is 498 out of 500 for the Functional dictionary).</p>
<p>Fourth, the cosine distance of topics is approximately ten times larger in the case of technical dictionaries, meaning that clusters enjoy a over cohesiveness. These findings point to an important contribution of technical dictionaries, and in particular of the Functional dictionary, to Technology intelligence, that is, technology mapping, clustering, interpretation, and foresight.</p>
<p>The Functional dictionary offers a large number of features that find a matching with the text of the patents, so that the resulting DFM is less sparse. The fact that the total number of topics is larger also means that each of the topics is populated by a smaller number of features. Each topic, therefore, lends itself to a clear understanding of the technical content. The differences between topics have also a clear technical interpretation.</p>
<p>It can be said that technical dictionaries, as opposed to non-technical dictionaries, allow a fine-grained intelligence of the technical content of patent sets. This is in itself an important achievement, given that the topic modelling algorithm has not been previously trained, but is only filtered by the dictionary. In practice we achieve the precise results of supervised topic modeling with an effort which is more similar to that of the unsupervised approach.</p>
<p>In order to give a better understanding of the potential of the technical dictionary approach, let us examine more in detail in table <a href="#tab:advtopicresults"><strong>??</strong></a> and <a href="#tab:functtopicresults"><strong>??</strong></a> the top 10 words in some of the topics identified in one of the four patent sets examined, namely Prosthesis.</p>


<p>In Table <a href="#tab:advtopicresults"><strong>??</strong></a> Topic 6 calls the attention to features of the prosthesis such as adjustability, facilitation and stability, that is, on ergonomic features. Topic 28 emphasizes comfort (as an advantage made possible by the invention) or discomfort (as a disadvantage addressed by the invention). The content of these topics, thanks to the power of the technical dictionary, is crystal clear.</p>
<p>In Table <a href="#tab:functtopicresults"><strong>??</strong></a> we see, in topic 15, the positioning of prosthesis on the neck of patients, and in topic 20, the orientation of arms with the support of prosthesis. Both these topics deal with the issue of placing the prosthesis around or over various parts of the human body. It is important to underline that, once generated, topics can also be combined together in further steps of the analysis.</p>
</div>
<div id="blacklist" class="section level4 unnumbered">
<h4>Blacklist</h4>
<p>Finally, the information power of technical dictionaries can be appreciated by comparing the results with those that would be obtained by filtering the texts with semantically poor words, such as generic words (stopwords) and “patentese” generic words. As stated above, the combination between generic and patentese stopwords is labelled Blacklist.</p>



<p>The Table <a href="#tab:dictblacklist"><strong>??</strong></a> shows the clustering results using as filter a blacklist, composed by traditional and patentese stopwords. The following observations are in order. First, we observe a remarkable flatness in the number of topics: it equals 40, which is the maximum value that can be achieved by the algorithm. Second, Burners and Toothbrushes have a smaller distance value, confirming that these two patent sets are composed by more homogeneous types of features.</p>
<p>Summing up, it seems that the dictionaries deliver different results. On the one hand, Non-technical dictionaries include more generic word expressions. Non-technical dictionaries deliver better performance if the goal is to provide a global representation of the technological field, since they generate a smaller number of topics and better indicators of effectiveness of clustering.</p>
<p>On the other hand, if the goal is to detect novelty and technological trends, or to identify the areas of exploration of emerging technologies, then Technical dictionaries perform better, as they generate a larger number of topics. Analysing these topics it is possible to get insights on technological differences among patents in the same patent set and to produce detailed technology maps. This is particularly important for innovations in the pre-paradigmatic stage.</p>
</div>
</div>
<div id="discussions" class="section level2">
<h2><span class="header-section-number">10.5</span> Discussions</h2>
<p>Patent clustering is a standard application in technology intelligence and has been common practice in the professional IP industry. In recent years, patent clustering based on text mining has gained large acceptance. In this section we have introduced a novel methodology to cluster patents. With respect to the existing literature we give a contribution by testing the use of dictionaries as a structured list of words to filter patent data and build up clusters. The clustering based on dictionaries is significantly different from the one provided by IPC classification. We have tested the use of multiple dictionaries on the same patent set. The resulting clusters allow a fine-grained interpretation, which is illuminating for purposes of technology intelligence. Kreuchauff and Korzinov <span class="citation">(Kreuchauff and Korzinov <a href="#ref-kreuchauff2017patent">2017</a>)</span> have developed a set of performance criteria to compare and evaluate the identification approaches proposed in the literature. These criteria include: degree of intervention of experts, portability, transparency, replicability, adaptability, updating capacity, and finally extent and relevance of data obtained. We suggest that the enriched dictionary approach satisfies all these criteria: it does not involve the role of experts, is portable and transparent (after publication of the dictionary in the open literature), it is therefore replicable to any technology, is adaptable and has capacity to update (particularly if based on Wikipedia), and offers broad and relevant data.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-rotolo2015emerging">
<p>Rotolo, Daniele, Diana Hicks, and Ben R Martin. 2015. “What Is an Emerging Technology?” <em>Research Policy</em> 44 (10). Elsevier: 1827–43.</p>
</div>
<div id="ref-blei2003latent">
<p>Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3 (Jan): 993–1022.</p>
</div>
<div id="ref-fantoni2013automatic">
<p>Fantoni, Gualtiero, Riccardo Apreda, Felice Dell’Orletta, and Maurizio Monge. 2013. “Automatic Extraction of Function–behaviour–state Information from Patents.” <em>Advanced Engineering Informatics</em> 27 (3). Elsevier: 317–34.</p>
</div>
<div id="ref-apreda2016functional">
<p>Apreda, Riccardo, Andrea Bonaccorsi, Felice dell’Orletta, and Gualtiero Fantoni. 2016. “Functional Technology Foresight. a Novel Methodology to Identify Emerging Technologies.” <em>European Journal of Futures Research</em> 4 (1). Springer: 13.</p>
</div>
<div id="ref-cascini2004natural">
<p>Cascini, Gaetano, Alessandro Fantechi, and Emilio Spinicci. 2004. “Natural Language Processing of Patents and Technical Documentation.” In <em>International Workshop on Document Analysis Systems</em>, 508–20. Springer.</p>
</div>
<div id="ref-dewulf2006directed">
<p>Dewulf, Simon. 2006. “Directed Variation: Variation of Properties for New or Improved Function Product Dna, a Base for ‘Connect and Develop’.” <em>ETRIA TRIZ Futures</em>.</p>
</div>
<div id="ref-cascini2007computer">
<p>Cascini, Gaetano, Davide Russo, and Manuel Zini. 2007. “Computer-Aided Patent Analysis: Finding Invention Peculiarities.” In <em>Trends in Computer Aided Innovation</em>, 167–78. Springer.</p>
</div>
<div id="ref-cascini2008measuring">
<p>Cascini, Gaetano, and Manuel Zini. 2008. “Measuring Patent Similarity by Comparing Inventions Functional Trees.” In <em>Computer-Aided Innovation (Cai)</em>, 31–42. Springer.</p>
</div>
<div id="ref-petrov2002laws">
<p>Petrov, Vladimir. 2002. “The Laws of System Evolution.” <em>The TRIZ Journal</em> 3: 9–17.</p>
</div>
<div id="ref-verhaegen2009relating">
<p>Verhaegen, P-A, Joris D’hondt, Joris Vertommen, Simon Dewulf, and Joost R Duflou. 2009. “Relating Properties and Functions from Patents to Triz Trends.” <em>CIRP Journal of Manufacturing Science and Technology</em> 1 (3). Elsevier: 126–30.</p>
</div>
<div id="ref-wang2010identifying">
<p>Wang, Ming-Yeu, Dong-Shang Chang, and Chih-Hsi Kao. 2010. “Identifying Technology Trends for R&amp;D Planning Using Triz and Text Mining.” <em>R&amp;d Management</em> 40 (5). Wiley Online Library: 491–509.</p>
</div>
<div id="ref-yoon2011automated">
<p>Yoon, Janghyeok, and Kwangsoo Kim. 2011a. “An Automated Method for Identifying Triz Evolution Trends from Patents.” <em>Expert Systems with Applications</em> 38 (12). Elsevier: 15540–8.</p>
</div>
<div id="ref-park2013identification">
<p>Park, Hyunseok, Jason Jihoon Ree, and Kwangsoo Kim. 2013. “Identification of Promising Patents for Technology Transfers Using Triz Evolution Trends.” <em>Expert Systems with Applications</em> 40 (2). Elsevier: 736–43.</p>
</div>
<div id="ref-yoon2012detecting">
<p>Yoon, Janghyeok, and Kwangsoo Kim. 2012. “Detecting Signals of New Technological Opportunities Using Semantic Patent Analysis and Outlier Detection.” <em>Scientometrics</em> 90 (2). Springer: 445–61.</p>
</div>
<div id="ref-yoon2011invention">
<p>Yoon, Janghyeok, Sungchul Choi, and Kwangsoo Kim. 2011. “Invention Property-Function Network Analysis of Patents: A Case of Silicon-Based Thin Film Solar Cells.” <em>Scientometrics</em> 86 (3). Springer: 687–703.</p>
</div>
<div id="ref-kim2010cause">
<p>Kim, Hongbin, Sungchul Choi, Cheolhyun Jeong, and Kwangsoo Kim. 2010. “Cause-and-Effect Function Analysis.” In <em>Management of Innovation and Technology (Icmit), 2010 Ieee International Conference on</em>, 518–23. IEEE.</p>
</div>
<div id="ref-yoon2011identifying">
<p>Yoon, Janghyeok, and Kwangsoo Kim. 2011b. “Identifying Rapidly Evolving Technological Trends for R&amp;D Planning Using Sao-Based Semantic Patent Networks.” <em>Scientometrics</em> 88 (1). Springer: 213–28.</p>
</div>
<div id="ref-choi2012sao">
<p>Choi, Sungchul, Hyunseok Park, Dongwoo Kang, Jae Yeol Lee, and Kwangsoo Kim. 2012. “An Sao-Based Text Mining Approach to Building a Technology Tree for Technology Planning.” <em>Expert Systems with Applications</em> 39 (13). Elsevier: 11443–55.</p>
</div>
<div id="ref-park2011identifying">
<p>Park, Hyunseok, Janghyeok Yoon, and Kwangsoo Kim. 2011b. “Identifying Patent Infringement Using Sao Based Semantic Technological Similarities.” <em>Scientometrics</em> 90 (2). Akadémiai Kiadó, co-published with Springer Science+ Business Media BV, Formerly Kluwer Academic Publishers BV: 515–29.</p>
</div>
<div id="ref-kreuchauff2017patent">
<p>Kreuchauff, Florian, and Vladimir Korzinov. 2017. “A Patent Search Strategy Based on Machine Learning for the Emerging Field of Service Robotics.” <em>Scientometrics</em> 111 (2). Springer: 743–72.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="15">
<li id="fn15"><p><a href="https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/wordnet-based-categorization-dictionary" class="uri">https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/wordnet-based-categorization-dictionary</a><a href="enriched-dictionaries-for-innovation.html#fnref15">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="explpatentnovel.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="impactresuser.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["thesis_source.pdf", "thesis_source.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
