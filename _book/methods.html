<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Mining Technical Knowledge from Texts</title>
  <meta name="description" content="This document contains the PhD thesis of Filippo Chiarello.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Mining Technical Knowledge from Texts" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Mining Technical Knowledge from Texts" />
  
  <meta name="twitter:description" content="This document contains the PhD thesis of Filippo Chiarello." />
  

<meta name="author" content="Filippo Chiarello">


<meta name="date" content="2018-09-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="sota.html">
<link rel="next" href="future-developments.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Text Mining Techniques for Knowledge Extraction from Technical Documents</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="goal.html"><a href="goal.html"><i class="fa fa-check"></i><b>1</b> Goal</a></li>
<li class="chapter" data-level="2" data-path="problem.html"><a href="problem.html"><i class="fa fa-check"></i><b>2</b> Problem</a></li>
<li class="chapter" data-level="3" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>3</b> Solutions</a></li>
<li class="chapter" data-level="4" data-path="challenges-understanding-and-programming.html"><a href="challenges-understanding-and-programming.html"><i class="fa fa-check"></i><b>4</b> Challenges: Understanding and Programming</a><ul>
<li class="chapter" data-level="4.1" data-path="challenges-understanding-and-programming.html"><a href="challenges-understanding-and-programming.html#understanding"><i class="fa fa-check"></i><b>4.1</b> Understanding</a></li>
<li class="chapter" data-level="4.2" data-path="challenges-understanding-and-programming.html"><a href="challenges-understanding-and-programming.html#programming"><i class="fa fa-check"></i><b>4.2</b> Programming</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="research-questions.html"><a href="research-questions.html"><i class="fa fa-check"></i><b>5</b> Research Questions</a></li>
<li class="chapter" data-level="6" data-path="stakeholders.html"><a href="stakeholders.html"><i class="fa fa-check"></i><b>6</b> Stakeholders</a></li>
<li class="chapter" data-level="7" data-path="knowledge-intensive-manegement-engineering.html"><a href="knowledge-intensive-manegement-engineering.html"><i class="fa fa-check"></i><b>7</b> Knowledge Intensive Manegement Engineering</a></li>
<li class="chapter" data-level="8" data-path="sota.html"><a href="sota.html"><i class="fa fa-check"></i><b>8</b> State of the Art</a><ul>
<li class="chapter" data-level="8.1" data-path="sota.html"><a href="sota.html#sotatools"><i class="fa fa-check"></i><b>8.1</b> Phases, Tasks, and Techniques</a><ul>
<li class="chapter" data-level="8.1.1" data-path="sota.html"><a href="sota.html#sotatoolsprogram"><i class="fa fa-check"></i><b>8.1.1</b> Program</a></li>
<li class="chapter" data-level="8.1.2" data-path="sota.html"><a href="sota.html#sotatoolsimport"><i class="fa fa-check"></i><b>8.1.2</b> Import</a></li>
<li class="chapter" data-level="8.1.3" data-path="sota.html"><a href="sota.html#sotatoolstidy"><i class="fa fa-check"></i><b>8.1.3</b> Tidy</a></li>
<li class="chapter" data-level="8.1.4" data-path="sota.html"><a href="sota.html#sotatoolstransform"><i class="fa fa-check"></i><b>8.1.4</b> Transform</a></li>
<li class="chapter" data-level="8.1.5" data-path="sota.html"><a href="sota.html#sotatoolsmodel"><i class="fa fa-check"></i><b>8.1.5</b> Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="sota.html"><a href="sota.html#sotatoolsvisualize"><i class="fa fa-check"></i><b>8.1.6</b> Visualize</a></li>
<li class="chapter" data-level="8.1.7" data-path="sota.html"><a href="sota.html#sotatoolscomunicate"><i class="fa fa-check"></i><b>8.1.7</b> Comunicate</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="sota.html"><a href="sota.html#sotadocuments"><i class="fa fa-check"></i><b>8.2</b> Documents</a><ul>
<li class="chapter" data-level="8.2.1" data-path="sota.html"><a href="sota.html#sotadocumentsunderstand"><i class="fa fa-check"></i><b>8.2.1</b> Understand</a></li>
<li class="chapter" data-level="8.2.2" data-path="sota.html"><a href="sota.html#sotadocumentspatents"><i class="fa fa-check"></i><b>8.2.2</b> Patents</a></li>
<li class="chapter" data-level="8.2.3" data-path="sota.html"><a href="sota.html#sotadocumentspapers"><i class="fa fa-check"></i><b>8.2.3</b> Papers</a></li>
<li class="chapter" data-level="8.2.4" data-path="sota.html"><a href="sota.html#sotadocumentswiki"><i class="fa fa-check"></i><b>8.2.4</b> Wikipedia</a></li>
<li class="chapter" data-level="8.2.5" data-path="sota.html"><a href="sota.html#sotadocumentstwitter"><i class="fa fa-check"></i><b>8.2.5</b> Social Media</a></li>
<li class="chapter" data-level="8.2.6" data-path="sota.html"><a href="sota.html#sotadocumentsjobs"><i class="fa fa-check"></i><b>8.2.6</b> Human Resources Documentation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>9</b> Case Studies: Methods and Results</a><ul>
<li class="chapter" data-level="9.1" data-path="methods.html"><a href="methods.html#patents"><i class="fa fa-check"></i><b>9.1</b> Patents</a><ul>
<li class="chapter" data-level="9.1.1" data-path="methods.html"><a href="methods.html#usersresults"><i class="fa fa-check"></i><b>9.1.1</b> Users</a></li>
<li class="chapter" data-level="9.1.2" data-path="methods.html"><a href="methods.html#advdrwresults"><i class="fa fa-check"></i><b>9.1.2</b> Advantages and Drawbacks</a></li>
<li class="chapter" data-level="9.1.3" data-path="methods.html"><a href="methods.html#trademakrs"><i class="fa fa-check"></i><b>9.1.3</b> Trademakrs</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="methods.html"><a href="methods.html#papers"><i class="fa fa-check"></i><b>9.2</b> Papers</a><ul>
<li class="chapter" data-level="9.2.1" data-path="methods.html"><a href="methods.html#sustainable-manufacturing"><i class="fa fa-check"></i><b>9.2.1</b> Sustainable Manufacturing</a></li>
<li class="chapter" data-level="9.2.2" data-path="methods.html"><a href="methods.html#blockchain"><i class="fa fa-check"></i><b>9.2.2</b> Blockchain</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="methods.html"><a href="methods.html#wikipedia"><i class="fa fa-check"></i><b>9.3</b> Wikipedia</a></li>
<li class="chapter" data-level="9.4" data-path="methods.html"><a href="methods.html#twitter"><i class="fa fa-check"></i><b>9.4</b> Twitter</a></li>
<li class="chapter" data-level="9.5" data-path="methods.html"><a href="methods.html#job-profiles"><i class="fa fa-check"></i><b>9.5</b> Job Profiles</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="future-developments.html"><a href="future-developments.html"><i class="fa fa-check"></i><b>10</b> Future Developments</a><ul>
<li class="chapter" data-level="10.1" data-path="future-developments.html"><a href="future-developments.html#marketing-1"><i class="fa fa-check"></i><b>10.1</b> Marketing</a></li>
<li class="chapter" data-level="10.2" data-path="future-developments.html"><a href="future-developments.html#research-and-development"><i class="fa fa-check"></i><b>10.2</b> Research and Development</a></li>
<li class="chapter" data-level="10.3" data-path="future-developments.html"><a href="future-developments.html#design"><i class="fa fa-check"></i><b>10.3</b> Design</a></li>
<li class="chapter" data-level="10.4" data-path="future-developments.html"><a href="future-developments.html#human-resources"><i class="fa fa-check"></i><b>10.4</b> Human Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="conclusions.html"><a href="conclusions.html"><i class="fa fa-check"></i><b>11</b> Conclusions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i>Glossary</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mining Technical Knowledge from Texts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="methods" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Case Studies: Methods and Results</h1>
<p>L’aprrocio metodologico generico…</p>
<p>This chapter describes the methods applied for the analysis of technical documents. The methods are ensamble of Natural Language Processing (NLP) and Text Mining <em>techniques</em> described in <a href="sota.html#sotatools">8.1</a>, re-designed depending on the analyzed document and the analysis goal. Not all the <em>techniques</em> have been applied to all the documents: table tot summarise the relations between the documents under analysis (introduced in section <a href="sota.html#sotadocuments">8.2</a>) and the NLP techniques.</p>
<p>Table documents vs tools</p>
<div id="patents" class="section level2">
<h2><span class="header-section-number">9.1</span> Patents</h2>
<p>Patents contain a large quantity of information which is usually neglected. This information is hidden beneath technical and juridical jargon and therefore so many potential readers cannot take advantage of it. State of the art natural language processing tools and in particular named entity recognition tools, could be used to detect valuable concepts in patent documents.</p>
<p>In this section we present three methodologies capable of automatically detecting and extracting threee of the multiple entities hidden in patents: the users of the invention, advantages and drawbacks of the invention and trademarks contained in patents. The results of the methodologies are described, togheter with example of applications of the extracted entities for intelligence tasks.</p>
<div id="usersresults" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Users</h3>
<p>In this section we show the approach used to extract the users of the invention described in a patent. The proposed process is shown in figure <a href="methods.html#fig:procesuser">9.1</a> and its phases are:</p>
<ol style="list-style-type: decimal">
<li><p><em>Generation of an input list of users</em>: search all possible sources with the aim of creating an input list of users with the largest possible coverage (section <a href="methods.html#sourc">9.1.1.2</a>);</p></li>
<li><p><em>Patent set selection</em>: select the set of documents from which extract the users (section <a href="methods.html#patsel">9.1.1.3</a>);</p></li>
<li><p><em>Patent text pre-processing</em>: application of natural language processing tools on the documents with the aim of preparing them for the automatic user extraction;</p></li>
<li><p><em>Automatic patent set annotation 1</em>: projection of the input list of users on the text to generate the Automatically Annotated Patent Set 1;</p></li>
<li><p><em>Relevant sentences extraction</em>: selection of sentences containing at least one user to generate an informative training set;</p></li>
<li><p><em>Automatic patent set annotation 2</em>: generation of a statistical model by a machine learning algorithm based on the training set sentences and automatically tagging the patent set to generate the Automatically Annotated Patent Set 2;</p></li>
<li><p><em>Difference computation</em>: generation of the new list of users by computing the difference between the lists of users found in the automatically annotated patent set 1 and 2;</p></li>
<li><p><em>Manual review</em>: manual selection of the entities that, in the new list of users, are effectively users. This new list will enrich the original list of users. This phase is described in section <a href="methods.html#manrev">9.1.1.6</a>.</p></li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:procesuser"></span>
<img src="_bookdown_files/figures/Process.pdf" alt="Process flow diagram of the proposed automatic user extraction system from patents. The diagram contains the representation of the documents and the operations performed on them. The process takes in input a patent set and a list of users and produces a list of new users as output." width="80%" />
<p class="caption">
Figure 9.1: Process flow diagram of the proposed automatic user extraction system from patents. The diagram contains the representation of the documents and the operations performed on them. The process takes in input a patent set and a list of users and produces a list of new users as output.
</p>
</div>
<p>Before the description of each phases, in section <a href="methods.html#userdef">9.1.1.1</a> the concept of <em>user of the invetion</em> is explained by giving a definition of users and presenting the way that this concept is exploited in different knowledge fields.</p>
<div id="userdef" class="section level4">
<h4><span class="header-section-number">9.1.1.1</span> Users: A key information hidden in patents</h4>
<p>Patents are documents that must provide a detailed public disclosure of an invention <span class="citation">(Idris <a href="#ref-wipo2">2008</a>)</span>. An <em>invention</em> is a new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>The notion of usefulness implies that the invention must have some value and not necessarily for a human entity. In fact, patents usually describe processes, machines or composition of matter which are useful for another process, machine or composition of matter.</p>
<p>Therefore, we distinguish between stakeholders and users, considering the definitions given by the authors in <span class="citation">(Bonaccorsi and D’amico <a href="#ref-bonam2017">2017</a>)</span>.</p>
<p>Definition 1: <strong>Stakeholder</strong> : <em>Stakeholders are entities on which the invention has or will have a positive or negative effect in order to show usefulness.</em></p>
<p>This definition covers all possible entities that engage an active or passive relation with the invention.Given the logical condition of usefulness of patents, all patents must have stakeholder information. If a patent has not got any stakeholder information in it the patent application should be rejected.</p>
<p>Definition 2: <strong>User</strong>: <em>Users are animated or previously animated entities (human or animal, alive or dead), on which the invention has a positive or negative effect at an unspecified moment.</em></p>
<p>Given definition 2, it is clear that every user is a stakeholder while non-users stakeholders include artifacts, machines, manufacturing or operational processes.</p>
<p>Corollary 1: <strong>Multiple roles</strong>: <em>Identities may have multiple roles as users.</em></p>
<p>Our idea of users describes roles, not identities. Animated entities have an identity, as it happens for a specific person. A person has many roles as a user. For example, a <em>working mother</em> starts her day taking on the role of a <em>mom</em>, in which she is expected to feed her children and get them ready for school. At the office she shifts to the role of <em>project manager</em>, so she oversees projects in a timely and professional manner. <em>Working mother</em>, <em>mom</em> or <em>project manager</em> can be considered user roles attributed to the same person, or identity. From this definition it is clear that users are close to what social sciences define as “social roles”.</p>
<p>Afterward, we can outline knowledge fields using the concept of user, with a twofold aim: help the reader to understand how the concept of users is interpreted in different knowledge fields; explain the background of the methodology we adopted.</p>
<div id="social-sciences-social-roles-as-users" class="section level5 unnumbered">
<h5>Social sciences: social roles as users</h5>
<p>In social sciences <em>social roles</em> are comparable with our definition of user. As defined in the psychological field <span class="citation">(Dog <a href="#ref-alleydog">2015</a>)</span>, <em>“social roles refer to the expectations, responsibilities and behaviors we adopt in certain situations.”</em>. The example of the working mother shown before, is the case of social roles.</p>
<p>The field of social sciences is the only one in which an attempt of automatic extraction of users has been done. In <span class="citation">(Beller et al. <a href="#ref-belieber">2014</a>)</span> the authors extracted social roles from Twitter using heuristic methods. The authors looked for all the words preceded by constructions like “I’m a” and similar variations. This search resulted in 63.858 unique roles identified, 44.260 of which appeared only once. The result of the extraction process is noisy and only a low percentage of the extracted words are social roles. Despite of this noisy extraction, some entities are consistent with our definition of user, e.g. <em>doctor, teacher</em>, <em>mother</em> or <em>christian</em>.</p>
<p>Another work <span class="citation">(Beller, Harman, and Van Durme <a href="#ref-prefine">2014</a>)</span> tries to identify social roles on Twitter exploiting a set of assumptions. The authors take into account roles, each one with a set of related verbs: if someone uses verbs from a set, that person may cover that particular social role. To sanitize the collection of positively identified users, the authors crowd-sourced a manual verification procedure, using the Mechanical Turk platform <span class="citation">(Kittur, Chi, and Suh <a href="#ref-kittur2008crowdsourcing">2008</a>)</span>. Also here some interesting extractions are performed, obtaining users like <em>artist, athlete, blogger, cheerleader, christian, DJ</em>, or <em>filmmaker</em>. These two works differ from the present study for what concerns the analyzed texts and the methods to extract the entities. Nevertheless, the extracted set of entities is consistent with our definition of user.</p>
</div>
<div id="human-resources-management-workers-as-users" class="section level5 unnumbered">
<h5>Human Resources Management: workers as users</h5>
<p>In organizations, Human Resources Management is the function designed to maximize employees performance <span class="citation">(Johnson <a href="#ref-HRM">2009</a>)</span>. Employees are key actors and they can be considered users according to our definition.</p>
<p>Human Resources Management has tried to classify employees, especially in sub-fields like insurance, social security or work psychology. Usually, we refer to those as lists of jobs. Classifications were made with the goal of grouping similar jobs for educational requirements, job outlooks, salary ranges or work environments to facilitate social analysis and the placement of new workers. Such lists are relevant because, even if they represent just one subset of all the possible users, they contain valid information. Many institutions developed lists of jobs <span class="citation">(“Suffix Codes for Jobs Defined in the Dictionary of Occupational Titles, Third Edition” <a href="#ref-listjobs">1967</a>)</span>.</p>
</div>
<div id="medicine-patients-as-users" class="section level5 unnumbered">
<h5>Medicine: patients as users</h5>
<p>Another field of interest is medicine, since patients can be considered users. Also in this case there are many lists of patients, illnesses and diseases <span class="citation">(Health and Services <a href="#ref-cdcdiseases">2018</a>)</span>, which are valuable in terms of information contained.</p>
</div>
<div id="design-and-marketing-between-users-and-customers" class="section level5 unnumbered">
<h5>Design and Marketing: between users and customers</h5>
<p>In the field of <em>Design</em> the concept of user plays a central role and it overlaps with our definition of user. Many tools and theories like “User Centered Design” are based on the concept of user <span class="citation">(ISO <a href="#ref-ucd">1999</a>)</span>. As stated by the authors in <span class="citation">(K. <a href="#ref-npdul">2008</a>)</span>, the quality of the design process is proportional to the user needs’ satisfaction. It implies that a designer has to understand the user needs; as a consequence he has to discover whom are potential users.</p>
</div>
</div>
<div id="sourc" class="section level4">
<h4><span class="header-section-number">9.1.1.2</span> List of users generation</h4>
<p>To generate the input list of users, we used two different approaches: a bottom-up approach and a top-down approach. The bottom-up approach is based on the merge of lists from heterogeneous sources. In the present work we used the following lists of entities:</p>
<ul>
<li><p><em>Lists of jobs</em> :<span class="citation">(“Suffix Codes for Jobs Defined in the Dictionary of Occupational Titles, Third Edition” <a href="#ref-listjobs">1967</a>)</span>, 11.142 entities</p></li>
<li><p><em>Lists of sports and hobbies</em><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>: 9.660 entities;</p></li>
</ul>
<p><em>List of animals</em>:<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> 600 entities;</p>
<ul>
<li><p><em>Lists of patients</em>:<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> 14.609 users;</p></li>
<li><p><em>List of generic words</em>: manually generated. It contains users with a higher level of abstraction (such as <em>person</em> or <em>human being</em>), 56 items.</p></li>
</ul>
<p>Bottom-up approach produced a list of 35.767 entries.</p>
<p>Afterwards, a top-down approach was applied. Starting from the list generated with the bottom-up approach, we looked for alternative methods to indicate a user, finding defined word patterns. The most relevant are:</p>
<ul>
<li>Patterns like “hobby_term + practitioner” for the hobbies;</li>
<li>Patterns like “person who has + disease_term” or “suffering from + disease_term” for the diseases;</li>
<li>Patterns like “practitioner of + sport_term” for sports.</li>
</ul>
<p>Top-down approach generated a total of 41.090 entries.</p>
<p>The whole process generated a total of 76.857 users and gave us a reasonable number of terms to be used in the next step of the process.</p>
<p>Obviously our lists have a limited coverage and, therefore, they do not contain all variations of a certain user. For instance, the lists miss some users belonging to the classes mentioned above (e.g. new jobs emerged in the last years) and all the alternative ways for referring to a user we do not spotted in the top-down approach. For example our lists miss jobs like <em>data analyst</em>, <em>lap dancer</em>, <em>undertaker</em>, <em>mortician</em> and <em>thief</em> or patients with emerging diseases like <em>work-alcoholic</em> and <em>web-addicted</em>. In addition, our lists miss a class of users related to religious groups, containing users like <em>christians</em> or <em>jewish</em>. Such terms have intentionally <strong>not</strong> been introduced in the input list because we considered these terms as candidates to be extracted by the process in our case study .</p>
</div>
<div id="patsel" class="section level4">
<h4><span class="header-section-number">9.1.1.3</span> Patent set selection</h4>
<p>Our choice of patent sets aimed at challenging our system to find new users missing in the input list. To reproduce a patent set selection, we took into consideration the International Patent Classification (IPC) <span class="citation">(Organization <a href="#ref-wipo1">1971</a>)</span>. IPC is a hierarchical system of patent classes representing different areas of technology. Then, we wondered which classes could contain new users according to our seed list. Furthermore, IPC class A, which is the first level in IPC differentiation, is based on human necessities. For this reason, we assumed that in this class we would have found likely users from patents texts.</p>
</div>
<div id="patent-text-analysis" class="section level4">
<h4><span class="header-section-number">9.1.1.4</span> Patent text analysis</h4>
<p>Our Entity Extraction system is composed by a set of sequential phases. The first three phases are related to the linguistic annotation: sentence splitting and tokenization, part of speech tagging and lemmatization. Then, the patent set is analyzed by the entity extractor, specialized for users extraction. A more detailed description of each phase is:</p>
<ul>
<li><p>Sentence splitting and Tokenization: These processes split the text into sentences and then segment each sentence in orthographic units called tokens. In our system, sentence splitting plays a key role since thanks to a given word, it is possible to find sentences where the word is used. Finding correct boundaries for a specific word allows to dramatically reduce the space to retrieve its surrounding contexts.</p></li>
<li><p>POS tagging and Lemmatization: The Part-Of-Speech tagging (or POS tagging) is the process of assigning unambiguous grammatical categories to words in context. It plays a key role in NLP and in many language technology systems. For the present application we used the most recent version of the Felice-POS-tagger described in <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>)</span>. Once the computation of the POS-tagged text is completed, the text is lemmatized according to the result of this analysis.</p></li>
<li><p>Semi-automatic Users Annotation: The Users Extraction tool is based on supervised methods. Such methods require an entity annotated corpus in order to extract new entities from unseen documents. A semi-automatic method has been used to generate an annotated corpus of users to avoid manual annotation of a patent set. The method is a projection of the list of users on the patent set defined in section @ref{patsel}. The list of users described in section @ref{sourc} is cleaned to avoid linguistic ambiguities when projecting these entities on the corpus. For example, the term <em>“guide”</em> has two different meanings when used as a verb or as a noun. Furthermore, as a noun it could indicate a component of a system (guide for mechanical parts) or a person (someone employed to conduct others) and therefore a user. Avoiding ambiguities is a crucial aspect to produce an informative training set, so ambiguous words were pruned.</p></li>
</ul>
<p>The entity annotation schema for a single token is defined using a widely accepted BIO annotation scheme :</p>
<ul>
<li><strong>B-USE</strong>: the token is the beginning of an entity representing an User;</li>
<li><strong>I-USE</strong>: the token is the continuation of a sequence of tokens representing an User;</li>
<li><strong>O</strong>: for all the other cases.</li>
</ul>
</div>
<div id="user-entity-extraction" class="section level4">
<h4><span class="header-section-number">9.1.1.5</span> User Entity Extraction</h4>
<p>The Users extraction problem is tackled by the implementation of a supervised classifier that is trained on an annotated patent set. Thus, the patent set is linguistically-annotated, using the steps described above and entity-annotated, exploiting the semiautomatic annotation process executed in the previous steps.</p>
<p>Given a set of features the classifier trains a statistical model using the feature statistics extracted from the corpus. For each new document the trained model assigns to each word the probability of belonging to one of the classes previously defined (B-USE, I-USE, O).</p>
<p>In our experiments the classifier has been trained using two different learning algorithms: Support Vector Machines (SVM) using the LIBSVM library <span class="citation">(Hearst, Dumais, Osman, et al. <a href="#ref-svm">1998</a>)</span> configured to use a linear kernel and Multi Layer Perceptron (MLP) implemented using the Keras library <span class="citation">(Chollet <a href="#ref-libkeras">2015</a>)</span>. It has been proven that LSTM methods are well suited for similar NER task. Anyway, we chose SVM and MLP method to study how two wheel established state of the art classifiers perform on the specific task of user extraction from patents and to evaluate their performance in terms of precision and computational effort. We also think that the popularity of these methods increment the reproducibility of the work.</p>
<p>The classifier uses different kind of features extracted from the text:</p>
<ul>
<li><p><em>linguistic features</em>, i.e. lemma, Part-Of-Speech, prefix and suffix of the analyzed token;</p></li>
<li><p><em>contextual features</em>, the linguistic characteristics of the context words of the analyzed token; in addition the entity category of the previous token is considered;</p></li>
<li><p><em>compositional features</em>, combinations of contextual features and linguistic features. i.e. Part-Of-Speech of the previous word and the lemma of the current word. These extra features allow to infer statistics on the interaction of the combined features that can not be captured by a linear SVM model.</p></li>
<li><p><em>word2vec features</em>: vector representations of words computed by the <em>word2vec</em> <span class="citation">(Mikolov et al. <a href="#ref-word2vec1">2013</a>)</span> tool.</p></li>
</ul>
<p><em>Word2vec</em> is a NLP tool able to produce word representations exploiting big corpora. The main property of the vectors produced by <em>word2vec</em> is that words sharing similar contexts have similar vector representations. By using word vectors instead of the corresponding words we were able overcome the problem of the limited lexical knowledge in the training phase. Using these features and excluding all the others (delexicalized model) we expected that the resulting user extraction system had a lower precision and an higher recall in the classification phase. We presumed to find new users not contained in the input seed list.</p>
</div>
<div id="manrev" class="section level4">
<h4><span class="header-section-number">9.1.1.6</span> Manual Review of the new list of users</h4>
<p>It is still possible that the classification process creates false positive results (words labeled as users that do not match the definition in section @ref{theuse}). Thus, it is necessary to make a manual review of the extracted entities with the aim of evaluating the output.</p>
</div>
<div id="results" class="section level4">
<h4><span class="header-section-number">9.1.1.7</span> Results</h4>
<p>The following section describes the performances of the automatic users extraction process on two different patent sets. To test the system four experiments were conducted}. Finally the performances and the outcomes of the system are shown and discussed.</p>
<p>Following the guidelines for the patent set selection described in section , we examined two patent sets belonging to the IPC class A:</p>
<ul>
<li><strong>A47G33</strong>. The IPC definition of the subclass is <em>“religious or ritual equipment in dwelling or for general”</em>.</li>
<li><strong>A61G1-A61G13</strong>. The IPC definition of the subclass A61G1 is <em>“Stretchers”</em> while the definition of the subclass A61G13 is <em>“Operating tables; Auxiliary appliances therefor”</em>.</li>
</ul>
<p>We extracted from the private Errequadro s.r.l.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> database a random sample of 2.000 patents from each IPC class. For each patent set we applied the semiautomatic set annotation process by projecting the input list of users on the morphosyntactically analyzed patent set. After this process, each semi-automatically annotated patent set was split in two parts: the first was used as training set for the user extractor, and the second one was used as test set.</p>
<p>To build an informative training set, from the semi-automatically patent set we selected a subset of sentences containing at least one user. The size of the training set in both cases is approximately composed by 600.000 tokens. For each patent set table <a href="methods.html#tab:patentsetdetails">9.1</a> shows the number of sentences of the training set, the number of sentences of the test set, and the number of distinct users in the training set (re-projected by the semi-automatic annotation process).</p>
<p>ref —&gt; patent set-details</p>
<table style="width:100%;">
<caption><span id="tab:patentsetdetails">Table 9.1: </span> Statistics related to the patent set groups analyzed in the case study</caption>
<colgroup>
<col width="18%" />
<col width="23%" />
<col width="20%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"><em>patent set group</em></th>
<th align="center"><em>#Sentences - training</em></th>
<th align="center"><em>#Sentences - test</em></th>
<th align="center"><em>#Distinct users projected on training</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">A47G33</td>
<td align="center">13.364</td>
<td align="center">214.029</td>
<td align="center">126</td>
</tr>
<tr class="even">
<td align="center">A61G1-A61G13</td>
<td align="center">15.108</td>
<td align="center">2.520.350</td>
<td align="center">121</td>
</tr>
</tbody>
</table>
<p>We chose two orders of magnitude for the sentences test-set to test the efficiency of multiple configurations of the system.</p>
<p>To test the performances of the implemented user extractor, we devised four different configurations. Each configuration uses a specific learning algorithm and a set of features to build the statistical model. The main purpose of this procedure is to find the configurations that better perform in the user extraction task. In addition, the different behaviour of the system in the classification phase is studied. In table <a href="methods.html#tab:feat-confs">9.2</a> are reported the detailed configurations used in our experiments.</p>
<table>
<caption><span id="tab:feat-confs">Table 9.2: </span> Context windows of the extracted features considering 0 as the current analyzed token.</caption>
<thead>
<tr class="header">
<th align="center"><em>Feature group</em></th>
<th align="center"><em>Context Window</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Lemma unigrams</td>
<td align="center">\([-2, -1, 0, 1]\)</td>
</tr>
<tr class="even">
<td align="center">Lemma bigrams</td>
<td align="center">\([(-1 ,0), (0, 1)]\)</td>
</tr>
<tr class="odd">
<td align="center">Word bigrams</td>
<td align="center">\([(-1 ,0), (-2, -1), (0, 1), (1, 2)]\)</td>
</tr>
<tr class="even">
<td align="center">Word trigrams</td>
<td align="center">\([(1, 0, 1) (-2, 1, 0)]\)</td>
</tr>
<tr class="odd">
<td align="center">Pos unigrams</td>
<td align="center">\([-2, -1, 0, 1]\)</td>
</tr>
<tr class="even">
<td align="center">Pos bigrams</td>
<td align="center">\(([(-2, -1) (-1, 0), (0,1)])\)</td>
</tr>
<tr class="odd">
<td align="center">Compositional feature #1</td>
<td align="center">\((POS_{-1}, Lemma_{0})\)</td>
</tr>
<tr class="even">
<td align="center">Compositional feature #2</td>
<td align="center">\((Lemma_{-1}, Lemma_{0})\)</td>
</tr>
<tr class="odd">
<td align="center">Compositional feature #3</td>
<td align="center">\((Lemma_{0}, Lemma_{1})\)</td>
</tr>
<tr class="even">
<td align="center">Compositional feature #4</td>
<td align="center">\((POS_{0}, Lemma_{1})\)</td>
</tr>
<tr class="odd">
<td align="center">Compositional feature #5</td>
<td align="center">\((NER_{-1}, Lemma_{0})\)</td>
</tr>
<tr class="even">
<td align="center">Word2vec</td>
<td align="center"><span class="math display">\[-2, -1, 0, 1, 2\]</span></td>
</tr>
</tbody>
</table>
<p>By using the first and the second configuration we expected to have a higher precision in the classification phase, since explicit lexical information is used in the training phase. For the same reason we expected to have low recall in classification phase. On the other hand, the third and fourth configurations are delexicalized: lexical information is provided by word vectors computed by word2vec_. In these two configurations we expected to have an higher recall and a lower precision, due to the characteristics of the computed vectors explained before. To limit errors when using the <em>word2vec</em> features, some linguistically motivated filtering rules were introduced. Specifically, sequences of tokens classified as users were constrained from the following categories: verbs, adjectives not preceded by articles, articles and adverbs.</p>
<p>To evaluate the whole user extraction process in each experiment, we defined some evaluation measures. Each measure was introduced to evaluate the characteristics of the extraction system concerning the configuration applied.</p>
<p>These measures are:</p>
<ul>
<li>Training time: time needed to create the statistical model using the training set;</li>
<li>Test time: time needed to re-annotate the semi-automatically annotated patent set;</li>
<li>Number of extracted users: number of unique entities classified as user in the automatically annotated patent set;</li>
<li>Number of known users: number of distinct extracted users in the automatically annotated patent set and belonging to the list of user in input;</li>
<li>Number of new users: number of distinct entities classified as user in the automatically annotated patent set and not belonging to the input list of users;</li>
<li>Number of new correct users: number of distinct entities considered as user and as correct after a manual review;</li>
<li>Precision: ratio between the number of new distinct correct users and the total number of new distinct users;</li>
<li>Gain: ratio between the number of new distinct correct users and the number of re-projected distinct users on the training set.</li>
</ul>
<p>Table <a href="methods.html#tab:runs-data">9.3</a> reports the values of the defined metrics across all the experiments run on the two patent sets.</p>
<table style="width:100%;">
<caption><span id="tab:runs-data">Table 9.3: </span> Comparison of the values of the defined metrics across all the experiments. The patent set annotation in the experiment (6) was not performed due to the computational costs. All the experiments were run on a machine provided with 10 AMD Opteron(tm) 6376 processors.</caption>
<colgroup>
<col width="11%" />
<col width="13%" />
<col width="10%" />
<col width="10%" />
<col width="6%" />
<col width="5%" />
<col width="11%" />
<col width="10%" />
<col width="10%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Experiment</th>
<th align="center">Training time</th>
<th align="center">Test Time</th>
<th align="center">Extracted</th>
<th align="center">Known</th>
<th align="center">New</th>
<th align="center">New correct</th>
<th align="center">New wrong</th>
<th align="center">Prec. (%)</th>
<th align="center">Gain (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">1 (SVM)</td>
<td align="center">83m</td>
<td align="center">321m</td>
<td align="center">161</td>
<td align="center">93</td>
<td align="center">68</td>
<td align="center">47</td>
<td align="center">21</td>
<td align="center">69.11</td>
<td align="center">37.30</td>
</tr>
<tr class="odd">
<td align="left">2 (MLP)</td>
<td align="center">1911m</td>
<td align="center">9091m</td>
<td align="center">196</td>
<td align="center">55</td>
<td align="center">141</td>
<td align="center">27</td>
<td align="center">114</td>
<td align="center">19.15</td>
<td align="center">21.42</td>
</tr>
<tr class="even">
<td align="left">3 (MLP-W2V)</td>
<td align="center">165m</td>
<td align="center">246m</td>
<td align="center">162</td>
<td align="center">35</td>
<td align="center">127</td>
<td align="center">45</td>
<td align="center">82</td>
<td align="center">35.43</td>
<td align="center">35.71</td>
</tr>
<tr class="odd">
<td align="left">4 (SVM-W2V)</td>
<td align="center">1265m</td>
<td align="center">4310m</td>
<td align="center">121</td>
<td align="center">29</td>
<td align="center">92</td>
<td align="center">45</td>
<td align="center">47</td>
<td align="center">48.91</td>
<td align="center">35.71</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">5 (SVM)</td>
<td align="center">148m</td>
<td align="center">3443m</td>
<td align="center">302</td>
<td align="center">120</td>
<td align="center">182</td>
<td align="center">88</td>
<td align="center">108</td>
<td align="center">48.35</td>
<td align="center">72.72</td>
</tr>
<tr class="even">
<td align="left">6 (MLP)</td>
<td align="center">1818m</td>
<td align="center">—</td>
<td align="center">—</td>
<td align="center">—</td>
<td align="center">—</td>
<td align="center">—</td>
<td align="center">—</td>
<td align="center">—</td>
<td align="center">—</td>
</tr>
<tr class="odd">
<td align="left">7 (MLP-W2V)</td>
<td align="center">333m</td>
<td align="center">3530m</td>
<td align="center">305</td>
<td align="center">38</td>
<td align="center">267</td>
<td align="center">44</td>
<td align="center">230</td>
<td align="center">16.48</td>
<td align="center">36.36</td>
</tr>
<tr class="even">
<td align="left">8 (SVM-W2V)</td>
<td align="center">1268m</td>
<td align="center">47020m</td>
<td align="center">313</td>
<td align="center">49</td>
<td align="center">264</td>
<td align="center">74</td>
<td align="center">197</td>
<td align="center">28.03</td>
<td align="center">61.15</td>
</tr>
</tbody>
</table>
<p>For what concerns training and test time of the automatic patent set annotation, it’s clear that the configuration based on the SVM learning algorithm without the <em>word2vec</em> features performs better in both the experiments (1, 5). When the features based on <em>word2vec</em> are introduced, the configuration based on the MLP learning algorithm is the fastest both in training and test time (3, 6): it is due to the fact that keras implementation of this algorithm exploits all the available CPU cores of the system. On the other side, the MLP algorithm does not scale properly with a higher number of features, as seen in training and annotation time in the experiment (2). In addition, we could not perform the patent set annotation in the experiment (6), since it would have required more than 60 machine days to complete the process. When <em>word2vec</em> features are introduced, the patent set annotation based on the SVM algorithm is 10 times slower than the MLP algorithm.</p>
<p>For what concerns the precision in the automatic patent set annotation, the SVM configuration without <em>word2vec</em> features is clearly the more reliable: the precision values are from 1.5 to 2 times higher in the experiments (1, 5) in contrast to the other experiments. The higher precision is justified by the fact that the configurations based on <em>word2vec</em> features lack explicit lexical information: words with very similar contexts are represented by similar <em>word2vec</em> vectors, probably leading to errors in the classification phase. On the other hand, the use of <em>word2vec</em> vectors aims at extracting entities that would not be extracted by considering explicit lexical information only.</p>
<p>Finally, for what concerns information gain, the same amount of new information (21-37%) is extracted in the experiments on the A47G33 patent set. The gain values drastically change in the experiments on the A61G1-A61G13 patent set: in the experiments (5, 8) a gain between 61% and 72% is obtained: it is due to the size of this patent set in comparison to the A47G33 one. In the experiment (7), despite the introduction of <em>word2vec</em> features, a gain of 36% is obtained. This fact, in conjunction with the non-feasibility of the experimental configuration 6, shows how MLP systems lack in efficacy and efficiency (in entity extraction in patent domain) when the test-set has an order of magnitude of millions of sentences. We think that this result is relevant, based on our experience with practical applications.</p>
<p>Furthremore, a way to maximize the overall informative gain is to merge the results of all manually reviewed user extractions obtained by executing the patent set annotation process with all possible configurations.</p>
<p>The overall informative gain of the merging process is related to intersections that occur among the results obtained by the patent set annotation process in each configuration: the less the intersections, the more the overall informative gain obtained. In table <a href="methods.html#tab:mergedata">9.4</a> is shown the overall gain obtained by merging results of the manually reviewed extractions in each patent set.</p>
<table>
<caption><span id="tab:mergedata">Table 9.4: </span> Gain obtained by merging correct entities extracted from each patent set annotation.</caption>
<thead>
<tr class="header">
<th align="center">Configuration</th>
<th align="center">A47G33 - Gain (%)</th>
<th align="center">A61G1+A61G11 - Gain (%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">SVM</td>
<td align="center">37.30</td>
<td align="center">72.72</td>
</tr>
<tr class="even">
<td align="center">MLP</td>
<td align="center">21.42</td>
<td align="center">—</td>
</tr>
<tr class="odd">
<td align="center">MLP-W2V</td>
<td align="center">35.71</td>
<td align="center">36.36</td>
</tr>
<tr class="even">
<td align="center">SVM-W2V</td>
<td align="center">35.71</td>
<td align="center">61.15</td>
</tr>
<tr class="odd">
<td align="center">SVM - MLP</td>
<td align="center">52.38</td>
<td align="center">—</td>
</tr>
<tr class="even">
<td align="center">SVM - MLP-W2V</td>
<td align="center">69.84</td>
<td align="center">126.44</td>
</tr>
<tr class="odd">
<td align="center">SVM - SVM-W2V</td>
<td align="center">73.01</td>
<td align="center">103.30</td>
</tr>
<tr class="even">
<td align="center">MLP - MLP-W2V</td>
<td align="center">55.55</td>
<td align="center">—</td>
</tr>
<tr class="odd">
<td align="center">MLP - SVM-W2V</td>
<td align="center">57.14</td>
<td align="center">—</td>
</tr>
<tr class="even">
<td align="center">MLP-W2V - SVM-W2V</td>
<td align="center">59.52</td>
<td align="center">76.30</td>
</tr>
<tr class="odd">
<td align="center">SVM - SVM-W2V - MLP-W2V</td>
<td align="center">90.47</td>
<td align="center">140.49</td>
</tr>
<tr class="even">
<td align="center">SVM - MLP - MLP-W2V</td>
<td align="center">82.53</td>
<td align="center">—</td>
</tr>
<tr class="odd">
<td align="center">SVM - MLP - SVM-W2V</td>
<td align="center">85.71</td>
<td align="center">—</td>
</tr>
<tr class="even">
<td align="center">MLP - SVM-W2V - MLP-W2V</td>
<td align="center">77.77</td>
<td align="center">—</td>
</tr>
<tr class="odd">
<td align="center">SVM - MLP - SVM-W2V - MLP-W2V</td>
<td align="center">103.17</td>
<td align="center">—</td>
</tr>
</tbody>
</table>
<p>The table shows that the merging process of manually reviewed entities extracted from each patent set annotation run effectively contributes to increase the overall informative gain. For instance in the A47G33 patent set an overall gain of 103.17% is obtained, tripling the best result achieved by the extraction performed using the best single configuration. Good results are also achieved in the A47G33 patent set user extraction. In this case an overall gain of 140.49% is obtained, doubling the best result achieved by the extraction performed using the best single configuration.</p>
<p>The results shown in section 5 prove that if the goal of the extraction is to reach the maximal recall, an ensemble method (combining the output of multiple classifier) over-performs every single classifier method. Anyway, the ensemble approach has clear efficiency issues, because the time of analysis will be the sum of every single approach time (in hypotheses of non-parallelization). This leads to a trade off between the speed of the system and the quality of the results, and whoever would use the presented system can decide to gain benefit in one or in another direction.</p>
<p>Finally, tables <a href="methods.html#tab:result-extraction-dl-a47g33">9.5</a> and <a href="methods.html#tab:result-extraction-dlw2v-a47g33">9.6</a> show an overview of extracted users randomly chosen from the A47G33 patent set (the only one in which were able to perform all experiments). Each table is divided in two blocks, representing the results of the extraction performed using a specific configuration. For each extracted user is shown the corresponding lemma (the root form), the frequency (how many times that user appears in the whole corpus) and the total number of patents containing the user. Users not contained in the starting user list, are highlighted in bold.</p>
<p>The table shows that the system was able to extract characteristic users of the patent set. The results are in fact not unexpected for the IPC class under analysis: this is an evidence of the correct performances of the proposed system. In other words, the results presented in the table show that it is possible to train a NER systems able to extract sparse and valuable information. Such users are the ones that an expert would manually extract but the NER system does it with an enormous saving in terms of time and efforts.</p>
<p>Other remarkable results are:</p>
<ul>
<li><p>many newly extracted entities have very low frequency in the patent set: it shows that the developed system is able to extract rare entities.</p></li>
<li><p>table <a href="methods.html#tab:result-extraction-dlw2v-a47g33">9.6</a> shows that configurations using <em>word2vec</em> features are able to find new users with a higher frequency in the patent set: it was an expected result, since the <em>word2vec</em> configurations are not explicitly lexicalized and more able to generalize during extraction phase.</p></li>
<li><p>The system is able to extract single words and multi-words.</p></li>
<li><p>Taking into consideration the definition of user of an invention, the system extracts unusual and sometimes borderline users. Examples like <em>saint</em>, <em>angel</em>, <em>god</em> and <em>ghost</em> need discussion that is far beyond the purposes of the present paper. These results are a remarkable evidence of the human-like generalization ability of the described method.</p></li>
</ul>
<table>
<caption><span id="tab:result-extraction-dl-a47g33">Table 9.5: </span> Extracted users from the A47G33 patent set using the SVM and DL configurations. New users extracted by the system are reported in bold.</caption>
<tbody>
<tr class="odd">
<td align="center">Lemma</td>
<td align="center">Frequency</td>
<td align="center"># Patents</td>
<td align="center">Lemma</td>
<td align="center">Frequency</td>
<td align="center"># Patents</td>
</tr>
<tr class="even">
<td align="center">female</td>
<td align="center">801</td>
<td align="center">109</td>
<td align="center">child</td>
<td align="center">402</td>
<td align="center">102</td>
</tr>
<tr class="odd">
<td align="center">child</td>
<td align="center">426</td>
<td align="center">108</td>
<td align="center">cleregy member</td>
<td align="center">128</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">guy</td>
<td align="center">156</td>
<td align="center">17</td>
<td align="center">patient</td>
<td align="center">113</td>
<td align="center">11</td>
</tr>
<tr class="odd">
<td align="center">patient</td>
<td align="center">115</td>
<td align="center">11</td>
<td align="center">man</td>
<td align="center">50</td>
<td align="center">26</td>
</tr>
<tr class="even">
<td align="center">parent</td>
<td align="center">70</td>
<td align="center">31</td>
<td align="center">young</td>
<td align="center">48</td>
<td align="center">32</td>
</tr>
<tr class="odd">
<td align="center">man</td>
<td align="center">51</td>
<td align="center">26</td>
<td align="center"><strong>angel</strong></td>
<td align="center">29</td>
<td align="center">23</td>
</tr>
<tr class="even">
<td align="center">merchant</td>
<td align="center">50</td>
<td align="center">6</td>
<td align="center">dog</td>
<td align="center">20</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">soon</td>
<td align="center">46</td>
<td align="center">29</td>
<td align="center">artisan</td>
<td align="center">12</td>
<td align="center">12</td>
</tr>
<tr class="even">
<td align="center">engineer</td>
<td align="center">45</td>
<td align="center">45</td>
<td align="center"><strong>male/female</strong></td>
<td align="center">12</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="center">adult</td>
<td align="center">39</td>
<td align="center">23</td>
<td align="center">hockey player</td>
<td align="center">7</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">young</td>
<td align="center">35</td>
<td align="center">24</td>
<td align="center"><strong>professional</strong></td>
<td align="center">7</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">society</td>
<td align="center">32</td>
<td align="center">21</td>
<td align="center">tennis player</td>
<td align="center">7</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center"><strong>angel</strong></td>
<td align="center">29</td>
<td align="center">23</td>
<td align="center">football player</td>
<td align="center">6</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">fund raiser</td>
<td align="center">27</td>
<td align="center">4</td>
<td align="center"><strong>ghost</strong></td>
<td align="center">5</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">priest</td>
<td align="center">22</td>
<td align="center">4</td>
<td align="center">children</td>
<td align="center">5</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">cheerleader</td>
<td align="center">15</td>
<td align="center">4</td>
<td align="center">manager</td>
<td align="center">5</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center"><strong>fund-raiser</strong></td>
<td align="center">11</td>
<td align="center">4</td>
<td align="center"><strong>spider</strong></td>
<td align="center">5</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center"><strong>athlete</strong></td>
<td align="center">10</td>
<td align="center">9</td>
<td align="center"><strong>vandal</strong></td>
<td align="center">5</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><strong>ghost</strong></td>
<td align="center">5</td>
<td align="center">5</td>
<td align="center"><strong>athlete</strong></td>
<td align="center">4</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center"><strong>adulterant</strong></td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">mother</td>
<td align="center">4</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>jew</strong></td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">soccer player</td>
<td align="center">4</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center"><strong>maid</strong></td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">squirrel</td>
<td align="center">3</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>tourist</strong></td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center"><strong>maid</strong></td>
<td align="center">3</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"><strong>indian</strong></td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center"><strong>god</strong></td>
<td align="center">3</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>beginner</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>mariner</strong></td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center"><strong>christians</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>male-female</strong></td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>datum entry operator</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>manufacturer</strong></td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center"><strong>expert</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>jew</strong></td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><strong>jewish</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>merchandizers</strong></td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"><strong>marinaro</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>parishioner</strong></td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:result-extraction-dlw2v-a47g33">Table 9.6: </span> Extracted users from the A47G33 patent set using the SVM-W2V and MLP-W2V configurations. New users extracted by the system are reported in bold.</caption>
<tbody>
<tr class="odd">
<td align="center">Lemma</td>
<td align="center">Frequency</td>
<td align="center"># Patents</td>
<td align="center">Lemma</td>
<td align="center">Frequency</td>
<td align="center"># Patents</td>
</tr>
<tr class="even">
<td align="center">child</td>
<td align="center">152</td>
<td align="center">68</td>
<td align="center">clergy member</td>
<td align="center">124</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">clergy member</td>
<td align="center">124</td>
<td align="center">5</td>
<td align="center"><strong>crowd</strong></td>
<td align="center">36</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">man</td>
<td align="center">50</td>
<td align="center">26</td>
<td align="center">basketball player</td>
<td align="center">20</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">engineer</td>
<td align="center">45</td>
<td align="center">45</td>
<td align="center"><strong>him</strong></td>
<td align="center">17</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">young</td>
<td align="center">29</td>
<td align="center">24</td>
<td align="center">woman</td>
<td align="center">16</td>
<td align="center">8</td>
</tr>
<tr class="odd">
<td align="center"><strong>choir</strong></td>
<td align="center">17</td>
<td align="center">1</td>
<td align="center"><strong>saint</strong></td>
<td align="center">14</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>infirm</strong></td>
<td align="center">13</td>
<td align="center">8</td>
<td align="center"><strong>youth</strong></td>
<td align="center">14</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center"><strong>bride</strong></td>
<td align="center">9</td>
<td align="center">4</td>
<td align="center"><strong>angel</strong></td>
<td align="center">8</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center"><strong>volunteer</strong></td>
<td align="center">8</td>
<td align="center">6</td>
<td align="center"><strong>choir</strong></td>
<td align="center">8</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">musician</td>
<td align="center">6</td>
<td align="center">6</td>
<td align="center">musician</td>
<td align="center">6</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">boy</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center"><strong>god</strong></td>
<td align="center">5</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">children</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">children</td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">girl</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">guy</td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center"><strong>creature</strong></td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">infant</td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center"><strong>deceased</strong></td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">priest</td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center"><strong>jewish</strong></td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center"><strong>bride</strong></td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>person</strong></td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center"><strong>consumer</strong></td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">mother</td>
<td align="center">2</td>
<td align="center">2</td>
<td align="center"><strong>everyone</strong></td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>audience</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>him/her</strong></td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center"><strong>boyfriend</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>spectator</strong></td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>derby member</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">farmer</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"><strong>gift giver</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">youngster</td>
<td align="center">2</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center"><strong>handicapped</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>boyfriend</strong></td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center"><strong>jesus</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>grandparent</strong></td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center"><strong>saint</strong></td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center"><strong>subject</strong></td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">husband</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">clown</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">lady</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">husband</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">runner</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">runner</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">society</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">society</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">teenager</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">tennis player</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>The total number of users is 109. 28,2% (564 on 2.000) of patents in analysis contains at least one user. This result is an evidence of the fact that patents actually contain users information, and, considering the approach we followed, this percentage is an accurate lower approximation of the actual percentage of patents containing at least one user.</p>
<p>In figure <a href="methods.html#fig:patentsperuser">9.2</a> for each user on the x axes is shown the number of patents in which the user is contained. The distribution is skewed, with some occurrences showing large numbers and many others with just one or few occurrences. It is clear that there is a Pareto like distribution, with the first 20% of users covering 70% of total users in terms of occurrence. It means that some users are more likely to be cited in patents and many more users that rarely appear. Following this observations, we can divide users in three groups:</p>
<ul>
<li><p><em>Group A</em>: users that appear in more than 100 patents (5% of the patent set). In our case these are <em>male</em>, <em>child</em> and <em>female</em>.</p></li>
<li><p><em>Group B</em>: users that appear in more than 20 patents (1% of the patent set). This group is composed by 13 different users. Some of these are <em>engineer</em>, <em>person</em>, <em>player</em>, <em>adult</em>, <em>angel</em> and _guy.</p></li>
<li><p><em>Group C</em>: users that appear in less then 20 patents. This group is composed by 93 different users. Some of these are <em>mother</em>, <em>athlete</em>, <em>priest</em>, <em>adulterant</em>, <em>golfer</em> and <em>hockey player</em>.</p></li>
</ul>
<p>Further research means to study how these users differ from patent set to patent set. We expect to see similar distribution but with different content of users. Frequent and non-specific users comprise Group A: in other patent set we could see differences in terms of entities contained in this class but its content will stay non-specific. These results seem to be generic social roles indicating the gender or the age of a person. Group B is composed of mainly non-specific users and some specific users that change from patent set to patent set. This class helps to identify the core users of the patent set. Lastly, Group C contains non-frequent users that are both specific and non-specific, making it the most interesting of the three for the purposes of our work. In this group we find users that are market niches, so the patent that contains these users is of great interest for marketers and designers. These are both samples of the more generic users (for example a <em>mother</em> is a <em>female</em> and a <em>hockey player</em> is a <em>player</em>) or specific users of the patent-set (like <em>priest</em>, <em>fund-raiser</em>, <em>doll</em>, <em>spouse</em> and <em>clergy member</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:patentsperuser"></span>
<img src="_bookdown_files/figures/user_rank.png" alt="Process flow diagram of the proposed automatic user extraction system from patents. The diagram contains the representation of the documents and the operations performed on them. The process takes in input a patent set and a list of users and produces a list of new users as output." width="80%" />
<p class="caption">
Figure 9.2: Process flow diagram of the proposed automatic user extraction system from patents. The diagram contains the representation of the documents and the operations performed on them. The process takes in input a patent set and a list of users and produces a list of new users as output.
</p>
</div>
</div>
</div>
<div id="advdrwresults" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Advantages and Drawbacks</h3>
<p>In this section we show the approach used to extract the advantages and the drawbacks of the invention described in a patent. More precisely, the system identifies textual elements which represent the advantages or the drawbacks described in a patent. Advantages and drawbacks information can strongly advantage designers in the phase of new product development or in the process of product improvement and marketers in the phase of customer understanding and product placing. Furthermore a patent based method has a strong advantage with respected to methods that extracts information using other type of documents (e.g. online reviews of products <span class="citation">(Mirtalaie et al. <a href="#ref-monireh">2018</a>)</span>): patents anticipate availability of products on the market by a factor varying from 6 to 18 months <span class="citation">(Golzio <a href="#ref-golzio2012">2012</a>)</span>.</p>
<p>The proposed extraction process is shown in figure <a href="methods.html#fig:advdrwprocessgeneral">9.3</a> and its macro-phases are:</p>
<ol style="list-style-type: decimal">
<li><em>Advantage and Drawback Clues Collection</em>: in this section we described the method to collect a reasonable number of generic advantage and drawback clues;</li>
<li><em>Domain Clues Extraction</em>: in this section is shown how the generic clues are used in exploiting machine learning algorithm to extract new domain specific advantages and drawback clues;</li>
<li><em>Domain Clues Validation</em> : since the new clues are automatically extracted, the output of the extraction phase surely contains a certain degree of noise. To clean this output a validation tool based on tweeter sentiment analysis is developed;</li>
<li><em>Advantages and Drawbacks Extraction</em>: here the extracted clues (generic and domain specific) are expanded according to specific regular expression pattern in order to obtain the advantages and the drawbacks of the analyzed patent set.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:advdrwprocessgeneral"></span>
<img src="_bookdown_files/figures/General.pdf" alt="Main overview of the advantages and drawbacks extraction process from patents." width="80%" />
<p class="caption">
Figure 9.3: Main overview of the advantages and drawbacks extraction process from patents.
</p>
</div>
<p>Before the description of each phases, in section <a href="methods.html#advdisdef">9.1.2.1</a> the concepts of <em>advantages and disadvantages of an invention</em> are explained.</p>
<div id="advdisdef" class="section level4">
<h4><span class="header-section-number">9.1.2.1</span> Advantages and drawbacks: an engineering point of view</h4>
<p>An effective development of new products or the redesign of an existing one require the analysis of its positive and negative properties. Due to that, advantages and drawbacks of products are extremely valuable information for companies. Unfortunately, this information is not easy to obtain and manage: a strong evidence of that is the effort in the development of tools able to manage this information <span class="citation">(Pahl and Beitz <a href="#ref-pahl2013engineering">2013</a>, <span class="citation">Ulrich (<a href="#ref-ulrich2003product">2003</a>)</span>)</span>. Companies frequently make use of Quality Functional Deployment (QFD) and requisites lists, users’ needs, users’ requirements with the purpose of tracking advantages <span class="citation">(Carnevalli and Miguel <a href="#ref-carnevalli2008review">2008</a>)</span>. On the other hand, companies use Failure mode and effects analysis (FMEA) to gather and study drawbacks, failure modes and their effects and causes <span class="citation">(Liu, Liu, and Liu <a href="#ref-liu2013risk">2013</a>.)</span> However, product developers can acquire QFD and FMEA data only from the users of the invention: this leads to the need of expensive processes of customer’s voice listening whose results are often unclear. Moreover, this information is not disclosed to researchers since this is part of the company know-how.</p>
<p>The description of the brought advantages and solved drawbacks are critical requirements for the patentability of a product, as stated by the politics and the guidelines given by World Intellectual Property Organization (WIPO) on writing patents <span class="citation">(Organization <a href="#ref-world2004wipo">2004</a>)</span>. As stated by WIPO, an invention is a <em>solution</em> to a specific <em>problem</em> . The problem that an invention solves is a negative effect that state-of-the-art technologies can not fully overcome; on the other side, a solution is a way to solve this problem. A solution can lead to some advantages with respect to the known art. Thus, starting from the definition of invention, it is clear how it can be characterized by the advantages that it brings and by the problems that it solves. Also, it is reasonable to assume that having a clear picture of both advantages and drawbacks of a technology is important for an effective design process<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
</div>
<div id="clues-of-advantages-and-drawbacks-in-patents" class="section level4">
<h4><span class="header-section-number">9.1.2.2</span> Clues of Advantages and drawbacks in patents</h4>
<p>First of all we have to define the concept of clue to an advantage or a drawback. To describe with a certain degree of precision an advantage or a drawback, patent writers need to use sequences of words of a certain length. Since NER systems do not perform well on long sequence of tokens, we split the problem of extracting advantages and drawbacks in two parts: first we extract entities that are clues in the sequence of words that describes the advantage or the drawback; then we extract the surrounding words to collect the whole sequence that describes the advantage or the drawback.</p>
<p>To better understand these concepts some examples are:</p>
<ul>
<li><p><advantage_clue> ease </advantage_clue> of access</p></li>
<li><p>cook food <advantage_clue> quickly </advantage_clue> and <advantage_clue>economically</advantage_clue></p></li>
<li><p><advantage_clue>benefits</advantage_clue> of keeping an outdoor cooker lid fixed</p></li>
</ul>
<p>For the present work, we refer to advantages and drawbacks as a sequence of words of minimal length that express the advantage or the drawback. The three phrases of the example are three advantages. On the other hand clues are words that are likely to be contained in advantages or drawbacks phrases.</p>
</div>
<div id="adv-drw-article-sourc" class="section level4">
<h4><span class="header-section-number">9.1.2.3</span> Advantages and Drawbacks Clue Collection</h4>
<div class="figure" style="text-align: center"><span id="fig:advdrwarticlecluecollection"></span>
<img src="_bookdown_files/figures/pointer-collection-phase.pdf" alt="Main overview of the patent set annotation process." width="80%" />
<p class="caption">
Figure 9.4: Main overview of the patent set annotation process.
</p>
</div>
<p>The approaches to generate a knowledge base of clues were two. The first approach was based on a manual collection of clues of advantages and drawbacks directly from patent texts. This process was performed on 2,000 patents, randomly chosen from the freepatent database.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> With this approach we were able to collect 3,254 advantages and 5,142 drawback clues. Some examples of the extracted clues are shown in table <a href="methods.html#tab:advdrwarticleextdclue">9.7</a>.</p>
<table>
<caption><span id="tab:advdrwarticleextdclue">Table 9.7: </span> Examples of the clues collected with the first approach.</caption>
<thead>
<tr class="header">
<th align="center"><em>Advantages Clues</em></th>
<th align="center"><em>Drawbacks Clues</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">ability</td>
<td align="center">aggravated</td>
</tr>
<tr class="even">
<td align="center">efficacy</td>
<td align="center">breakage</td>
</tr>
<tr class="odd">
<td align="center">ensure</td>
<td align="center">damage</td>
</tr>
<tr class="even">
<td align="center">healthy</td>
<td align="center">defect</td>
</tr>
<tr class="odd">
<td align="center">innovative</td>
<td align="center">error</td>
</tr>
<tr class="even">
<td align="center">optimum</td>
<td align="center">improper</td>
</tr>
<tr class="odd">
<td align="center">protect</td>
<td align="center">leak</td>
</tr>
<tr class="even">
<td align="center">quick-release</td>
<td align="center">problem</td>
</tr>
<tr class="odd">
<td align="center">reinforce</td>
<td align="center">unavailable</td>
</tr>
<tr class="even">
<td align="center">securely</td>
<td align="center">wrong</td>
</tr>
</tbody>
</table>
<p>The second approach consisted in looking for alternative methods to indicate advantages or drawbacks clues, looking defined word patterns. The most relevant are the negations of advantages to obtain drawbacks, and the negation of drawbacks to obtain advantages. Some examples of such constructions are shown in table <a href="methods.html#tab:advdrwarticleexbaclue">9.8</a>.</p>
<table>
<caption><span id="tab:advdrwarticleexbaclue">Table 9.8: </span> Examples of the clues collected with the second approach.</caption>
<thead>
<tr class="header">
<th align="center"><em>Advantages Clues</em></th>
<th align="center"><em>Drawbacks Clues</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">non damaged</td>
<td align="center">out of</td>
</tr>
<tr class="even">
<td align="center">anti-corrosion</td>
<td align="center">in need of comfort</td>
</tr>
<tr class="odd">
<td align="center">loss reduction</td>
<td align="center">non user-friendly</td>
</tr>
<tr class="even">
<td align="center">prevent</td>
<td align="center">diminish comfort</td>
</tr>
<tr class="odd">
<td align="center">defect free</td>
<td align="center">issue with</td>
</tr>
<tr class="even">
<td align="center">reduce waste</td>
<td align="center">problem with</td>
</tr>
<tr class="odd">
<td align="center">reduction of</td>
<td align="center">lacks of</td>
</tr>
<tr class="even">
<td align="center">cost less</td>
<td align="center">lacks with</td>
</tr>
<tr class="odd">
<td align="center">less severe</td>
<td align="center">loss of</td>
</tr>
<tr class="even">
<td align="center">avoid disease</td>
<td align="center">unfacilitate</td>
</tr>
</tbody>
</table>
<p>At the end of this process, a total of 6.568 advantages and the 14.809 drawbacks formed the knowledge base for the system, and gave us a reasonable number of clues to be used in the next step of the process.</p>
<p>The first approach was restricted by the lists being extracted from a random and limited sample of patents. On the other side, the rules used in the second approach are non exhaustive, and this can create non-sense clues, due to all of the possible combinations of words. Anyway, it is reasonable to assume that a large set of non-domain dependent clues are collected and will be used in the next steps of the process.</p>
<p>It is important to underline that the list of advantages and drawbacks clues is designed to avoid linguistic ambiguities when projecting these entities on the corpus. For example <em>guide</em> has two different meanings<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> when used as a verb or as a noun: as a verb it means <em>“to assist something or someone to travel through, or reach a destination in, an unfamiliar area, as by accompanying or giving directions”</em>, so it could be the clue to an advantage; at the same time as a noun it assumes the mean of <em>“a book, pamphlet, etc., giving information, instructions, or advice; handbook”</em> thus indicating a product and not an advantage. Avoiding such ambiguities is a crucial aspect to produce an informative training set, so ambiguous words were avoided to be projected on patents.</p>
</div>
<div id="advdrwarticledomaincluecoll" class="section level4">
<h4><span class="header-section-number">9.1.2.4</span> Domain Clues Extraction</h4>
<p>The domain-specific clues collection process takes in input the automatically annotated patent set 1. The analyzed patent set is automatically annotated using the domain-independent clues, and is used to extract new domain-specific clues. We decided for the present methods to analyze the whole text of the patents and not to focus only on a specific section.</p>
<div class="figure" style="text-align: center"><span id="fig:advdrwarticleprocessfigclueexpansionphase"></span>
<img src="_bookdown_files/figures/pointer-expansion-phase.pdf" alt="Overview of the domain specific advantages and failures clues extraction process." width="80%" />
<p class="caption">
Figure 9.5: Overview of the domain specific advantages and failures clues extraction process.
</p>
</div>
<p>Our system resorts to state-of-the-art NLP tools which are part of the linguistic analysis pipeline shown in figure <a href="methods.html#fig:advdrwarticleprocessfigclueexpansionphase">9.5</a>. In addition we developed a specific advantages and drawbacks clues extraction tool, still based on Natural Language Processing techniques.</p>
<p>The automatic patent set annotation 2 process, as shown in figure <a href="methods.html#fig:advdrwarticleprocessfigclueexpansionphase">9.5</a>, is composed by a set of sequential steps. The first three steps are related to the linguistic annotation: sentence splitting and tokenization, part of speech tagging and lemmatization. Once these three steps are completed the entity extractor collects the advantages and drawbacks clues from the analyzed patents.</p>
<p><em>Sentence splitting and Tokenization</em> steps split the text into sentences and then segment each sentence in orthographic units called tokens.</p>
<p>The <em>Part-Of-Speech tagging</em> (or POS tagging) step assigns unambiguous grammatical categories to the tokens. For the present application we use the most recent version of the Felice-POS-tagger described in <span class="citation">(Dell’Orletta <a href="#ref-dell2009ensemble">2009</a>)</span>. Once the computation of the POS-tagged text is completed, the text is automatically <em>lemmatized</em> in order to group inflected forms of a word in a single item. Some of the following steps of the entire extraction process exploit the lemmatized texts in order to achieve better extraction results.</p>
<p>Successively the <em>semi-automatic annotation of advantages and drawbacks clues</em> is performed. The advantages and drawbacks clues extraction tool is the key ingredient of the present paper, and it is based on supervised methods. Such methods require an entity annotated corpus in order to extract new entities from unseen documents. Since the manual annotation of a patent set is too expensive both in terms of time and manual effort, we apply a semi-automatic method to generate an advantage and drawback annotated corpus.</p>
<p>The entity annotation schema for a single token is defined using a widely accepted BIO annotation scheme <span class="citation">(Ramshaw and Marcus <a href="#ref-ramshaw">1999</a>)</span>:</p>
<ul>
<li><strong>B-ADV</strong>: the token is the start of an entity representing an advantage clue;</li>
<li><strong>I-ADV</strong>: the token is the continuation of a sequence of tokens representing an advantage clue;</li>
<li><strong>B-DRW</strong>: the token is the start of an entity representing a drawback clue;</li>
<li><strong>I-DRW</strong>: the token is the continuation of a sequence of tokens representing a drawback clue;</li>
<li><strong>O</strong>: for the remaining case.</li>
</ul>
<p>The <em>Advantages and Drawbacks Clues Extractor</em> is a supervised classifier that, given an annotated patent set, is trained on these examples. The patent set is: (a) linguistically-annotated, using the steps described above; (b) entity-annotated, exploiting the semiautomatic annotation process executed in the previous steps. Given a set of features the classifier trains a statistical model using the feature statistics extracted from the corpus. This trained model is then employed in the classification of unseen patents: it extracts new domain specific clues from patents and assigns them a probability score whether they are an advantage or a drawback. In our experiments the classifier has been trained using the Support Vector Machines (SVM) learning algorithm using the LIBSVM <span class="citation">(Hearst, Dumais, Osman, et al. <a href="#ref-svm">1998</a>)</span> library configured to use a linear kernel. The classifier uses two different kinds of features that are extracted from the text:</p>
<ul>
<li><p><strong>raw features</strong>: prefix and suffix of the analyzed token; it works particularly well with advantages ending with -full -ious and with drawbacks starting with un- dis- etc..</p></li>
<li><p><strong>word2vec features</strong>: vector representations of words computed by the <em>word2vec</em> <span class="citation">(Mikolov et al. <a href="#ref-word2vec1">2013</a>)</span> tool.</p></li>
</ul>
<p>Table <a href="methods.html#tab:featconfsadvdis">9.9</a> reports the detailed features chosen for the proposed advantage and drawbacks clues extractor.</p>
<table>
<caption><span id="tab:featconfsadvdis">Table 9.9: </span> Context windows of the extracted features considering 0 as the current analyzed token.</caption>
<thead>
<tr class="header">
<th align="center"><em>Feature group</em></th>
<th align="center"><em>Context Window</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Prefixes up to 4</td>
<td align="center"><span class="math display">\[0\]</span></td>
</tr>
<tr class="even">
<td align="center">Suffixes up to 4</td>
<td align="center"><span class="math display">\[0\]</span></td>
</tr>
<tr class="odd">
<td align="center">Word2vec</td>
<td align="center"><span class="math display">\[-2, -1, 0, 1, 2\]</span></td>
</tr>
<tr class="even">
<td align="center">TAG</td>
<td align="center"><span class="math display">\[-1\]</span></td>
</tr>
</tbody>
</table>
<p>By introducing prefixes and suffixes of the analyzed token, the classifier is able to identify frequent orthographic patterns which allow to maximize the precision in classification phase. On the other hand, the <em>word2vec</em> features are introduced in order to maximize the recall, since semantically similar clues should have similar <em>word2vec</em> vectors. Finally, the tag of the previous token is added to the final feature vector in order to improve the accuracy classification of multi-word clues.</p>
<div id="word2vec-feature-computation" class="section level5 unnumbered">
<h5>Word2vec feature computation</h5>
<p>While contextual, linguistic and compositional features are commonly used for entity extraction task in patents, from a computational linguistic point of view the presented system introduces the novelty of using <em>word2vec</em> features for entity extraction in patents.</p>
<p><em>Word2vec</em> is a NLP tool able to produce word representations exploiting big corpora. The main property of the vectors produced by <em>word2vec</em> is that words that share similar contexts have similar vector representations. By using word vectors instead of the corresponding words we were able to overcome the problem of the limited lexical knowledge in the training phase.</p>
<p>To build our <em>word2vec</em> vectors we used the Skipgram model with a context window of 5 tokens. As reported in table <a href="methods.html#tab:word2vecpatents">9.10</a>, we used a corpus consisting of 48,194 different patents, containing more than 400,000,000 tokens.</p>
<p>The corpus was designed to contain patents belonging to different classes (12 in total) in order to acquire an extended knowledge of the contexts in which the words in general are surrounded. In addition, patents belonging to two of these classes are analyzed and, in the same section, detailed configurations of the entity extractor has been provided.</p>
<table>
<caption><span id="tab:word2vecpatents">Table 9.10: </span> Statistics of the documents on which the word2vec vectors have been learned. The patent sets of the analyzed case study are reported in bold.</caption>
<thead>
<tr class="header">
<th align="center">Patent class</th>
<th align="center"># Patents</th>
<th align="center"># Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>A47G33</strong></td>
<td align="center"><strong>2423</strong></td>
<td align="center"><strong>5.225.000</strong></td>
</tr>
<tr class="even">
<td align="center"><strong>A61G13</strong></td>
<td align="center"><strong>2991</strong></td>
<td align="center"><strong>15.937.000</strong></td>
</tr>
<tr class="odd">
<td align="center"><strong>A61G1</strong></td>
<td align="center"><strong>5040</strong></td>
<td align="center"><strong>36.348.000</strong></td>
</tr>
<tr class="even">
<td align="center"><strong>A61H</strong></td>
<td align="center"><strong>5199</strong></td>
<td align="center"><strong>41.831.000</strong></td>
</tr>
<tr class="odd">
<td align="center">A61P25/24</td>
<td align="center">5297</td>
<td align="center">103.098.000</td>
</tr>
<tr class="even">
<td align="center">A63F1</td>
<td align="center">5461</td>
<td align="center">75.900.000</td>
</tr>
<tr class="odd">
<td align="center">A63F3</td>
<td align="center">4923</td>
<td align="center">40.909.000</td>
</tr>
<tr class="even">
<td align="center">A63F7</td>
<td align="center">4747</td>
<td align="center">13.807.000</td>
</tr>
<tr class="odd">
<td align="center">E02B3</td>
<td align="center">3796</td>
<td align="center">14.434.000</td>
</tr>
<tr class="even">
<td align="center">E04H9</td>
<td align="center">2221</td>
<td align="center">12.500.000</td>
</tr>
<tr class="odd">
<td align="center">G01V11</td>
<td align="center">1345</td>
<td align="center">11.166.000</td>
</tr>
<tr class="even">
<td align="center">G08B13</td>
<td align="center">4831</td>
<td align="center">40.904.000</td>
</tr>
<tr class="odd">
<td align="center"><em>Total</em></td>
<td align="center">48194</td>
<td align="center">412.065.000</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="clue-validation-using-tweets-sentiment-analysis" class="section level4">
<h4><span class="header-section-number">9.1.2.5</span> Clue validation using tweets sentiment analysis}</h4>
<div class="figure" style="text-align: center"><span id="fig:advdrwarticleprocessfigcluevalidationphase"></span>
<img src="_bookdown_files/figures/pointer-validation-phase.pdf" alt="Overview of the domain specific advantages and failures clue validation process." width="80%" />
<p class="caption">
Figure 9.6: Overview of the domain specific advantages and failures clue validation process.
</p>
</div>
<p>Figure <a href="methods.html#fig:advdrwarticleprocessfigcluevalidationphase">9.6</a> gives an overview of the activities performed to validate the collected clues using twtitter. Since the manual review of the new domain specific clues can be very time consuming, an innovative approach to automatic validation of these entities is proposed. The approach is based on the assumption that advantages of technological innovations can be considered positive factors by the users. Conversely, the drawbacks of the artifacts are considered negative factors impacting on the satisfaction of the users. Both advantages and drawbacks are common terms or chunks of terms commonly used in other contexts, too. Therefore, if we can identify a wide source of sentences tagged with a polarity score and containing advantages or drawbacks, the probability of assigning the proper polarity to advantages and drawbacks increases.</p>
<p>Social media platforms provide powerful venues for consumers to interact not only with brands but also with other consumers as they engage in the processes of curation, creation, and collaboration <span class="citation">(Evans, Bratton, and McKee <a href="#ref-evans2010social">2010</a>)</span>. Such virtual platforms are places where users discuss about products, about their features but also about problems and failures they experienced during the daily use. The way they discuss or describe products or services is often unambiguous and highly polarized.</p>
<p>Our approach to the automatic validation of advantages and drawbacks exploits the information contained in the Twitter platform.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> More precisely, for each extracted advantage or drawback clue we collect a set of tweets in which the clue is mentioned. Once a significant number of tweets is collected (in our case 3,073,959 - around 2,738 per entity in average), they are analyzed by a sentiment classifier. The main idea behind this process is to assign to each clue a sentiment polarity score which should express the feeling of the user with respect to the considered clue on the social media.</p>
<p>The tweet collection can be easily performed by using the Twitter streaming API,<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> which is freely available. By assigning a polarity score to each clue, we expect to detect tagging anomalies: entities tagged as advantages by the classifier are expected to have a positive polarity in the extracted tweets. Vice versa, entities tagged as drawbacks by the classifier are expected to have a negative polarity in the extracted tweets.</p>
<div id="sentiment-classifier-features-classification-model-and-performance-evaluation" class="section level5 unnumbered">
<h5>Sentiment Classifier: features, classification model and performance evaluation</h5>
<p>In our sentiment classifier we focused on a wide set of features ranging across different levels of linguistic description. The whole set of features we started with is described below, organized into four main categories:</p>
<ul>
<li>raw and lexical text features</li>
<li>morpho-syntactic features</li>
<li>syntactic features</li>
<li>lexicon features.</li>
</ul>
<p>This proposed four-fold partition closely follows the different levels of linguistic analysis which is automatically carried out on the text being evaluated, (i.e. tokenization, lemmatization, morpho-syntactic tagging and dependency parsing) and the use of external lexical resources.</p>
<p>Raw and lexical text features are extracted considering the text available in the tweet. For this work we considered a number of tokens, character n-grams, word n-grams, lemma n-grams, char repetition sequences, mentions number, hashtags number and punctuation.</p>
<p>Morpho-syntactic and Syntactic Features consider the part of speech tags and the syntactic analysis of the text. Our sentiment analyzer extracts Part-Of-Speech n-grams (coarse and fine), coarse grained Part-Of-Speech distribution and syntactic dependency types n-grams.</p>
<p>To extract features based on lexicons we exploited three freely available resources. The Bing Liu Lexicon <span class="citation">(Hu and Liu <a href="#ref-hu2004mining">2004</a>)</span>, which includes approximately 6,000 English words, the Multi–Perspective Question Answering Subjectivity Lexicon <span class="citation">(Wilson, Wiebe, and Hoffmann <a href="#ref-wilson2005recognizing">2005</a>)</span>, which consists of approximately 8,200 English words and the SentiWordNet 3.0 Lexicon <span class="citation">(Baccianella, Esuli, and Sebastiani <a href="#ref-baccianella2010sentiwordnet">2010</a>)</span> which consists of more than 117,000 words. For each word in these lexicons the associated polarity is provided. In addition, we manually developed a lexicon of positive and negative emoticons, which usually is a strong indicator of tweets polarity. By exploiting the described resources, the following features were extracted: positive/negative emoticon distribution, sentiment polarity n-grams, sentiment polarity modifiers, the distribution of sentiment polarity, the most frequent sentiment polarity and changes of polarity in tweet sections. A more detailed description of these features is provided in [<span class="citation">Cimino et al. (<a href="#ref-cimino2014linguistically">2014</a>)</span>.</p>
<p>In order to assign a sentiment polarity score to each tweet, we employed an adapted version of the ItaliaNLP Sentiment Polarity Classifier for the English language <span class="citation">(Cimino et al. <a href="#ref-cimino2014linguistically">2014</a>)</span>. This classifier operates on morpho-syntactically tagged and dependency parsed texts and assigns to each document a score expressing its probability of belonging to a given polarity class. The highest score represents the most probable class. Given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus. This model is used in the classification of unseen documents. The set of features and the machine learning algorithm can be parametrized through a configuration file. For this work, we used a tandem Long Short Term Memory Recurrent Neural Network (LSTM) - Support Vector Machines (SVM) architecture.</p>
</div>
<div id="validation-of-the-extracted-clues" class="section level5 unnumbered">
<h5>Validation of the extracted clues</h5>
<p>The sentiment classifier is employed to validate the advantage and drawback clues which were previously extracted from patents by the clue extractor. In order to do so, we exploited the output of the tweet classifier on the tweets we previously downloaded. Each tweet, as said before, contains one or more clues to be validated. The sentiment classifier assigns the likeliness to each tweet to positive, neutral or negative. Consequently by analyzing all the tweets we previously downloaded, we obtained for each clue the distribution of positive, neutral and negative tweets.</p>
<p>Then to make a decision regarding each clue we used another SVM based classifier. This classifier is trained on a gold set of advantages and drawbacks clues: we manually labeled 344 words as advantages and 193 words as drawbacks, obtaining the gold set. The features used by this classifier are a number of positive, neutral and negative tweets extracted in which the entities are mentioned and the <em>word2vec</em> vector representing the considered entity. In table <a href="methods.html#tab:sentiment-validation">9.11</a> the classification results of the proposed approach over a 5-fold cross validation are reported. The obtained results show that the proposed entity validation method is suitable for the automatic advantages/drawbacks clues validation process.</p>
<table>
<caption><span id="tab:sentiment-validation">Table 9.11: </span> Classification results of the proposed validation method over a 5-fold validation.</caption>
<thead>
<tr class="header">
<th align="center"><em>Method</em></th>
<th align="center">Global accuracy</th>
<th align="center"><em>ADV Prec.</em></th>
<th align="center"><em>ADV Rec.</em></th>
<th align="center"><em>ADV F1</em></th>
<th align="center"><em>DRW Prec.</em></th>
<th align="center"><em>DRW Rec.</em></th>
<th align="center"><em>DRW F1</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">SVM-W2V</td>
<td align="center">87.71</td>
<td align="center">89.89</td>
<td align="center">91.29</td>
<td align="center">90.57</td>
<td align="center">83.35</td>
<td align="center">80.66</td>
<td align="center">81.92</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="advantages-and-drawbacks-sentences-extraction" class="section level4">
<h4><span class="header-section-number">9.1.2.6</span> Advantages and Drawbacks sentences extraction</h4>
<div class="figure" style="text-align: center"><span id="fig:advantagedrawbacksextraction"></span>
<img src="_bookdown_files/figures/relevant-sentences-extraction.pdf" alt="Overview of the advantages and failures extraction process." width="80%" />
<p class="caption">
Figure 9.7: Overview of the advantages and failures extraction process.
</p>
</div>
<p>The advantages and drawbacks extraction process is shown in figure <a href="methods.html#fig:advantagedrawbacksextraction">9.7</a>. Once all the domain specific advantages and drawbacks clues are extracted, these are merged with the ones belonging to the original knowledge base, obtaining a final list which will be processed by the advantages and drawbacks sentences extractor. The advantages and drawbacks sentences extractor exploits predefined linguistic and clues filters which operate on the automatic pos–tagged patents. Specifically, for each advantage and drawback term identified in patents, we used a pos–clue-pattern constraining the start-token and the rest of the token pos. Since we were interested in phrases containing words belonging to specific morphological categories, we identified sequences of allowed pos–clue-pattern in order to cover most of the English morphosyntactic multi–words structures, using the following pattern:</p>
<p>(ADVClue|textbar DISClue)+Noun.*Noun.*Noun.</p>
<p>The pattern is applied to the previously lemmatized text in order to have less sparse and more informative extractions.</p>
<p>This choice was made because the pattern:</p>
<ol style="list-style-type: decimal">
<li>expresses an advantage or a drawback exhaustively;</li>
<li>increases the precision and the recall of the final output list of advantages and drawbacks;</li>
<li>allows to build a three-level named based tree over the final output list.</li>
</ol>
<p>In particular, the tree is built by grouping terms which share at the first level the same clue, at the second level the same noun and at the third level the same noun. This grouping procedure allows to easily represent the final output list in a tree structure which can be easily navigated by the end user of the system.</p>
</div>
<div id="results-case-study" class="section level4">
<h4><span class="header-section-number">9.1.2.7</span> Results: case study</h4>
<p>In this section we describe the experimental use of the proposed process by applying it on four different patent sets.</p>
<p>To test the proposed methodology, we chose 4 patent sets composed of a sample of 3,000 patents each. The patent sets belong to 4 different IPC patent classes. The chosen classes and the definitions given by WIPO are reported in table <a href="methods.html#tab:advdrwarticleexampleipcclasses">9.12</a>.</p>
<table>
<caption><span id="tab:advdrwarticleexampleipcclasses">Table 9.12: </span> The patent IPC classes from which samples of 3,000 patents were chosen for the experimental analysis.</caption>
<thead>
<tr class="header">
<th align="center"><em>IPC name</em></th>
<th align="center"><em>Definition</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">A61G13</td>
<td align="center">Operating tables and auxiliary appliances therefor</td>
</tr>
<tr class="even">
<td align="center">A61H</td>
<td align="center">Physical therapy apparatus</td>
</tr>
<tr class="odd">
<td align="center">A61C15</td>
<td align="center">Devices for cleaning between the teeth</td>
</tr>
<tr class="even">
<td align="center">A47J37</td>
<td align="center">Baking; Roasting; Grilling; Frying</td>
</tr>
</tbody>
</table>
<p>Our choice of patent sets aimed at challenging our system to find new domain specific advantages and drawback clues in different domains. Furthermore, we only selected patent sets from the IPC class A, which is based on human necessities, to maximize the probability of finding advantages and drawbacks that impacts on the users and not only on other products/components.</p>
<p>Once the advantages and drawbacks are extracted, a manual review process was performed on the output of the system to compute the number of true positive clues. In this way we were able to compute the precision of the process for both the advantages and the drawbacks. The output of the clue extraction validated by the clue validator is analyzed in table <a href="methods.html#tab:advdrwarticlecluemeas">9.13</a>.</p>
<p>The table shows that the number of the extracted true positive advantage clues is higher than the number of the extracted true positive drawback clues. On the other side, the automatic evaluation process has a lower performance on advantage clues in terms of precision.</p>
<p>A first hypotheses to explain these results is that our knowledge base contained more drawback clues than advantage clues. Another possible reason could be that the applicant is minded to describe the invention highlighting the positive effects of the invention.</p>
<table>
<caption><span id="tab:advdrwarticlecluemeas">Table 9.13: </span> Number of clues filtered with the clue validator and number of true positive clues.</caption>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"># <em>Advantage clues</em></th>
<th align="center"># <em>Drawbacks clues</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><em>Tot extracted clues</em></td>
<td align="center">3607</td>
<td align="center">1244</td>
</tr>
<tr class="even">
<td align="center"><em>Automatically Validated clues</em></td>
<td align="center">1976</td>
<td align="center">576</td>
</tr>
<tr class="odd">
<td align="center"><em>True Positive</em></td>
<td align="center">984</td>
<td align="center">448</td>
</tr>
<tr class="even">
<td align="center"><em>Precision</em></td>
<td align="center">49.8%</td>
<td align="center">77.8%</td>
</tr>
</tbody>
</table>
<p>In order to assess the performance of the overall process, another important measure is the amount of new information that is obtained, which we call <em>information gain</em>. As shown in table <a href="methods.html#tab:infogain1">9.14</a>, the percentage of new discovered clues decreases with the number of starting clues. Obviously, the more patent sets are analyzed, the less new generic and domain specific clues are extracted. The percentage of information gain (represented as delta in the table), stabilizes at a 5% value in the advantage clues case, and 1% in the drawback clues case. This trend could be an evidence that the clue extraction process has a natural saturation level.</p>
<table>
<caption><span id="tab:infogain1">Table 9.14: </span> Information gained by applying the extraction process on different patent sets. Each row reports the percentage of information gained by incrementally adding the extracted entities to the knowledge base and the overall number of entities belonging to the extended knowledge base.</caption>
<thead>
<tr class="header">
<th align="center"><em>Patent set</em></th>
<th align="center">∆ Adv.</th>
<th align="center"># Adv.</th>
<th align="center">∆ Draw.</th>
<th align="center"># Draw.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Knowledge Base</td>
<td align="center">N/A</td>
<td align="center">6,568</td>
<td align="center">N/A</td>
<td align="center">14,809</td>
</tr>
<tr class="even">
<td align="center">A47J33</td>
<td align="center">+23%</td>
<td align="center">8,133</td>
<td align="center">+3%</td>
<td align="center">15,332</td>
</tr>
<tr class="odd">
<td align="center">A61C15</td>
<td align="center">+12%</td>
<td align="center">9,178</td>
<td align="center">+2%</td>
<td align="center">15,644</td>
</tr>
<tr class="even">
<td align="center">A61G13</td>
<td align="center">+5%</td>
<td align="center">9,653</td>
<td align="center">+1%</td>
<td align="center">15,849</td>
</tr>
<tr class="odd">
<td align="center">A61H</td>
<td align="center">+5%</td>
<td align="center">10,175</td>
<td align="center">+1%</td>
<td align="center">16,053</td>
</tr>
</tbody>
</table>
<p>Tables <a href="methods.html#tab:advantageclueextracted">9.15</a> and <a href="methods.html#tab:drawbacksclueextracted">9.16</a> show the frequencies of a randomly chosen set of the new extracted advantages and drawbacks domain specific clues for each of the four analyzed patent sets. The results show that the domain specific clues clearly characterize the different technical areas of the patent sets. It is thus interesting to notice how valuable information is contained in the context specific clues them self. Further research can decide to stop here the process, without extracting the whole sentence.</p>
<table>
<caption><span id="tab:advantageclueextracted">Table 9.15: </span> Extracted domain specific advantages clues with the measures of occurrences for each patent set.</caption>
<colgroup>
<col width="18%" />
<col width="27%" />
<col width="26%" />
<col width="28%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">A47J37 (Baking)</td>
<td align="center">A61H (Therapy apparatus)</td>
<td align="center">A61C15 (Teeth cleaning)</td>
<td align="center">A61G13 (Operating tables)</td>
</tr>
<tr class="even">
<td align="center">transport 295</td>
<td align="center">regenerative 144</td>
<td align="center">elasticity 495</td>
<td align="center">rigidity 784</td>
</tr>
<tr class="odd">
<td align="center">integrity 246</td>
<td align="center">waterproof 101</td>
<td align="center">rigidity 461</td>
<td align="center">ventilation 177</td>
</tr>
<tr class="even">
<td align="center">rigidity 233</td>
<td align="center">hygienic 85</td>
<td align="center">disinfection 247</td>
<td align="center">hygiene 135</td>
</tr>
<tr class="odd">
<td align="center">insure 180</td>
<td align="center">ergonomically 77</td>
<td align="center">precision 199</td>
<td align="center">versatility 121</td>
</tr>
<tr class="even">
<td align="center">adjusting 164</td>
<td align="center">disinfection 48</td>
<td align="center">ergonomically 108</td>
<td align="center">reliably 113</td>
</tr>
<tr class="odd">
<td align="center">unobstructed 73</td>
<td align="center">prevent excessive 39</td>
<td align="center">economically 105</td>
<td align="center">disinfection 56</td>
</tr>
<tr class="even">
<td align="center">uniformity 64</td>
<td align="center">hemodynamics 25</td>
<td align="center">waterproof 81</td>
<td align="center">humidification 48</td>
</tr>
<tr class="odd">
<td align="center">sensitivity 44</td>
<td align="center">prophylaxis 22</td>
<td align="center">hygienically 33</td>
<td align="center">ergonomically 39</td>
</tr>
<tr class="even">
<td align="center">hygienic 30</td>
<td align="center">prevent slippage 21</td>
<td align="center">quick-connect 26</td>
<td align="center">sanitation 20</td>
</tr>
<tr class="odd">
<td align="center">selectively 34</td>
<td align="center">smoothly 15</td>
<td align="center">sanitation 24</td>
<td align="center">non-invasive 12</td>
</tr>
</tbody>
</table>
<table>
<caption><span id="tab:drawbacksclueextracted">Table 9.16: </span> Extracted domain specific drawbacks clues with the measures of occurrences for each patent set.</caption>
<colgroup>
<col width="22%" />
<col width="25%" />
<col width="24%" />
<col width="26%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">A47J37 (Baking)</td>
<td align="center">A61H (Therapy apparatus)</td>
<td align="center">A61C15 (Teeth cleaning)</td>
<td align="center">A61G13 (Operating tables)</td>
</tr>
<tr class="even">
<td align="center">accidental 61</td>
<td align="center">infection 595</td>
<td align="center">infection 446</td>
<td align="center">syndrome 134</td>
</tr>
<tr class="odd">
<td align="center">burnt 59</td>
<td align="center">trauma 378</td>
<td align="center">inconvenience 126</td>
<td align="center">costly 72</td>
</tr>
<tr class="even">
<td align="center">malfunctioning 12</td>
<td align="center">abrasion 106</td>
<td align="center">irregularity 40</td>
<td align="center">claustrophobia 38</td>
</tr>
<tr class="odd">
<td align="center">time-consuming 8</td>
<td align="center">fragmentation 37</td>
<td align="center">pathogen 14</td>
<td align="center">malfunctioning 36</td>
</tr>
<tr class="even">
<td align="center">non-compliant 6</td>
<td align="center">paralysis 17</td>
<td align="center">infected 8</td>
<td align="center">unnecessarily 27</td>
</tr>
<tr class="odd">
<td align="center">dirty 6</td>
<td align="center">hematoma 15</td>
<td align="center">unintentionally 6</td>
<td align="center">discoloration 19</td>
</tr>
<tr class="even">
<td align="center">ignites 4</td>
<td align="center">uncomfortably 8</td>
<td align="center">abnormal 6</td>
<td align="center">hyperglycemia 11</td>
</tr>
<tr class="odd">
<td align="center">turbulence 4</td>
<td align="center">undetectable 8</td>
<td align="center">burn 4</td>
<td align="center">unavoidable 10</td>
</tr>
<tr class="even">
<td align="center">cross-contamination 3</td>
<td align="center">embarrassment 8</td>
<td align="center">toxic 3</td>
<td align="center">not linearly 8</td>
</tr>
<tr class="odd">
<td align="center">violently 3</td>
<td align="center">discoloration 8</td>
<td align="center">erosive 3</td>
<td align="center">catastrophic 6</td>
</tr>
</tbody>
</table>
<p>Then, after the re-projection of the extracted clues on the text, the regular expression described in section is used to extract the sentences highlighted by the clues.</p>
<p>The number of advantages and drawbacks sentences extracted from each patent are shown in table <a href="methods.html#tab:advantagedrawbackssentences">9.17</a>. The table shows that the occurrence of sentences describing an advantage is higher than the ones containing a drawback <a href="methods.html#tab:infogain1">9.14</a>. This result may be due to the fact that the applicant is minded to describe the invention by highlighting the positive effects of the invention.</p>
<table>
<caption><span id="tab:advantagedrawbackssentences">Table 9.17: </span> Number of sentences containing advantages and drawbacks for each analyzed patent set.</caption>
<thead>
<tr class="header">
<th align="center"><em>Patent class</em></th>
<th align="center"># <em>Advantage sentences</em></th>
<th align="center"># <em>Drawbacks sentences</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><em>A61G13</em></td>
<td align="center">7,836</td>
<td align="center">1,048</td>
</tr>
<tr class="even">
<td align="center"><em>A61H</em></td>
<td align="center">10,879</td>
<td align="center">1,463</td>
</tr>
<tr class="odd">
<td align="center"><em>A61C15</em></td>
<td align="center">9,551</td>
<td align="center">1,572</td>
</tr>
<tr class="even">
<td align="center"><em>A47J37</em></td>
<td align="center">9,973</td>
<td align="center">1,662</td>
</tr>
<tr class="odd">
<td align="center"><em>Total</em></td>
<td align="center">38,239</td>
<td align="center">5,745</td>
</tr>
</tbody>
</table>
<p>Figure <a href="methods.html#fig:processimagetest1">9.8</a> and <a href="methods.html#fig:processimagetest2">9.9</a> show two subsets of the taxonomies obtained by the extraction of the advantages and drawbacks for each of the four analyzed patent sets. The two figures respectively refers to a subset of the leaves linked to the advantage clue <em>Improve</em> and a subset of the leaves linked to the drawback clue _Damage. In both cases an additional trimming action is performed by removing those branches or leaves containing terms belonging to the stop-word list typical of patent lexicon (e.g. claim, embodiment, invention, comprise, figure, etc..). The figure shows that our process can extract highly informative sentences also starting from generic and non-contextual clues like <em>improve</em> or <em>damage</em>. Moreover the words that follow the generic clues are specific of the technical field of the analyzed products. Both the results are promising for future applications, especially for the design fields. In particular figure <a href="methods.html#fig:processimagetest1">9.8</a> allows designers to focus on the positive side of the effects provided by the product and to better meet the explicit and implicit user needs. Similarly, figure <a href="methods.html#fig:processimagetest2">9.9</a> helps designers to redesign of the product in a proactive way, to keep attention to the critical issues identified by the drawbacks and to conceive possible corrective actions to solve such drawbacks.</p>
<div class="figure" style="text-align: center"><span id="fig:processimagetest1"></span>
<img src="_bookdown_files/figures/improve.png" alt="Sample of the tree based taxonomy extracted from the analyzed patent sets. The sample contains some of the leaves linked to the advantage clue Improve" width="80%" />
<p class="caption">
Figure 9.8: Sample of the tree based taxonomy extracted from the analyzed patent sets. The sample contains some of the leaves linked to the advantage clue Improve
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:processimagetest2"></span>
<img src="_bookdown_files/figures/damage.png" alt="Sample of the tree based taxonomy extracted from the analyzed patent sets. The sample contains some of the leaves linked to the drawback clue Damage. " width="80%" />
<p class="caption">
Figure 9.9: Sample of the tree based taxonomy extracted from the analyzed patent sets. The sample contains some of the leaves linked to the drawback clue Damage.
</p>
</div>
</div>
</div>
<div id="trademakrs" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Trademakrs</h3>
<p>The market interest for both patents and trademarks has increased during the last decades, with a significant increase in filing for both types of Intellectual Property. The academia also took attention to patent data and trademark data, but with different and (almost) disconnected research approaches. Furthermore, the analysis of patents to study R&amp;D is predominant while less attention has been paid to trademarks <span class="citation">(Griliches <a href="#ref-griliches1981market">1981</a>)</span>. Indeed, very few are the works where patents and trademarks are investigated together and not generically cited together as parts of the intellectual property right framework. Moreover cultural differences exist between United States and Europe (at least) and practical consequences can be observed in daily life: for example, even if Europe had an earlier and more enduring interest in trademarking than the US the presence of the symbols ® and ™ in the product labels are more common in US than in Europe <span class="citation">(MERCER, Lopes, and DUGUID <a href="#ref-mercer2010reading">2010</a>)</span>. The difference exists also from an industrial perspective, across different sectors <span class="citation">(Baroncelli, Fink, and Smarzynska <a href="#ref-baroncelli2004global">2004</a>)</span>. Indeed, trademarks have their larger use worldwide in the R&amp;D intensive scientific equipment, pharmaceuticals sector and advertising intensive manufacturing industries (clothing, footwear, detergents and food products).</p>
<p>The present chapter studies the usage (if any) of <em>trademarks</em> and <em>trade symbols</em> within patent texts. The aim is to investigate possible indicators about IP strategies and innovation output utilizing information about the interlink between patents and trademarks. An additional goal is to understand if patent applicants makes a right use of trademarks and trade symbols when writing a patent document. The analysis clearly stands at the boundary between patent and trademark research areas, and since such intermediate territory has not been investigated in depth, there are several research questions we would like to answer. The starting point is a sheer numerical evidence: we have found out that, at the date of 01/09/2017, a total of 2.162.962 patents worldwide (for details about the database and its coverage see section 3) contain a trademark symbol. It is therefore a not negligible phenomenon, worth of investigation and that should provide interesting insights about the mechanisms and strategies that companies adopt at the frontier between technical innovation and marketing.</p>
<div id="the-use-of-trademark-symbols-and-in-patents-from-a-legal-perspective" class="section level4">
<h4><span class="header-section-number">9.1.3.1</span> The use of trademark symbols ™ and ® in patents from a legal perspective</h4>
<p>It is evident from our preliminary analysis that there exist an high number of patent documents containing a trademark indication. In the present section we investigate what is the legal perspective of this usage.</p>
<p>In <span class="citation">(Butler <a href="#ref-butler1969rules">1969</a>)</span> the authors published a series of rules to standardize the use of trade terms in patents. They also specifies different rules in the case of use of trade terms in specifications and the use of trade terms in patent claims. The rules are:</p>
<p>1- A trade term is properly used in a specification if those skilled in the art can make the product designated by the trade term at the time the application is filed, using the specification and/or published literature that is implicated by the specification. 2- A trade term is also properly used in a specification if the product is generally known to persons skilled in the art and is readily obtainable at the time the application is filed, provided the composition of the product is a trade secret and there is reason to believe that whenever the composition of the product is modified the trade term will also be changed. 3- A trade term is also properly used in a specification if it designates a component of the embodiment which is not essential to the invention. 4- A trade term can be used in a claim only if its meaning has been adequately defined in the specifications, whereby it imparts specific limitations to the claim.”</p>
<p>The second rule is particularly interesting because it seems preventing early patenting or patenting before having produced and used a trade name. Rule number two is even more interesting since it contains a case of a product that is a trade secret used to build another product subject to patent application.</p>
<p>Similar guidelines are contained in the European patent convention <span class="citation">(Hall and Helmers <a href="#ref-hall2018impact">2018</a>)</span>. Here is state that is not desirable to use trademarks or trade names if such words merely denote origin or where they may relate to a range of different products. Anyway trademarks could be used in patent application to satisfy art.83, that states that the application shall disclose the invention in a manner sufficiently clear and complete for it to be carried out by a person skilled in the art. In this case the product must be sufficiently identified, without reliance upon the word. A special case are such words that have become internationally accepted as standard descriptive terms and have acquired a precise meaning (e.g. “Bowden” cable, “Belleville” washer, “Panhard” rod, “caterpillar” belt). In this case they may be allowed without further identification of the product to which they relate. It is clear specifications afflicts the analysis that we want to carry out. Always in the same document are given also guidelines for the usage of trademarks in claims. These are, as expected, different from the specifications. For claims we have the problem that it may not be guaranteed that the product or feature referred to is not modified while maintaining its name during the term of the patent. They may be allowed exceptionally if their use is unavoidable and they are generally recognised as having a precise meaning. It is the applicant’s responsibility then to ensure that registered trademarks are acknowledged as such in the description. From that we can deduce that the presence of a trademark in patents and more specifically in claims decreases the reproducibility of the invention and thus the quality of the patent. Also the the US patent legislation <span class="citation">(Jaffe <a href="#ref-jaffe2000us">2000</a>)</span> focuses on the use of trademarks in patents claims. Here the presence of a trademark or trade name in a claim is not considered improper but the examiner should analyze the claim to determine how the mark or name is used. In fact, the trademark or trade name should identify a source of goods, and not the goods themselves. In this case the claim scope is uncertain since the trademark or trade name cannot be used properly to identify any particular material or product. In fact, the value of a trademark would be lost to the extent that it became descriptive of a product, rather than used as an identification of a source or origin of a product. Thus, the use of a trademark or trade name in a claim to identify or describe a material or product would not only render a claim indefinite, but would also constitute an improper use of the trademark or trade name.</p>
<p>Finally, in <span class="citation">(Pressman and Stim <a href="#ref-pressman2018nolo">2018</a>)</span> the authors gives some guidelines on how to introduce trademarks when it is unavoidable. First, the trademark should be capitalized and used as an adjective (not a noun), followed by the generic name of the product or service. Furthermore when referring to the trademark there should also be a reference to the trademark owner.</p>
</div>
<div id="finding-tradenames-from-patents" class="section level4">
<h4><span class="header-section-number">9.1.3.2</span> Finding Tradenames from Patents</h4>
<p>With respect to Users @ref(#usersresults) and to Advantages and Distadvantages @ref(#advdrwresults) the process to extract tradenames from patents is trivial. The main activity performed are (for details aobut the activities see section <a href="sota.html#sotatools">8.1</a>:</p>
<ul>
<li>Sentence splitting</li>
<li>Tokenization</li>
<li>Part-of-speech tagging</li>
<li>Lemmatization</li>
</ul>
<p>After that is is possibile to identify tradenames searching for the symbols ™ and ®.</p>
</div>
<div id="trademarks-and-ipc-classes" class="section level4">
<h4><span class="header-section-number">9.1.3.3</span> Trademarks and IPC Classes</h4>
<p>The first investigation has the aim of understanding if the ™ and ® symbols has different content depending on different IPC <span class="citation">(Organization <a href="#ref-wipo1">1971</a>)</span> classes. The IPC classes are: The main evidence in figure <a href="methods.html#fig:histtm">9.10</a> is that ® is used more than ™ in patent, with and average percent of presence of 7.4% and 5.6% respectively. The reason of this evidence could be that patents contains unregistered trademarks that are never converted to registered trademarks; another reason is the confusion between the ® and ™ symbol, usually considered as synonymous. Furthermore the writer will use the symbol only if he/she knows that the mark is protected, otherwise he/she will not indicate any symbols or will use ™ or ® without checking the right use. Another possible problem is that the word has become of common use and thus usually used without symbols. Furthemore, classes A (Human Necessities) and C (Chemistry; Metallurgy) clearly has more trade symbols than the other classes.</p>
<div class="figure" style="text-align: center"><span id="fig:histtm"></span>
<img src="_bookdown_files/figures/hist_tm.png" alt="Histogram of the percentage of ® and ™ in patentes for each IPC class (limited to US patents). A (Human Necessities); B (Performing Operations; Transporting); C (Chemistry; Metallurgy); D (Textiles; Paper); E (Fixed Constructions); F (Mechanical Engineering; Lighting; Heating; Weapons; Blasting); G (Physics); H (Electricity)." width="80%" />
<p class="caption">
Figure 9.10: Histogram of the percentage of ® and ™ in patentes for each IPC class (limited to US patents). A (Human Necessities); B (Performing Operations; Transporting); C (Chemistry; Metallurgy); D (Textiles; Paper); E (Fixed Constructions); F (Mechanical Engineering; Lighting; Heating; Weapons; Blasting); G (Physics); H (Electricity).
</p>
</div>
<p>Secondly, from table <a href="methods.html#tab:ipcclassestm">9.18</a> it is evident that the average ratio is 6.4 for ® (the usage is 6.4 times higher in the year span 2007-2014 than the span 1995-2002) and 5.4 for ™ (the usage is 5.4 times higher in the year span 2007-2014 than the span 1995-2002) thus the usage of trade-symbols in patent is growing and the presence of ® is increasing faster than the presence of ™. From the present results is not possible to say if this effect is due to an higher quality of the process of patent application (and thus an higher awareness of applicant and examiner) or of a positive trend in trademark registration. In particular for the IPC class E (fields constructions) the number of ® and ™ has increased 10.5 and 9.0 times respectively. This could be an evidence of the fact that the number of trademark in the field of fixed constructions is increasing.</p>
<table>
<caption><span id="tab:ipcclassestm">Table 9.18: </span> For each IPC class the ratio of the percentage of patents containing at least one ® or ™ character is computed for two patents sets 1995 to 2002 and from 2007 to 2014.</caption>
<thead>
<tr class="header">
<th align="left">IPC Classes</th>
<th align="left">Ratio ® 2007-2014 over ® 1995-2002  </th>
<th align="left">Ratio ™ 2007-2014 over ™ 1995-2002  </th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="left">5.9</td>
<td align="left">5.0</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="left">6.2</td>
<td align="left">5.4</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="left">5.9</td>
<td align="left">5.2</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="left">5.6</td>
<td align="left">5.7</td>
</tr>
<tr class="odd">
<td align="left">E</td>
<td align="left">10.5</td>
<td align="left">9.0</td>
</tr>
<tr class="even">
<td align="left">F</td>
<td align="left">7.5</td>
<td align="left">6.2</td>
</tr>
<tr class="odd">
<td align="left">G</td>
<td align="left">4.6</td>
<td align="left">3.2</td>
</tr>
<tr class="even">
<td align="left">H</td>
<td align="left">5.1</td>
<td align="left">3.2</td>
</tr>
<tr class="odd">
<td align="left">Average</td>
<td align="left">6.4</td>
<td align="left">5.4</td>
</tr>
</tbody>
</table>
</div>
<div id="the-selected-tradenames" class="section level4">
<h4><span class="header-section-number">9.1.3.4</span> The Selected Tradenames</h4>
<p>We a set of popular tradenames referring to <span class="citation">(Morris <a href="#ref-morris2016trademarks">2016</a>)</span>. Then we filtered the ambigouus ones (the one that can refer to surnames), because this can create an onverstimation of the number of patent citing it witouth the tradenames. The total namber of tradenames is 38 and these are:</p>
<ul>
<li>amazon®</li>
<li>blackberry®</li>
<li>bose®</li>
<li>budweiser®</li>
<li>chiquita®</li>
<li>chrome®</li>
<li>coca-cola®</li>
<li>ebay®</li>
<li>facebook®</li>
<li>fender®</li>
<li>firefox®</li>
<li>gibson®</li>
<li>gillette®</li>
<li>heineken®</li>
<li>ibanez®</li>
<li>intel®</li>
<li>iphone®</li>
<li>kellog®</li>
<li>kodak®</li>
<li>lego®</li>
<li>marlboro®</li>
<li>matlab®</li>
<li>mcdonald’s®</li>
<li>nitinol®</li>
<li>nutella®</li>
<li>nylon®</li>
<li>photoshop®</li>
<li>polaroid®</li>
<li>post-it®</li>
<li>powerpoint®</li>
<li>prozac®</li>
<li>sennheiser®</li>
<li>spotify®</li>
<li>teflon®</li>
<li>velcro®</li>
<li>whatsapp®</li>
<li>xanax®</li>
</ul>
<p>In figure <a href="methods.html#fig:tmlistcpmplete">9.11</a> are shown the number of patent containing the tradenames with and withouht ™ and ®; furthermore the information is divided for all the years, from 1995 to 2002 and from 2007 to 2014. In the previous analysis we noticed that ® is generally more used than ™. Now the goal is to understand if patent writers uses tradesymbols or not when citing tradenames, if there are any differences between different tradenames and if we can notice different behaviours in recent years. From figure <a href="methods.html#fig:tmlistcpmplete">9.11</a> is evident that despite the fact that it is mandatory to use ™ o ®, these symbols are not always used.</p>
<div class="figure" style="text-align: center"><span id="fig:tmlistcpmplete"></span>
<img src="_bookdown_files/figures/table_all_tradenames.png" alt="Number of patents citing one of the 38 selected tradenames. The data are divided for any years, from 1995 to 2002 and from 2007 to 2014." width="80%" />
<p class="caption">
Figure 9.11: Number of patents citing one of the 38 selected tradenames. The data are divided for any years, from 1995 to 2002 and from 2007 to 2014.
</p>
</div>
</div>
<div id="tradenames-usage-in-patents" class="section level4">
<h4><span class="header-section-number">9.1.3.5</span> Tradenames Usage in Patents</h4>
<p>It is relevant to understand if there exists any difference between different tradenames in the correct usage of trade symbols (a trade name has been cited in the patent texts with the trade symbol). Figure <a href="methods.html#fig:tmcitedwell">9.12</a> illustrates in a bi-logaritmic plot the distribution of trade names correctly cited in patents with ® and ™ (Y-axis) versus the same trade names cited without using the proper symbol. Most of trade names are located under the diagonal. Those around the diagonal are those we can considered as well known trademarks and not so subject to genericization phenomena (green area). Conversely on the right we can find a read area where the ratio is even more shifted toward the absence of any indication of protected trademark. In the middle the orange area where many of the most famous trademarks fall down and that could be in phase of generalisation or conversely, since the trademark of a product is totally entangled with the owner (Iphone-Apple; Intel), inventors do not feel the reason to cite them as a trademark. A remark on Figure <a href="methods.html#fig:tmcitedwell">9.12</a> is necessary: the plot presents a correct measurement of trademarks with ® and ™ plotted in the y axis, while the search for trademark with missing trademark symbols in some case can introduce undesired errors and include other meaning of the word. Take for example the case of Blackberry®: the datum is referring to mobile phones and accessories or software applications. In all the cases where ® or ™ are correctly written, it is probably true that the inventor is referring to the Blackberry® mobile phones, but machines for jam or juice production extracted out of the blackberry introduce false positive examples. Similarly Fender is probably located too far on the right because it is also a part of a car, bike, motorbike, boat, etc.. and Gibson could introduce citations of works performed by someone called Gibson, therefore if cleaned results are necessary the searching strategies for the trademarks without symbols have to be refined.</p>
<div class="figure" style="text-align: center"><span id="fig:tmcitedwell"></span>
<img src="_bookdown_files/figures/TrendsMarks.001.jpeg" alt="Plot of the trade names correctly cited in patents with ® and ™ on the y-axis and the same trade names cited without using the proper symbol on the x-axes. Both axes are on a logaritmic scale." width="80%" />
<p class="caption">
Figure 9.12: Plot of the trade names correctly cited in patents with ® and ™ on the y-axis and the same trade names cited without using the proper symbol on the x-axes. Both axes are on a logaritmic scale.
</p>
</div>
</div>
<div id="the-popularity-of-tradenames-in-patens" class="section level4">
<h4><span class="header-section-number">9.1.3.6</span> The Popularity of Tradenames in Patens</h4>
<p>In figure <a href="methods.html#fig:populartm">9.13</a> are plotted the trends of the numbers of patents that correctly cites a tradenames. We anlyzed 12 different tradenames, divided in couples: each couple belong to a similar sector. We make use of a generalized additive model fitting function to better analyze the results and to make more easily to understand and compare the two element of each pair.</p>
<p>Each pair has been chosen to compare trademarks in homogeneous markets. Some of them shows:</p>
<ul>
<li>(d, e) similar trends but different incidence;</li>
<li>(a, c) dissimilar behaviours (Amazon vs Ebay, Blackberry vs. Iphone: linear vs. exponential);</li>
<li><ol start="6" style="list-style-type: lower-alpha">
<li>similar results (e.g. Gibson vs Fender in the music market have almost the same behaviour and a reduced presence in patents)</li>
</ol></li>
<li>similar behaviour but shifted in time (e.g. Firefox vs Chrome).</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:populartm"></span>
<img src="_bookdown_files/figures/Marks_Years.jpeg" alt="Plot of the trade names correctly cited in patents from 2000 to 2014." width="80%" />
<p class="caption">
Figure 9.13: Plot of the trade names correctly cited in patents from 2000 to 2014.
</p>
</div>
</div>
<div id="the-similarity-of-tradenames-in-patens" class="section level4">
<h4><span class="header-section-number">9.1.3.7</span> The Similarity of Tradenames in Patens</h4>
<p>Further informations can be derived by analysing the IPC classes where the patent have been chosen by the assignees-attorneys-examiners.The choice of a class is not only matter of market segment but rather of market use. Moreover it is necessary to remark that from a formal point of view the IPC classes form a feature vector, denoted y ∈ N639. IPC classes form a complete dataset of 639 elements of such feature vector. Each tradename is represented in such a feature vector where the number of patents in each class constitutes the length of the vector along that direction (feature). Our training data is therefore D = (x,y,z), where x∈[1, .., 12], y∈[1, .., 639], and z represents the patent numerosity z∈[0..15*106]. This dataset contains information about how z varies as a function of x and y. The matrix is quite empty since each product/brand addresses needs of specific market/s, after cleaning of the totally empty columns the matrix reduces to the dimensions of 12x479 and demonstrates a correct choice of the trademarks since they covered almost 75% of the IPC space.</p>
<p>With such a dataset a series of computations can be performed: here we show the results of the correlation analysis among the chosen trademarks to analyse their relative positioning on the invention landscape (not only the market); Correlation analysis is shown in Figure <a href="methods.html#fig:similartm">9.14</a> The matrix is triangular owing to the symmetry of the relationship. The main evidences are the following:</p>
<div class="figure" style="text-align: center"><span id="fig:similartm"></span>
<img src="_bookdown_files/figures/correlation_matrix_tm.png" alt="Heat-map of the IPC-based similiraties between trademarks." width="80%" />
<p class="caption">
Figure 9.14: Heat-map of the IPC-based similiraties between trademarks.
</p>
</div>
</div>
</div>
</div>
<div id="papers" class="section level2">
<h2><span class="header-section-number">9.2</span> Papers</h2>
<p>There are fields (mature technologies) where patents anticipate papers, while in others (basic research) the opposite happens. For this reason, scientific literature is the place where companies and scholars gather information about the problems that researchers are facing in the development of new technologies. Anyway, standard approaches of knowledge extraction from papers requires skilled personnel, they are time consuming and lacks in reproducibility. Furthemore, the volume of scientific literature has grown rapidly raising an imminent question about how to extract knowledge from this source. On the other hand, this tehcnical knowledge if managed, can be used to answer questions that 10 years ago would need the help of domain experts to be answered. In the present section is shown how text mining techniques can help to unswer some of this questions. In particular, it is an essential problem when different classification codes are used in order to organize scientific knowledge on a specific domain, becaues a specific categorization in a certain scientific field is missing. This leads to unnecessary complications in the researchers’ aims who want to quickly and easily find literature on a specific topic among the large amount of scientific publications, or want to effectivelly position a new research. Text mining techniques, and in particular topic modelling, can help scholars in solving this problems, if properly used togheter with domain expertise.</p>
<p>In this section we present two methodologies capable of automatically segmenting a knowledge field. These selected knowledge field are: sustainable manufacturing and block-chain. The results of the methodologies are described, togheter with example of applications.</p>
<div id="sustainable-manufacturing" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Sustainable Manufacturing</h3>
<p>One of the most important objectives of Sustainable Manufacturing (SM) is developing innovative and viable engineered materials, manufacturing processes and systems to provide multiple life-cycle of products. In SM the old concept “from cradle to grave” is now transforming into “from cradle to cradle” <span class="citation">(Jawahir and Bradley <a href="#ref-jawahir2016technological">2016</a>)</span>, tending toward multiple product life-cycles or even a “near-perpetual” product/material life. Scientific contributions in the sustainable manufacturing field mostly deals with energy and resource consumption. In this respect, two different main fields of causes can be identified: the process level and the material efficiency one. As a matter of facts, manufacturing processes have a significant role also in putting in place material efficiency strategies <span class="citation">(Ingarao <a href="#ref-ingarao2017manufacturing">2017</a>)</span>. As far as the processes are concerned, a first classification of research contributions was discussed in the CIRP General Assembly <span class="citation">(Duflou et al. <a href="#ref-duflou2012towards">2012</a>)</span>. There the authors state that research in manufacturing field, oriented to environmental impact reduction, can be clustered in 5 main sub-classes:</p>
<ol style="list-style-type: decimal">
<li>unit process level (Individual device or machine tool in the manufacturing system)</li>
<li>manufacturing system level,</li>
<li>facility,</li>
<li>multi-factory system up to considering the whole</li>
<li>supply chain level.</li>
</ol>
<p>Another review paper was presented at the ASME international manufacturing science and engineering conference <span class="citation">(Haapala et al. <a href="#ref-haapala2013review">2013</a>)</span>. In that paper the authors scrutinize the research papers focusing more on the differentiation between manufacturing processes and manufacturing system. Considering the process level, Ingarao <span class="citation">(Ingarao <a href="#ref-ingarao2017manufacturing">2017</a>)</span> clustered the scientific papers in 4 subsections: 1. effect of process parameters 2. role of machine tool architecture and related technology 3. applied process 4. manufacturing approach selection.</p>
<p>As concerns material efficiency options, a framework is presented by <span class="citation">(Allwood <a href="#ref-allwood2014squaring">2014</a>)</span>. The authors there provide strategies within three main classes corresponding to principles: reduce, reuse and recycling. Within each class guidelines for material efficiency practices are detailed. As concerns material efficiency options, a framework is presented by Allwood <span class="citation">(Allwood <a href="#ref-allwood2014squaring">2014</a>)</span>. There the authors provide strategies within three main classes (i.e. principle): reduce, reuse and recycling. Within each classes guideline for material efficiency practices are detailed. A good framework of all the possible reuses of materials is provided by Coopper <span class="citation">(Cooper and Allwood <a href="#ref-cooper2012reusing">2012</a>)</span>. In this research the authors identify four main reuse strategies for metals: Remanufacture, Reshape (applying metal shaping processes, additive, subtractive, mass conserving) to obtain a new geometry, Relocate: (recovering component and applying little refurbishment, components reused in the same type of products), Cascade: recovering component and use it in another less demanding use (downgrading). The role of manufacturing processes in putting in place material efficiency/reuse strategy was also outlined by Ingarao <span class="citation">(Ingarao <a href="#ref-ingarao2017manufacturing">2017</a>)</span>. In fact, manufacturing processes deserve to be considered as means for enabling material efficiency strategies. SM may be pursued by several strategies, such as re-designing and/or even changing manufacturing practices to conceive new-generation products, as well as by creating a closed-loop of environmentally-friendly material flow.</p>
<div id="the-6r-framework" class="section level4">
<h4><span class="header-section-number">9.2.1.1</span> The 6R Framework</h4>
<p>All the life-cycle stages of the product development (namely, design, production, use and post-use) should be considered carefully, with a particular attention to the design phase. This represents the real difference introduced by sustainable manufacturing concept with respect to traditional manufacturing, to lean manufacturing or even to green manufacturing, precursors of the SM. In a word, SM paradigm embraces those principles belonging to the 6R framework, namely: Reduce, Reuse, Recycle, Recover, Redesign and Remanufacture. The latter three principles where not included into the previous 3R framework. To a certain extent, it appears that the better definition of sustainability for any manufacturing operation provided so far is the extent to these principles are applied. This statement justifies because in the search to find a common rationale behind the mess of SM applications, one possible way is to find common roots in the principles that inspired the same applications: namely principles, which allows to the decision maker either an effective way to search for new sustainable solutions or to a certain extent measure the “degree of sustainability”. The higher the number of principles that are satisfied, the higher the potential positive impact on sustainability can be for a given adopted solution. In this sense, far from stating that the 6R framework is a widespread and commonly adopted metric of SM, the starting assumption in this paper is that 6R is the only approach, to the author’s knowledge, that provides a criteria of classification and selection of solution, and thus indirectly to provide a clear definition of what a SM application may look like. The true question nowadays is <em>if the 6R framework may capture the essence of SM</em>, provided that it is really critical to clearly define the SM paradigm rationale to the scientific community. This paper aims to stimulate the reflection from the Italian perspective to this point, with a particular concern to the production technology side, by using an automatic classification of papers within the 6R framework and then by benchmarking approaches followed by the Italian Technologist research-network SOSTNERE on SM issues.</p>
<p>Sustainability (from sustain plus ability) refers to the set of properties of a given system (either natural or artificial), which allows the same system to maintain itself for an almost indefinite period of time. This concept was officially introduced in a document of the World Commission on Environment and Development (WCED) entitled “Our Common Future”, where the Sustainable Development was defined as follows: <em>“Sustainable development is development that meets the needs of the present without compromising the ability of future generations to meet their own needs”</em><span class="citation">(WCED <a href="#ref-wced1987world">1987</a>)</span>. Starting from this general definition, further conceptualizations have been produced for manufacturing activities and/or processes, as here briefly recalled. According to the Organization for Economic Co-operation and Development, Sustainable manufacturing is a formal name for an exciting new way of doing business and creating value. Different statements of sustainability in literature<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> share the same focus on the following three aspects: economy, environment and society. It is thus possible to summarize from the literature analyzed a possible definition for sustainable manufactured processes/products, according to the following set of prescriptions:</p>
<ul>
<li>(-) minimize business risk;</li>
<li>(-) minimize negative environmental impacts;</li>
<li>(-) conserve energy and natural resources;</li>
<li>(-) are safe for employees, communities and consumers;<br />
</li>
<li>(-) are economically sound;</li>
<li>(+) a new way of creating value;</li>
<li>(+) are socially and creatively rewarding for all working people;</li>
<li>(+) providing access to basic services, green and decent jobs and a better quality of life for all ;</li>
<li>(+) adopt sustainable infrastructures.</li>
</ul>
<p>where the (-) sign stands for those prescriptions oriented to preservation of resources without any significant change of the present condition, while the (+) sign indicates those prescriptions aiming at ameliorating/modification of the trends with respect to traditionally manufactured processes/products. To summarize simplifying, sustainable manufacturing is all about minimizing business risks of any manufacturing operation while maximizing the new opportunities that arise from improving processes and products.<br />
Fascinating in principle, these general statements are really difficult to deploy into real operations and production settings. The focus of the present analysis is to derive a clear definition of the concept of manufacturing sustainability based on clear and paper based evidences, rather than referring to general ethical or social principles, which appear to be a <strong>top down</strong> definition. Conversely, information extracted from all the scientific papers available on SM can provide a sort of <strong>bottom up</strong> statement. Evidences are relevant words and short concepts that can be related to sustainability, as it will be explained in the following paragraphs by also providing some sound examples from the italian point of view.</p>
</div>
<div id="smtextdrivenbottomup" class="section level4">
<h4><span class="header-section-number">9.2.1.2</span> Sustainable Manufacturing: A text driven bottom-up definition</h4>
<p>In this section, we describe the process involved to analyse the papers in the sustainable manufacturing field. To give a more detailed vision of the topic, we consider this field divided in 6 sub-classes (one for each principle), adopting the well know 6R framework. The goal of the process is both to identify which the topics in each of the 6R are, then to accurately measure the way this framework corresponds to the topics of sustainable manufacturing. As seen in the flowchart of figure 1 the main activities of the process are four: Paper manual classification, Automatic Keywords Extraction, Keywords manual selection and Clustering. Each activity will be described in the next subsection.</p>
<div id="assignment-of-6r-principle-meanings" class="section level5 unnumbered">
<h5>Assignment of 6R principle meanings</h5>
<p>The first critical issue is the selection criteria of 6R principles for the assignment. This was done by trying to assign a semantic specification of each R principle, as below explicated, built by summarizing all the possible definitions and concepts extracted by the selected bibliography (partially cited in the present paper). The semantic specification provided below refers to the functional scope of each class, intended as an activity to be performed.</p>
<div class="figure" style="text-align: center"><span id="fig:wfsm"></span>
<img src="_bookdown_files/figures/workflow_sm.png" alt="Flowchart of the 6R papers analysis process. The white boxes represent the activities and the yellow boxes the predicted documents. In the figure are also highlighted the input and the main outputs of the process." width="80%" />
<p class="caption">
Figure 9.15: Flowchart of the 6R papers analysis process. The white boxes represent the activities and the yellow boxes the predicted documents. In the figure are also highlighted the input and the main outputs of the process.
</p>
</div>
<p><em>Reuse</em></p>
<p>Reuse is the action or practice of using something again, whether for its original purpose (conventional reuse) or to fulfill a different function (creative reuse or repurposing). Reuse involves taking, but not reprocessing, previously used items in order to save time, money, energy, and resources. In particular reuse is useful for machineries, especially when they are expensive, and for their components. Besides, technologies can be also reused and readapted to different situations. In addition, reuse is important because it minimizes disposal needs and costs.</p>
<p><em>Reduce</em></p>
<p>In pre-manufacturing it is possible to implement the reduction minimizing the use of resources. During the production phase, it refers to the use of energy and material, for example, it may involve the use of lower cost materials, the elimination of unnecessary product characteristics, the reduction of overhead costs or the readjustment of product processes. It implies a decrease of costs. As a consequence, the derived effect of this principle is to minimize and optimize performance in terms of cost, time and waste and it focuses primarily on the first three stages of the four life cycle of the product before recalled (§1).</p>
<p><em>Recycle</em></p>
<p>Recycle involves the process of converting material that would otherwise be considered waste, into new products and it corresponds to the breaking down of used items to make raw materials for the manufacture of new goods. Recycling can prevent the waste of potentially useful materials and reduce the consumption of fresh raw materials, thereby decreasing: energy usage, air pollution (from incineration) and water pollution (from landfilling). Recyclable materials include many kinds of glass, paper and cardboard, metal, plastic, tires, textiles and electronics.</p>
<p><em>Recover</em></p>
<p>Product recovery operations refer to operational processes (e.g. disassembling, sorting and cleaning) on products at the end of their use, in order to make them usable in n subsequent life-cycles. Differently from recycling, the product life-cycle is shortened, skipping all that phases of retreatment of wastes up to their second use. The risks of implementing product recovery operations is the uncertainty of returned product quality and quantity and for this reason recovering activities have only recently been considered in manufacture.</p>
<p><em>Redesign</em></p>
<p>The redesign activity involves the act of redesigning the next generation of products, which would use components, materials and resources recovered from the previous life-cycle or previous generation of products. It refers to the evaluation of ideas turning them into concrete innovative products obtained from products in their post-use phase.</p>
<p><em>Remanufacture</em></p>
<p>Remanufacture involves the re-processing of used products, to restore them to their original like-new state through the reuse of as many parts as possible without loss of functionality. It includes and exceeds the activity of recovering, because a remanufactured product should match the same customer expectation as a new one.</p>
<p>Despite this initial classification, some problem occurred for experts in assigning the papers to the different classes, due to the absence of the formalization that is needed to define unique criteria of belonging. This fact brought to the unbalance of the number of words within each class (corresponding to each principle), that drove to the fuzziness of the training set and, as a consequence, the difficulty in performing a real classification of scientific papers. This is the first critical issue of the followed approach so far, that may be overcome by alignment sessions of the decision makers.</p>
</div>
<div id="automatic-text-analysis-and-keywords-extraction" class="section level5 unnumbered">
<h5>Automatic Text analysis and Keywords extraction</h5>
<p>In order to analyze the information coming from scientific papers belonging to topics encompassed within the 6R framework, we exploit text mining techniques, which employ methods from different fields of data mining to extract meaningful information 8. The second activity in our analysis process is thus devoted to transforming the set of papers in a set of numeric vectors to be elaborated by the clustering algorithm. To this aim, some text mining techniques are applied in sequence 12 to automatically extract meaningful information and knowledge from unstructured texts. The text mining process performed is summarized in the following. First, the information content of the document is converted into a structured form (vector space representation). In fact, most of text mining techniques are based on the idea that a document can be faithfully represented by the set of words contained in it (bag-of-words representation 9). According to this representation, each document j of a collection of documents is represented as an M-dimensional vector, where M is the number of words defined in the document collection, and w(tji) specifies the weight of the word ti in document j. The simplest weighting method assigns a binary value to w(tji), thus indicating the absence or the presence of the word ti, while other methods assign a real value to w(tji).</p>
<p>In the following, the text mining steps performed are described (for details aobut the activities see section <a href="sota.html#sotatools">8.1</a>:</p>
<ul>
<li><em>Tokenization</em> is the first step of our text mining process, and consists in transforming a string of characters into a string of processing units called tokens that could be syllables, words, or phrases. Typically, with tokenization other operations are performed with the aim of making the text cleaner. Such operations are the removal of punctuation and other non-text characters, and the normalization of symbols (e.g., accents, apostrophes, hyphens, tabs and spaces). In the proposed system, the tokenizer removes all punctuation marks and splits each text into tokens corresponding to words, bigrams and trigrams.</li>
<li><em>Stop-word filtering</em> consists in eliminating the words which provide little or no information to the text analysis, or that, in our case, could make the clustering process fuzzier. Common stop-words belong to certain part of speech classes, such as articles, conjunctions, prepositions, pronouns, etc. Other stop-words are those that typically appear very often in sentences of the considered language (language-specific stop-words), or in the set of texts being analysed (domain-specific stop-words). In our case, this second group consists of typical words of the scientific articles such as “paper”, “state-of-the-art”, “present work”.</li>
<li><em>Stemming</em> is the process of reducing each word (i.e., token) to its root form. The purpose of this step is to group words with the same theme having closely related semantics. In the proposed system, the stemmer exploits the Snowball Stemmer for the English language, based on the Porter’s algorithm.</li>
<li><em>Feature representation</em> consists in building, for each text, the corresponding vector of numeric features. Indeed, in order to cluster the texts, we have to represent them in the same feature space. In particular, we consider the F- dimensional set of features corresponding to the set of relevant stems.</li>
</ul>
</div>
<div id="manual-keywords-selection" class="section level5 unnumbered">
<h5>Manual keywords selection</h5>
<p>Keyword selection procedure was performed by hand from a team of 4 experts in the scientific field of sustainable manufacturing and with consolidated expertise on research and innovation. In order to prevent misunderstandings and difference in the judgement, a preliminary analysis of the single attitudes to classification was statistically performed and misalignment were eliminated by a guided training, based on the contemporary evaluation of a same sample from different experts so as to align the judgement. This process was done, as above mentioned, based on the 6R explicit framework and sharing the meaning of each principle.</p>
<p>It is clear that assignment of keywords to classes is not a trivial task and susceptible of interpretation. Disambiguation process would be required to explain the criteria of affinity to a given class, hopefully by referring to the functional scope of each class. Still an ambiguity may result, which can hardly be removed without a more profound specification with appropriate examples, which is out of the scope of the present paper.</p>
<p>According to this procedure the 6R refer to the following keywords:</p>
<ul>
<li>RECOVER: recovery, planning processes, renewable resources, new approach, waste recovery; process scrap recovery; energy recovery; heat recovery; collection; separation; design for environment; material recover optimization; recovery logistics;</li>
<li>RECYCLE: recycle, adhesive technologies, materials conversions, government regulation; (Advanced) recycling technologies; Recyclability; End of first life; Down cycle; New/Old process scraps; Material scraps; Landfill taxes; Recycling benefit awarding; Secondary material production; Embodied energy saving; Environmental legislation</li>
<li>REDESIGN: machining, cutting, lubrication, environment technologies, cryogenic; Material-efficient design; Design for Environment; New Materials; Eco-friendly design; Eco-friendly materials; Material reduction; Light- weighting; Design for Disassembly</li>
<li>REDUCE: reduce, optimize, waste minimization, consumption, energy payback, pollution, reduction, emission, saving; Energy reduction; Waste reduction; Resource reduction; Material usage reduction; Process sustainability optimization; Energy efficiency; Heat efficiency; Manufacturing efficiency; Doing with less;</li>
<li>REMANUFACTURING: remanufacturing, material processing, eco-efficiency, renewable, innovations, nanocomposites; Product renewal; Product upgrade; Reconditioning; Part replacement; Modularity; Disassembly; Inspection; Separation; Re-Assembly; Design for Remanufacturing</li>
<li>REUSE: reuse, replacing, material flow analysis, eco-efficiency; Component reuse; product re-conditioning; product upgrade; non-destructive recycling; reuse supply chain; product maintenance; product repair; product monitoring;</li>
</ul>
<p>It is clear that the keyword extraction is not a trivial task, since the outcome of the process may range from a useless single word to a meaningful group of combined words that, on the other hand, could become too specific to be significant for the training set of the search engines.</p>
</div>
<div id="paper-clustering" class="section level5 unnumbered">
<h5>Paper Clustering</h5>
<p>Document clustering is the application of the general process of cluster analysis to texts. The practical applications of document clustering systems are several and vary from automatic document organization to automatic topic extraction. For further details see section <a href="sota.html#sotatoolsmodelnetanal">8.1.5.4</a>. Independently from the kind of algorithm, before the clustering phase, each document has to be represented as a set of features. These features are typically the n-grams contained in documents, so a critical activity for clustering effectiveness is the n-grams extraction (or feature selection). This goal is achieved with a series of sub-activity. These are typically tokenization (the process of parsing text data into smaller units), stemming and lemmatization (reducing all tokens to its semantic base), removing stop-words (less important words), and finally computing term frequencies (or other measures of the relationship between documents and words).</p>
<p>To get an exploratory view of the degree of precision with which the 6R framework represents the papers in analysis, the manually classified documents were clustered using a clustering Spherical K-Means Clustering algorithm <span class="citation">(Buchta et al. <a href="#ref-buchta2012spherical">2012</a>)</span>. We will then compare the output of the algorithm with the manual classification in the following paragraph.</p>
<p>Spherical k-means exploit cosine dissimilarities to perform prototype-based partitioning of term weight representations of the documents. Prototypes are centroids defined in the same feature space of the documents. The aim of the algorithm is to minimize, given a set of objects (documents in our case) and prototypes described in the same features space (the selected keywords) the cosine distance between each element and the closest prototype. This implies that the output of the algorithm is a membership matrix, in which each document is assigned to a certain prototype.</p>
</div>
</div>
<div id="results-1" class="section level4">
<h4><span class="header-section-number">9.2.1.3</span> Results</h4>
<p>In the present section, we describe the output of the application of the methodology described in section <a href="methods.html#smtextdrivenbottomup">9.2.1.2</a> on the 339 selected scientific papers on sustainable manufacturing extracted by the query “sustainable manufacturing” in the paper search field of the SCOPUS® database. Accordingly, apart the deliberate selection of the source database, no other filter was applied in the journal selection and this guarantees the significance of the journal sample selected. The main output, as highlighted in Figure 1, is the distribution of documents among the 6R principles (i.e. classes) as manually classified, the automatic keywords representations of the classes and the outputs of the clustering algorithm (which is an unsupervised assignment of the documents) compared to the manual classification.</p>
<p>The assignment of a paper to a class was made by recognizing the application, or tools or scope of the paper to one 6R’s principles. In this sense, for instance, a paper belonging to a recover might have as scope, or keywords or approach or an activity performed with the principal task to assure the recovery of a give good. It is thus clear, from the beginning, how complex may be to recognize just one principle more than a multiple possibility to satisfy more principles. This fact will be discussed in the followings.</p>
<div id="distributions-of-documents-among-the-6r" class="section level5 unnumbered">
<h5>Distributions of documents among the 6R</h5>
<p>The main point coming out from the histogram in Figure <a href="methods.html#fig:smhistogram">9.16</a> is that the distribution of scientific papers assigned to the 6R principles (classes) is not homogeneous. If the four classes Reduce, Recycle, Redesign and Remanufacture collect a comparable number of papers, the other two (Reuse, Recover) are poorly represented. One possible explanation of this is that cases of reuses or recoveries are less generalizable compared to other “R”, and thus potentially less interesting for conferences and journals. Recovery and reuse, in fact, often refers to specific functions embedded into the product, and finding a general rule for assessing or designing such processes might be more difficult. They rather might find room in technical magazines, not included in the present analysis as concern processes. It worth noting that reuse and recover here refers to product and not explicitly to materials. Different approach concerns the design for reuse, which to a certain extent would belong to remanufacturing, as in the most of papers discussed, to mention a few, in the Annals of CIRP 18,19,20. It is difficult to think that the classification criteria, which plays a critical role into the assignment to classes, contributed to this situation, depending on the meaning assigned to each principle. Whether a different combination of classed have been provided, a different histogram would be expected to appear.</p>
<div class="figure" style="text-align: center"><span id="fig:smhistogram"></span>
<img src="_bookdown_files/figures/sm_histogram.png" alt="Histogram of the number of papers manually assigned to each of the 6R principles." width="80%" />
<p class="caption">
Figure 9.16: Histogram of the number of papers manually assigned to each of the 6R principles.
</p>
</div>
</div>
<div id="keyword-vector-representation-of-6r" class="section level5 unnumbered">
<h5>Keyword vector representation of 6R</h5>
<p>After the phases of automatic keyword extraction and manual keyword selection, we derived the paper/selected keywords matrix. A sample of the elements of this matrix (keyword and their occurrency) is:</p>
<ul>
<li><em>RECOVER</em>: Life cycle 5, environmental impact 4, raw material 3, cycle assessement 3, tossii fuel 3, energy payback 2, cleaner products 2, energy cost 2, energy usage 2, pv System 2, product process 2, managements System 2, new technologies 2, energy save 2, climate change 2, risk assessements 2</li>
<li><em>RECYCLE</em>:lite cycle 40, environmental impact 31, waste management 30, resource conservation recycle 25, solid waste 21, lite cycle assessement 18, recycle material 14, recycle process 12, reuse recycle 11, electronic equipement 11, global warm 9, waste disposai 9, heavy metal 8, environment friendly 8, waste treatment 8, waste stream 8</li>
<li><em>REDESIGN</em>: manufacturing process 20, lite cycle assess 19, sustainable manufacturing 17, machine tool 14, naturai gas 14, cutting tool 13, raw material 13, tool life 13, cutting fluid 13, tool wear 13, mechanical engineering 12, System boundaries 11, machining process 11, manufacturing System 11, cutting speed 11, cutting zone 10</li>
<li><em>REDUCE</em>: life cycle 40, environment impact 38, energy consumption 31, life cycle assessment 28, energy usage 22, energy requirements 18, solar energy 15, cleaner product 13, global warm 13, climate changes 12, solid waste 12, gas emission 11, solar celi 11, cutting conditions 11, co2 emissions 10, environmental management 10</li>
<li><em>REMANUFACTORING</em>: sustainable manufacturing 28, manufacturing process 22, remanufacture product 18, supply chain 16, new product development 16, climate change 12, business model 12, product remanufacturing 12, manufacturing System 11, environmental performances 10, remanufacturing industry 10, management System 9, waste generation 9, design for environment 9, remanufacture and recycle 8, automotive industry 7</li>
<li><em>REUSE</em>: life cycle 7, environment impact 7, energy usage 4, mechanical engineering 4, industriai ecology 4, cleaner product 3, life cycle assessement 3, environment management 3, design for manufacturing 3, environment protection agency 3, waste management 3, united nation 3, environment management System 2, process design 2, sustainable manufacturing 2, green chemistry 2</li>
</ul>
<p>The keywords were divided according to the 6R principles (i.e. class), listing the number of the paper belonging to each class.The sample list of keywords and their occurrency, reflects the distribution of figure <a href="methods.html#fig:smhistogram">9.16</a> and cannot thus be significant for classification but only for clustering purposes at the present.</p>
</div>
<div id="clustersclasses-confusion-matrix" class="section level5 unnumbered">
<h5>Clusters/classes confusion matrix</h5>
<p>To analyze from a semantic dimension how well 6R framework represent sustainable manufacturing definition through the scientific papers considered, the manually classified documents were clustered using a clustering Spherical K-Means Clustering algorithm. The results (the assignments of each paper to a cluster) was then compared with the manual classification. The number of resulted clusters was 6, having this number of clusters no specific relation with the number of 6R classes. The output of the clustering process is thus a 6x6 Matrix (Class(row)/Cluster (column)) shown in Figure <a href="methods.html#fig:clusterssm">9.17</a>. Numbers in the cells of the matrix represents centroids that can be characterized by the related keywords.</p>
<div class="figure" style="text-align: center"><span id="fig:clusterssm"></span>
<img src="_bookdown_files/figures/heat_map_sm.png" alt="Matrix: comparing the manual classification and the clustering output." width="60%" />
<p class="caption">
Figure 9.17: Matrix: comparing the manual classification and the clustering output.
</p>
</div>
</div>
</div>
<div id="an-extended-mapping-of-the-sm-field-worldwide" class="section level4">
<h4><span class="header-section-number">9.2.1.4</span> An extended mapping of the SM field worldwide</h4>
<p>The methodology described in section <a href="methods.html#smtextdrivenbottomup">9.2.1.2</a> has been then re-adapted to give an extended anlysis of the 6R framework from a bottom-up perspective. In the present section, we map the state of the art of Sustainable Manufacturing using topic modelling, a widely used text mining techniques. The aim is to give a broad overview of how this knowledge field is approached world-wide and then, in section 3, to have a deeper look at the Italian way to sustainable manufacturing.</p>
<p>The process of topic extraction is composed by the sequent activities:</p>
<ol style="list-style-type: decimal">
<li><em>Papers Collection</em></li>
<li><em>Keyword Extraction</em></li>
<li><em>Topic Modelling:</em> Number of topic selection</li>
<li><em>Topic Modelling:</em> LDA Model fitting</li>
<li><em>Manual topic labelling</em></li>
</ol>
<p>Each activity is described in the sequent sections.</p>
<div id="papers-collection" class="section level5 unnumbered">
<h5>Papers collection</h5>
<p>The analysis starts form a corpus of papers on sustainable manufacturing. The papers were downloaded from the Scopus database searching for the query:</p>
<span class="math display">\[\begin{equation*} 
  TITLE-ABS-KEY(&quot;sustainable.*manufacturing&quot;)
\end{equation*}\]</span>
<p>At the date of 29/05/2018 the query results in 1,628 documents.</p>
</div>
<div id="keyword-extraction" class="section level5 unnumbered">
<h5>Keyword extraction</h5>
<p>We represent each article as a set of keywords, merging Author Keywords and Index Keywords. The keywords are then “sanitized” following the sequent rules:</p>
<ul>
<li>Eliminate duplicated keywords</li>
<li>Eliminate brackets and its content</li>
<li>Substitute non-alphanumeric character with a blank space</li>
<li>Merge synonyms, alternative spelling and keywords pointing to similar concepts</li>
<li>Eliminate scientific literature specific keywords like “article” and “review”</li>
<li>Filter the generic keywords. The metrics for the threshold is the percentage of papers that contains the keywords. The value has been set to 7.5%.</li>
</ul>
<p>Finally, we compute the Keywords Term Matrix. The matrix is composed of 1,628 documents and 26,272 keywords. Having a mathematical representation of the documents allows us to apply standard mathematical techniques to them.</p>
</div>
<div id="topic-modelling-number-of-topic-selection-and-lda-model-fitting" class="section level5">
<h5><span class="header-section-number">9.2.1.4.1</span> Topic Modelling: Number of Topic Selection and LDA Model Fitting</h5>
<p>The goal of the present section is to compute a topic model based on the keyword representation of the papers. A topic model allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document’s balance of topics is. To deploy it for the purposes of the present paper, keywords will cluster to form a topic: each topic will represent a different view (say, principle) of sustainable manufacturing. This approach brings a new perspective to the definition of sustainable manufacturing with respect to the 6R’s framework.</p>
<p>To compute the topic model, we use the Latent Dirichlet Allocation (LDA) algorithm. To select the number of topics for LDA, the most efficient and effective way is to calculate multiple metrics of the quality of the results in function of the number of topics. All existing methods require to train multiple LDA models to select one with the best performance. Several approaches tried to take the problem of automatically finding the right number of topics contained in a set of documents. Every approach follows the idea of computing distances (or similarities) between pair of topics varying the number of topics. We used four different methods to evaluate the output of a topic model for different value of k. These methods are:</p>
<ul>
<li>Caojuan2009 <span class="citation">(Cao et al. <a href="#ref-cao2009density">2009</a>)</span>: Minimize the average cosine distance between every pair of topics. The best topic number K has a minimal final distance between topics in Latent Dirichlet Allocation</li>
<li>Arun2010 <span class="citation">(Arun et al. <a href="#ref-arun2010finding">2010</a>)</span>: Minimize the symmetric KL-Divergence of the salient distribution that are derived from the matrices of factors. These matrices are the re-projections of the documents on the topics and of the topics on the vocabulary (the selected tokens). The divergence values are higher for non-optimal K values.</li>
<li>Griffiths2004 <span class="citation">(Griffiths and Steyvers <a href="#ref-griffiths2004finding">2004</a>)</span>: Maximize the likely of the data given the model built considering K topics. This is a problem of model selection using Bayesian statistics.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:topicnumsm"></span>
<img src="_bookdown_files/figures/Griffithsetc.pdf" alt="Relationship between the measures of the quality of the topic modelling output and the number of topics." width="60%" />
<p class="caption">
Figure 9.18: Relationship between the measures of the quality of the topic modelling output and the number of topics.
</p>
</div>
<p>We computed these for measures fitting a LDA model for every K between 2 and 20. We choose the higher value because we wanted to obtain a reasonable number of topics representing the concepts (or we may say, principles) behind sustainable manufacturing. The results of the analysis are shown in figure <a href="methods.html#fig:topicnumsm">9.18</a>. From this figure is evident how the measure we want to minimize intersect each other at a value between 11 and 12 topics. We thus decided to visualize the results of the LDA models for a number of topics k between 7 and 15: the expert panel interviewed then allowed to decide that the best results was at 12 topics.</p>
<p>We then extract the one-topic-per-term-per-row probabilities, beta. Beta measures what is the probability for each topic to produce a term. Figure <a href="methods.html#fig:topicpicture">9.19</a> shows the top 5 terms for each topic. Here each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term). It is possible for some keywords to belong to multiple topics: for example, optimization belongs to topic 1 and 9. It is easy to spot these keywords because typically (belonging to two different topics) are far from the group of keywords of the topics they belong.</p>
<div class="figure" style="text-align: center"><span id="fig:topicpicture"></span>
<img src="_bookdown_files/figures/geom_point_topic.pdf" alt="Content of the 12 topics. Each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term)." width="60%" />
<p class="caption">
Figure 9.19: Content of the 12 topics. Each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term).
</p>
</div>
</div>
<div id="manual-labelling-of-the-topics" class="section level5 unnumbered">
<h5>Manual Labelling of the topics</h5>
<p>To have a clearer representation of the topics, these were manually labelled by 6 independent experts in the field of manufacturing and sustainability. Then a seventh experts took in input the results of the manual naming and synthetized the label. The final results are the names of the 12 topics. These names are:</p>
<ol style="list-style-type: decimal">
<li>Smartness for Sustainability</li>
<li>Sustainable Machining</li>
<li>Manufacturing Environmental Efficiency</li>
<li>Modelling Manufacturing Sustainability</li>
<li>Welding Sustainability</li>
<li>AM Sustainability</li>
<li>Life-cycle Product Management</li>
<li>Advanced Material Sustainability</li>
<li>Production Management for Sustainability</li>
<li>Sustainable Energies</li>
<li>Innovation for Sustainability</li>
<li>Sustainable Logistic</li>
</ol>
<p>This final picture provides a clear representation of how topics recognized are mostly homogeneous, provided there are few outlier keywords per each topic, as in figure <a href="methods.html#fig:topicpicture">9.19</a>. This “new perspective” of sustainable manufacturing represents a way of defining its contents by the grace of the tools or domain of interest involved. For instance, topic n.1 (smart sustainability) refers to the use of the new group of tools and approaches belonging to the Industry 4.0 stream, thus defining specific contents and criteria useful to pursue the sustainability in manufacturing.</p>
</div>
</div>
</div>
<div id="blockchain" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Blockchain</h3>
<p>In section <a href="methods.html#advdrwresults">9.1.2</a> has been proposed a new usage of sentiment analysis to extract advantages (considered synonym of the words benefit, gain or profit) and disadvantages (drawback, failure) of technologies from patents (Chiarello, 2017) with the aim of helping researchers and designers to effectively develop new products, analyzing positive and negative properties of an innovation.</p>
<p>The main contributions of the present analysis is to list the problems of a technological field. We want to understand which are the problems that research is trying to solve for a technology and if is there any focus from researchers on the solution of certain technical problem. Furthermore using text mining techniques it is possibile to correlate these problems between them and understand if is there any correlation between problems of technologies and if this map help researchers and companies to understand the research agenda.</p>
<p>This section thus proposes an innovative methodology for the (semi-automatic) extraction of technical knowledge from academic articles, using text mining techniques. Among many information related to technical knowledge, thw methodology focus on the collection and the analysis of the problems of a technology. The meaning of problems of a technology is related to the concept of disadvantages of a technology but with a broader sense. In fact, using papers as a source makes it possible to collect not only technical disadvantages (typically described in patents) but also, organizational, business or even social problems, since these are faced by different intellectual fields in the scientific literature. We also propose a case study on the application of the method to the field of <em>block-chain</em>. We chose this technology because:</p>
<ul>
<li>It is highly innovative</li>
<li>It is having an impact in many different field</li>
<li>The problems of this technology are both technical and managerial</li>
</ul>
<div id="how-blockchain-technology-works" class="section level5 unnumbered">
<h5>How Blockchain Technology Works</h5>
<p>The blockchain can be exemplified as a process in which a set of subjects shares computer resources (memory, CPU, band) to make available all private users in which each participant has a copy of the data.</p>
<p>The use of cryptographic validation techniques generates the mutual trust of the participants in the data stored by the blockchain, which makes it comparable to the registries managed in a centralized manner by recognized and regulated authorities (banks, insurance companies, etc.) <span class="citation">(Pilkington <a href="#ref-pilkington201611">2016</a>)</span>.</p>
<p>A blockchain is an open and distributed register that can store transactions between parties in a safe, verifiable and permanent way. Once written, the data in a block can not be retroactively altered without the modification of all the subsequent blocks, and this, due to the nature of the protocol and the validation scheme, would require the consensus of the majority of the network. <span class="citation">(Iansiti and Lakhani <a href="#ref-iansiti2017truth">2017</a>)</span></p>
<p>The blockchain is a continuously growing list of records, called blocks, which are linked to each other and made safe by the use of cryptography. Each block in the chain contains a hash pointer (that is a link to the previous block), a timestamp and the transaction data.</p>
<p>The distributed nature and the cooperative model makes the validation process robust and secure, but it has considerable time and costs, due in large part to the <em>price of the electricity</em> needed to validate the blocks <span class="citation">(Underwood <a href="#ref-underwood2016blockchain">2016</a>)</span>. Authentication takes place through mass collaboration and is fueled by collective interests. The result of all this is a robust workflow where participants’ data security expertise is not required. The use of this technology also makes it possible to overcome the problem of the infinite reproducibility of a digital asset and of the double expense without using a central server or an authority. <span class="citation">(Karame, Androulaki, and Capkun <a href="#ref-karame2012double">2012</a>)</span></p>
<p>There exist an overtrust about blochain technology for businesess, and there exist a growing interest about the drawbacks of this technology <span class="citation">(Eyal and Sirer <a href="#ref-eyal2018majority">2018</a>. <span class="citation">Lin and Liao (<a href="#ref-lin2017survey">2017</a>)</span>, <span class="citation">Yli-Huumo et al. (<a href="#ref-yli2016current">2016</a>)</span>)</span>. Has been pointed out that a 51 percent attack 26 would be enough to access or control a private blockchain because, in most cases, the organization that controls it already controls one hundred percent of the block creation. In fact, if someone could attack or damage the blocking tool on a private company server, would be possibile to get complete management of the network and the ability to access and modify the data <span class="citation">(Hampton <a href="#ref-hampton2016understanding">2016</a>)</span>. This is because the centralization due to the privatization of the blockchain leads to a single point of failure, or a central break point, which in a public blockchain could not happen as distributed, so there would be no central point to attack. This could have profound implications in financial crises or debt crises like the 2007-08 financial crisis.</p>
</div>
<div id="problems-extraction-methodology" class="section level4">
<h4><span class="header-section-number">9.2.2.1</span> Problems Extraction Methodology</h4>
<p>Considering our objectives, the approach we adopted for paper analysis is radically new with respect to the one traditionally established in the literature (bibliometric analysis and keyword approaches). These approaches do not allow a deeper understanding of the technical content described in the text. For this reason, we relied on text mining techniques supported by our technical knowledge base. For the sentiment computation in fact, we used a technical sentiment lexicon developed by the authors that extracts advantages and disadvantages of inventions from patents (<a href="methods.html#advdrwresults">9.1.2</a>) and a novel dictionary lookup approach that incorporates weighting for valence shifters. Following a bottom up-approach, we redesigned and updated these lexicons for an optimal application to paper documents.</p>
<div class="figure" style="text-align: center"><span id="fig:bcworkflow"></span>
<img src="_bookdown_files/figures/bcworkflow.png" alt="Proposed workflow for the problem extraction from papers. " width="80%" />
<p class="caption">
Figure 9.20: Proposed workflow for the problem extraction from papers.
</p>
</div>
<p>The workflow of the proposed methodology is shown in figure <a href="methods.html#fig:bcworkflow">9.20</a>. The process starts with the collection of abstracts belonging to the same technological field. The abstracts are downloaded using the Scopus API, extracting all the documents that contains keywords of the selected technology in the title, abstract or keywords fields. The texts are then pre-processed using state of the art natural language processing tools (sentence splitter, tokenizer, lemmatization). Then, for each sentence we computed a negative sentiment polarity and took into consideration only the sentences having a negative polarity score below a threshold level. We applied topic modelling algorithm on the negative sentences with the aim of clustering. The output of topic modelling is evaluated by technology domain experts with the aim of labeling the identified clusters.</p>
<p>The application scenarios of our methodology have a wide range of users:</p>
<ol style="list-style-type: decimal">
<li>Companies that want to rapidly map a certain technological field</li>
<li>Policy maker that want to invest to solve problems of a technology to boost its innovation</li>
<li>Journals, to understand the hot-topic of a specific field</li>
<li>Research networks, to exploit possible collaborations and synergies between scholars.</li>
</ol>
<div id="document-collection-and-selection" class="section level5 unnumbered">
<h5>Document collection and selection</h5>
<p>The documents were extracted from Scopus searching for the query:</p>
<span class="math display">\[\begin{equation*} 
  TITLE-ABS-KEY(&quot;blockchain&quot; OR &quot;block chain&quot; OR &quot;block-chain&quot;)
\end{equation*}\]</span>
<p>At the date of 29/05/2018 the query results in 1,628 documents.</p>
<p>The term blockchain and its morphological variations is used in other fields different from computer science and economics, in particular in chemistry. For this reason we filtered from the data set all the papers belonging to one of the sequent All Science Journal Classification (ASJC) classes: chemistry, physics and astronomy, chemical engineering, biochemistry, genetics and molecular biology, pharmacology, toxicology and pharmaceutics, immunology and microbiology. The result of this lead us to a set of 1,364 papers.</p>
<p>To have a dataset with an higher precision, we also filtered using rules based on the content of the abstracts, since many conferences do not have an ASJC class in scopus. This characteristic of the scopus database could lead to bias in the results, especially for innovative technologies like block-chain for which the scientific discussion is stronger in conferences then in more structured journals. For this reason we analyzed the abstracts of the 264 articles belonging to one of the ASJC classes listed above (chemistry related) extracting the most frequent words and filtering for a blacklist of generic words contained in scientific articles. This results in a dictionary of words that has ben used to search and filter al the papers containing in the abstract one or more of the words in the dictionary. The final number of selected papers is 1,276.</p>
</div>
<div id="sentence-splitting" class="section level5 unnumbered">
<h5>Sentence Splitting</h5>
<p>A sentence splitter (for furhter details see setction <a href="sota.html#sotatoolstransformsentencesplit">8.1.4.1</a>) is then applied to the abstract to divide the texts in sentences. From this step we had an output of 8,406 sentences.</p>
<div class="figure" style="text-align: center"><span id="fig:histbllength"></span>
<img src="_bookdown_files/figures/length_histogram_bl.pdf" alt="Histogram of the lengths of the sentences." width="80%" />
<p class="caption">
Figure 9.21: Histogram of the lengths of the sentences.
</p>
</div>
<p>The next step involved the measure of the sentences length. Some of the sentences in fact could be too long due to several fact for example the style of the author or an error in the sentences splitting phase). Since it is not a goal of the present work to analyze the style of the authors or to design a better sentence splitter, we decided to filter the sentences that are too long. The decision on the threshold level of number of word has been taken analyzing the distribution shown in figure <a href="methods.html#fig:histbllength">9.21</a>. The 97% of the population of sentences contain a number of words that is lower than 53. Thus all the sentences contain more than 53 words has been filtered. From this step we had an output of 8143 sentences.</p>
</div>
<div id="polarity-computation" class="section level5 unnumbered">
<h5>Polarity Computation</h5>
<p>At this point has been possibile to apply a sentiment polarity measurement on the sentences <span class="citation">(Rinker <a href="#ref-sentimentr2018">2018</a>)</span>. As output we had a polarity score between -1 (strongly negative) and 1 (strongly positive) for each sentence. Since we are interested in extracting the problems of the state-of-the-art blockchain systems, we visualize the distribution of the polarity scores of the sentences labeled as negative (having a polarity score lower then 0). The histogram of this distribution is shown in figure TOT. In this histogram are represented 1,108 different sentences. As we can see we have a distribution center in -0.2, with a tail that goes down to -0.8. This is an evidence that the system give a reasonable measures since it would be unreasonable to have more probable extreme values. Furthermore, the number of sentences having a polarity equal to 0 is 4,435 that is about the 50% of the sentences and the number of sentences having a positive polarity is 2,700 : it is reasonable that the higher number of sentences falls in these classes since the scientific jargon has to be neutral. The positive bias has been seen also for patent documents (see section <a href="methods.html#advdrwresults">9.1.2</a>)).</p>
<div class="figure" style="text-align: center"><span id="fig:sentintimebl"></span>
<img src="_bookdown_files/figures/polarity_histogram_time_bl.pdf" alt="Box plots of the distributions  by years of the polarities of the sentences having a negative polarity score." width="80%" />
<p class="caption">
Figure 9.22: Box plots of the distributions by years of the polarities of the sentences having a negative polarity score.
</p>
</div>
<p>One interesting evidence we found is that the number of outliers (having a polarity values lower than the 95% of the population) is growing over the year as shown in figure <a href="methods.html#fig:sentintimebl">9.22</a>. Even if the mean remains near to -0.2, int he last five years the number of strongly negative sentences has increased. This shows how there has been a trend in the focus on the problems of blockchain.</p>
</div>
<div id="tokenization-and-token-filtering" class="section level5 unnumbered">
<h5>Tokenization and Token Filtering</h5>
<p>In the next phase we apply a state of the art natural language processing pipeline (see section <a href="sota.html#sotatools">8.1</a> for more details) to the 1,108 negative sentences.</p>
<p>The tokenization process results in 220,094 tokens. To extract the meaningful words (words that adds information about the problem that the sentence is describing) we applied a series of filtering steps. As meaningful unit to analyze (token) we decided to use the lemma of the word. The filtering chain we applied is described in table <a href="methods.html#tab:tableblfilter">9.19</a>- The 9,451 final token is a reasonable number considering the 1,108 sentences in input: we have a mean of 9 words per sentence. Considering the mean of 25 words per sentence shown in figure <a href="methods.html#fig:histbllength">9.21</a>, we now have a summary of the sentences contains only the meaningful words. This output is a clean input for the topic modelling phase.</p>
<table>
<caption><span id="tab:tableblfilter">Table 9.19: </span> Table of the filters applied to select the relevant tokens.</caption>
<thead>
<tr class="header">
<th align="left">Filter Description</th>
<th align="left">Filter Type</th>
<th align="left">Examples of filtered words</th>
<th align="right">Number of token in output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Select only nouns, adjective and verbs</td>
<td align="left">POS tagging based</td>
<td align="left">the, a, anyway, with, because</td>
<td align="right">20.365</td>
</tr>
<tr class="even">
<td align="left">Filter words contained in the advantages and drawback lexicon</td>
<td align="left">Stop-words</td>
<td align="left">impossibile, problem, insufficient, challenge, issue, dangerous</td>
<td align="right">18.354</td>
</tr>
<tr class="odd">
<td align="left">Filter words typical of scientific litterature</td>
<td align="left">Stop-words</td>
<td align="left">paper, study, work, research, review, discuss, find, focus</td>
<td align="right">14.505</td>
</tr>
<tr class="even">
<td align="left">Filter words contained in more than 10% of the sentences</td>
<td align="left">Frequency filter</td>
<td align="left">block, chain, technology, be, have, datum</td>
<td align="right">12.473</td>
</tr>
<tr class="odd">
<td align="left">Filter words contained in less than 0.1% of the sentences</td>
<td align="left">Frequency filter</td>
<td align="left">philosophy, dialogue, prospect, liberalized, interorganizational, curriculum, multidimensional</td>
<td align="right">9.988</td>
</tr>
<tr class="even">
<td align="left">Filter all the token shorter than 3 character</td>
<td align="left">Morphology filtering</td>
<td align="left">a, b, c, ©, %, us, bc</td>
<td align="right">9.451</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="topic-modelling" class="section level4">
<h4><span class="header-section-number">9.2.2.2</span> Topic modelling</h4>
<p>Topic modeling is a method for unsupervised classification of documents which finds groups of documents fitting a statistical model to the data. In other words, these models captures word correlations in a collection of documents with a set of topics. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language (for further details see section <a href="sota.html#sotatoolsmodeltopicmodel">8.1.5.6</a>). If the topic modelling process is good, we have as output a structure where every topic is an understandable, meaningful and compact semantic cluster that in our case clearly represent a state-of-the-art problem of block chain technology.</p>
<div id="document-term-matrix" class="section level5 unnumbered">
<h5>Document Term Matrix</h5>
<p>The first step is to take in input the 9,451 tokens and count how many times each one occurs in the 1,108 sentences. In this way we obtain a document-term matrix (DTM) where the documents are the sentences and the terms are our tokens (the lemmas). In our case the DTM has 1077 rows representing the sentences (31 sentences did not contain any relevant tokens) and 1147 tokens.</p>
</div>
<div id="definition-of-the-optimal-number-of-topics-a-machine-human-hybrid-approach" class="section level5 unnumbered">
<h5>Definition of the Optimal Number of Topics: a Machine-Human Hybrid Approach</h5>
<p>To fit a LDA model we have to give as a parameter the number of topic k that we think that best represent the corpus in analysis. In literature there exists many measurement to compute an optimal value of k. However it is worth to keep in mind that these measures are not always correlated with expert judgement about topics interpretability and coherence. For this reason in the present paper we use an hybrid approach in which we find an optimal neighborhood of value using state of the art k tuning methods and then we compute a graphical output for the 5 best k values to be evaluated by 4 different experts.</p>
<p>Several approaches tried to take the problem of automatically finding the right number of topics contained in a set of documents. Every approach follow the idea of computing distances (or similarities) between pair of topics varying the number of topics. We used four different methods to evaluate the output of a topic model for different value of k. These methods are:</p>
<ul>
<li>Caojuan2009 <span class="citation">(Cao et al. <a href="#ref-cao2009density">2009</a>)</span>: Minimize the average cosine distance between every pair of topics. The best topic number K has a minimal final distance between topics in Latent Dirichlet Allocation</li>
<li>Arun2010 <span class="citation">(Arun et al. <a href="#ref-arun2010finding">2010</a>)</span>: Minimize the symmetric KL-Divergence of the salient distribution that are derived from the matrices of factors. These matrices are the re-projections of the documents on the topics and of the topics on the vocabulary (the selected tokens). The divergence values are higher for non-optimal K values.</li>
<li>Griffiths2004 <span class="citation">(Griffiths and Steyvers <a href="#ref-griffiths2004finding">2004</a>)</span>: Maximize the likely of the data given the model built considering K topics. This is a problem of model selection using Bayesian statistics.</li>
<li>Deveaud2014 <span class="citation">(Deveaud, SanJuan, and Bellot <a href="#ref-deveaud2014accurate">2014</a>)</span>: Maximize the information divergence between all pairs of topics. The optimal value k is the value for which LDA modeled the most scattered topics.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:topicnumsmbl"></span>
<img src="_bookdown_files/figures/FindTopicsNumber_plot_bl.pdf" alt="Measures of quality of the topic modelling results for growing number of topics. The figure is divided in metrics to minimize (top figure) and to maximized (bottom figure)." width="60%" />
<p class="caption">
Figure 9.23: Measures of quality of the topic modelling results for growing number of topics. The figure is divided in metrics to minimize (top figure) and to maximized (bottom figure).
</p>
</div>
<p>We computed these for measures fitting a LDA model for every K between 2 and 30. We choose the higher value because we wanted to obtain a reasonable number of topics representing the problems of blockchain. After a brain storming with experts it emerges that there can not exists more than 15 classes of problems. We decide to take the double of this number to take in to consideration the biases of human experts. The results of the analysis are shown in figure <a href="methods.html#fig:topicnumsmbl">9.23</a>. From this figure is evident how the measure we want to minimize interest each other at a value of 7 topics; the values we want to minimize are a little more unstable (especially Arun2010) and intersect both at level of 9 and 11. We thus decided to visualize and make choose the results of the LDA models for a number of topics k between 7 an 10.</p>
<p>To help experts in their decision process, an interactive map of the problem hass been generated for 7, 8, 9, 10 and 11 topics. The map was created using Shiny <span class="citation">(Chang et al. <a href="#ref-shiny2017">2017</a>)</span>. The interactive map of problems (a screen shot of the app is shown in figure <a href="methods.html#fig:topicmodelschinybc">9.24</a>) helped experts to understand the relationships between the problems. Here a multidimensional scaling algorithm compute the inter-topic distance that permits to visualize the distribution of topics in two dimensions. Clicking on a topic the experts can see its content, and for each word can see both at the estimated term frequency within the selected topic and at the overall term frequency.</p>
<div class="figure" style="text-align: center"><span id="fig:topicmodelschinybc"></span>
<img src="_bookdown_files/figures/bc_topicmodel_shiny.png" alt="Screenshot of the shiny app used by experts to choose the optimal number of topcis. " width="60%" />
<p class="caption">
Figure 9.24: Screenshot of the shiny app used by experts to choose the optimal number of topcis.
</p>
</div>
<p>The experts agreed for a total number of 8 topics as optimal.</p>
</div>
<div id="topic-modelling-results" class="section level5 unnumbered">
<h5>Topic Modelling Results</h5>
<p>The output topic modelling phase is show in figure <a href="methods.html#fig:topicmodelsoutputbc">9.25</a>. The results of the topic modelling are visualized to make it possibile for the expert to explore and label the topic representing the problems of block chain technology. Each point is a word, and word belonging to the same topic are grouped. The size of the label is proportional to the probability ß that the word belong to that topic.</p>
<div class="figure" style="text-align: center"><span id="fig:topicmodelsoutputbc"></span>
<img src="_bookdown_files/figures/output_bl_topicmodels.png" alt="Content of the 8 topics of problems of blockchain. Each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term" width="60%" />
<p class="caption">
Figure 9.25: Content of the 8 topics of problems of blockchain. Each topic has a different vertical position and a different color. Each topic is represented by a set of keywords and each keyword has a different position on the y axes. For each intersection topic/keyword, the dimension of a circle represent beta (what is the probability that topic to produce that term
</p>
</div>
</div>
</div>
</div>
</div>
<div id="wikipedia" class="section level2">
<h2><span class="header-section-number">9.3</span> Wikipedia</h2>
</div>
<div id="twitter" class="section level2">
<h2><span class="header-section-number">9.4</span> Twitter</h2>
</div>
<div id="job-profiles" class="section level2">
<h2><span class="header-section-number">9.5</span> Job Profiles</h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-wipo2">
<p>Idris, Kamil. 2008. “Wipo Intellectual Property Handbook: Policy, Law and Use.” <em>Geneva: WIPO Publication</em>, no. 489.</p>
</div>
<div id="ref-bonam2017">
<p>Bonaccorsi, Fantoni, Chiarello, and D’amico. 2017. “Mapping Users in Patents. Towards a New Methodology and the Definition of a Research Agenda.” In <em>EPIP 2017 Conference Bordeaux</em>.</p>
</div>
<div id="ref-alleydog">
<p>Dog, Alley. 2015. “Psychological Dictionaty.” <a href="http://alleydog.com" class="uri">http://alleydog.com</a>.</p>
</div>
<div id="ref-belieber">
<p>Beller, Charley, Rebecca Knowles, Craig Harman, Shane Bergsma, Margaret Mitchell, and Benjamin Van Durme. 2014. “I’m a Belieber: Social Roles via Self-Identification and Conceptual Attributes.” In <em>ACL (2)</em>, 181–86.</p>
</div>
<div id="ref-prefine">
<p>Beller, Charley, Craig Harman, and Benjamin Van Durme. 2014. “Predicting Fine-Grained Social Roles with Selectional Preferences.” <em>ACL 2014</em>, 50.</p>
</div>
<div id="ref-kittur2008crowdsourcing">
<p>Kittur, Aniket, Ed H Chi, and Bongwon Suh. 2008. “Crowdsourcing User Studies with Mechanical Turk.” In <em>Proceedings of the Sigchi Conference on Human Factors in Computing Systems</em>, 453–56. ACM.</p>
</div>
<div id="ref-HRM">
<p>Johnson, Phil. 2009. “2 Hrm in Changing Organizational Contexts.” <em>Strategic HRM</em>, 19.</p>
</div>
<div id="ref-listjobs">
<p>“Suffix Codes for Jobs Defined in the Dictionary of Occupational Titles, Third Edition.” 1967. <em>United States Employment Service, U.S. Dept. of Labor</em>.</p>
</div>
<div id="ref-cdcdiseases">
<p>Health, U.S. Department of, and Human Services. 2018. “Diseases and Conditions.” <a href="http://www.cdc.gov/DiseasesConditions" class="uri">http://www.cdc.gov/DiseasesConditions</a>.</p>
</div>
<div id="ref-ucd">
<p>ISO, ISO13407. 1999. “13407: Human-Centred Design Processes for Interactive Systems.” <em>Geneva: ISO</em>.</p>
</div>
<div id="ref-npdul">
<p>K., Ulrich. 2008. “Users, Experts, and Institutions in Design.” <em>Handbook of New Product Development Management</em>. Elsevier, 421–38.</p>
</div>
<div id="ref-wipo1">
<p>Organization, WIPO: World Intellectual Property. 1971. “International Patent Classification (Ipc).” <a href="http://www.wipo.int/classifications/ipc/en/" class="uri">http://www.wipo.int/classifications/ipc/en/</a>.</p>
</div>
<div id="ref-dell2009ensemble">
<p>Dell’Orletta, Felice. 2009. “Ensemble System for Part-of-Speech Tagging.” <em>Proceedings of EVALITA</em> 9: 1–8.</p>
</div>
<div id="ref-svm">
<p>Hearst, Marti A., Susan T Dumais, Edgar Osman, John Platt, and Bernhard Scholkopf. 1998. “Support Vector Machines.” <em>Intelligent Systems and Their Applications, IEEE</em> 13 (4). IEEE: 18–28.</p>
</div>
<div id="ref-libkeras">
<p>Chollet, François. 2015. “Keras.” <a href="https://github.com/fchollet/keras" class="uri">https://github.com/fchollet/keras</a>; GitHub.</p>
</div>
<div id="ref-word2vec1">
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv Preprint arXiv:1301.3781</em>.</p>
</div>
<div id="ref-monireh">
<p>Mirtalaie, Monireh Alsadat, Omar Khadeer Hussain, Elizabeth Chang, and Farookh Khadeer Hussain. 2018. “Extracting Sentiment Knowledge from Pros/Cons Product Reviews Discovering Features Along with the Polarity Strength of Their Associated Opinions.” <em>Expert Systems with Applications</em>. Elsevier.</p>
</div>
<div id="ref-golzio2012">
<p>Golzio, Domenico. 2012. “WWWWWHOW Read a Patent!” ICEAA.</p>
</div>
<div id="ref-pahl2013engineering">
<p>Pahl, Gerhard, and Wolfgang Beitz. 2013. <em>Engineering Design: A Systematic Approach</em>. Springer Science; Business Media.</p>
</div>
<div id="ref-ulrich2003product">
<p>Ulrich, Karl. 2003. <em>Product Design and Development</em>. Tata McGraw-Hill Education.</p>
</div>
<div id="ref-carnevalli2008review">
<p>Carnevalli, Jose A, and Paulo Cauchick Miguel. 2008. “Review, Analysis and Classification of the Literature on Qfd—Types of Research, Difficulties and Benefits.” <em>International Journal of Production Economics</em> 114 (2). Elsevier: 737–54.</p>
</div>
<div id="ref-liu2013risk">
<p>Liu, Hu-Chen, Long Liu, and Nan Liu. 2013. “Risk Evaluation Approaches in Failure Mode and Effects Analysis: A Literature Review.” <em>Expert Systems with Applications</em> 40 (2). Elsevier: 828–38.</p>
</div>
<div id="ref-world2004wipo">
<p>Organization, World Intellectual Property. 2004. <em>WIPO Intellectual Property Handbook: Policy, Law and Use</em>. 489. World Intellectual Property Organization.</p>
</div>
<div id="ref-ramshaw">
<p>Ramshaw, Lance A, and Mitchell P Marcus. 1999. “Text Chunking Using Transformation-Based Learning.” In <em>Natural Language Processing Using Very Large Corpora</em>, 157–76. Springer.</p>
</div>
<div id="ref-evans2010social">
<p>Evans, Dave, Susan Bratton, and Jake McKee. 2010. <em>Social Media Marketing: The Next Generation of Business Engagement</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-hu2004mining">
<p>Hu, Minqing, and Bing Liu. 2004. “Mining and Summarizing Customer Reviews.” In <em>Proceedings of the Tenth Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 168–77. ACM.</p>
</div>
<div id="ref-wilson2005recognizing">
<p>Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 2005. “Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.” In <em>Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</em>, 347–54. Association for Computational Linguistics.</p>
</div>
<div id="ref-baccianella2010sentiwordnet">
<p>Baccianella, Stefano, Andrea Esuli, and Fabrizio Sebastiani. 2010. “Sentiwordnet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.” In <em>Lrec</em>, 10:2200–2204. 2010.</p>
</div>
<div id="ref-cimino2014linguistically">
<p>Cimino, Andrea, Stefano Cresci, Felice Dell’Orletta, and Maurizio Tesconi. 2014. “Linguistically-Motivated and Lexicon Features for Sentiment Analysis of Italian Tweets.” <em>4th Evaluation Campaign of Natural Language Processing and Speech Tools for Italian (EVALITA 2014)</em>, 81–86.</p>
</div>
<div id="ref-griliches1981market">
<p>Griliches, Zvi. 1981. “Market Value, R&amp;D, and Patents.” <em>Economics Letters</em> 7 (2). Elsevier: 183–87.</p>
</div>
<div id="ref-mercer2010reading">
<p>MERCER, JOHN, Teresa Da Silva Lopes, and PAUL DUGUID. 2010. “Reading Registrations: An Overview of 100 Years of Trademark Registrations in France, the United Kingdom, and the United States.” In <em>Trademarks, Brands, and Competitiveness</em>, 27–48. Routledge.</p>
</div>
<div id="ref-baroncelli2004global">
<p>Baroncelli, Eugenia, Carsten Fink, and Beata Smarzynska. 2004. <em>The Global Distribution of Trademarks: Some Stylized Facts</em>. The World Bank.</p>
</div>
<div id="ref-butler1969rules">
<p>Butler, F Prince. 1969. “Rules Defining the Use of Trade Terms in Patent Applications.” <em>J. Pat. Off. Soc’y</em> 51. HeinOnline: 339.</p>
</div>
<div id="ref-hall2018impact">
<p>Hall, Bronwyn, and Christian Helmers. 2018. “The Impact of International Patent Systems: Evidence from Accession to the European Patent Convention.” National Bureau of Economic Research.</p>
</div>
<div id="ref-jaffe2000us">
<p>Jaffe, Adam B. 2000. “The Us Patent System in Transition: Policy Innovation and the Innovation Process.” <em>Research Policy</em> 29 (4-5). Elsevier: 531–57.</p>
</div>
<div id="ref-pressman2018nolo">
<p>Pressman, David, and Richard Stim. 2018. <em>Nolo’s Patents for Beginners: Quick &amp; Legal</em>. Nolo.</p>
</div>
<div id="ref-morris2016trademarks">
<p>Morris, P Sean. 2016. “Trademarks as Sources of Market Power: Drugs, Beers and Product Differentiation.” <em>JL &amp; Com.</em> 35. HeinOnline: 163.</p>
</div>
<div id="ref-jawahir2016technological">
<p>Jawahir, IS, and Ryan Bradley. 2016. “Technological Elements of Circular Economy and the Principles of 6r-Based Closed-Loop Material Flow in Sustainable Manufacturing.” <em>Procedia Cirp</em> 40. Elsevier: 103–8.</p>
</div>
<div id="ref-ingarao2017manufacturing">
<p>Ingarao, Giuseppe. 2017. “Manufacturing Strategies for Efficiency in Energy and Resources Use: The Role of Metal Shaping Processes.” <em>Journal of Cleaner Production</em> 142. Elsevier: 2872–86.</p>
</div>
<div id="ref-duflou2012towards">
<p>Duflou, Joost R, John W Sutherland, David Dornfeld, Christoph Herrmann, Jack Jeswiet, Sami Kara, Michael Hauschild, and Karel Kellens. 2012. “Towards Energy and Resource Efficient Manufacturing: A Processes and Systems Approach.” <em>CIRP Annals-Manufacturing Technology</em> 61 (2). Elsevier: 587–609.</p>
</div>
<div id="ref-haapala2013review">
<p>Haapala, Karl R, Fu Zhao, Jaime Camelio, John W Sutherland, Steven J Skerlos, David A Dornfeld, IS Jawahir, Andres F Clarens, and Jeremy L Rickli. 2013. “A Review of Engineering Research in Sustainable Manufacturing.” <em>Journal of Manufacturing Science and Engineering</em> 135 (4). American Society of Mechanical Engineers: 041013.</p>
</div>
<div id="ref-allwood2014squaring">
<p>Allwood, Julian M. 2014. “Squaring the Circular Economy: The Role of Recycling Within a Hierarchy of Material Management Strategies.” In <em>Handbook of Recycling</em>, 445–77. Elsevier.</p>
</div>
<div id="ref-cooper2012reusing">
<p>Cooper, Daniel R, and Julian M Allwood. 2012. “Reusing Steel and Aluminum Components at End of Product Life.” <em>Environmental Science &amp; Technology</em> 46 (18). ACS Publications: 10334–40.</p>
</div>
<div id="ref-wced1987world">
<p>WCED, SPECIAL WORKING SESSION. 1987. “World Commission on Environment and Development.” <em>Our Common Future</em>. Oxford University Press London.</p>
</div>
<div id="ref-buchta2012spherical">
<p>Buchta, Christian, Martin Kober, Ingo Feinerer, and Kurt Hornik. 2012. “Spherical K-Means Clustering.” <em>Journal of Statistical Software</em> 50 (10). American Statistical Association: 1–22.</p>
</div>
<div id="ref-cao2009density">
<p>Cao, Juan, Tian Xia, Jintao Li, Yongdong Zhang, and Sheng Tang. 2009. “A Density-Based Method for Adaptive Lda Model Selection.” <em>Neurocomputing</em> 72 (7-9). Elsevier: 1775–81.</p>
</div>
<div id="ref-arun2010finding">
<p>Arun, Rajkumar, Venkatasubramaniyan Suresh, CE Veni Madhavan, and MN Narasimha Murthy. 2010. “On Finding the Natural Number of Topics with Latent Dirichlet Allocation: Some Observations.” In <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>, 391–402. Springer.</p>
</div>
<div id="ref-griffiths2004finding">
<p>Griffiths, Thomas L, and Mark Steyvers. 2004. “Finding Scientific Topics.” <em>Proceedings of the National Academy of Sciences</em> 101 (suppl 1). National Acad Sciences: 5228–35.</p>
</div>
<div id="ref-pilkington201611">
<p>Pilkington, Marc. 2016. “11 Blockchain Technology: Principles and Applications.” <em>Research Handbook on Digital Transformations</em>. Edward Elgar Publishing, 225.</p>
</div>
<div id="ref-iansiti2017truth">
<p>Iansiti, Marco, and Karim R Lakhani. 2017. “The Truth About Blockchain.” <em>Harvard Business Review</em> 95 (1): 118–27.</p>
</div>
<div id="ref-underwood2016blockchain">
<p>Underwood, Sarah. 2016. “Blockchain Beyond Bitcoin.” <em>Communications of the ACM</em> 59 (11). ACM: 15–17.</p>
</div>
<div id="ref-karame2012double">
<p>Karame, Ghassan O, Elli Androulaki, and Srdjan Capkun. 2012. “Double-Spending Fast Payments in Bitcoin.” In <em>Proceedings of the 2012 Acm Conference on Computer and Communications Security</em>, 906–17. ACM.</p>
</div>
<div id="ref-eyal2018majority">
<p>Eyal, Ittay, and Emin Gun Sirer. 2018. “Majority Is Not Enough: Bitcoin Mining Is Vulnerable.” <em>Communications of the ACM</em> 61 (7). ACM: 95–102.</p>
</div>
<div id="ref-lin2017survey">
<p>Lin, Iuon-Chang, and Tzu-Chun Liao. 2017. “A Survey of Blockchain Security Issues and Challenges.” <em>IJ Network Security</em> 19 (5): 653–59.</p>
</div>
<div id="ref-yli2016current">
<p>Yli-Huumo, Jesse, Deokyoon Ko, Sujin Choi, Sooyong Park, and Kari Smolander. 2016. “Where Is Current Research on Blockchain Technology?—a Systematic Review.” <em>PloS One</em> 11 (10). Public Library of Science: e0163477.</p>
</div>
<div id="ref-hampton2016understanding">
<p>Hampton, Nikolai. 2016. “Understanding the Blockchain Hype: Why Much of It Is Nothing More Than Snake Oil and Spin.” <em>Computerworld</em> 5.</p>
</div>
<div id="ref-sentimentr2018">
<p>Rinker, Tyler W. 2018. <em>sentimentr: Calculate Text Polarity Sentiment</em>. Buffalo, New York. <a href="http://github.com/trinker/sentimentr" class="uri">http://github.com/trinker/sentimentr</a>.</p>
</div>
<div id="ref-deveaud2014accurate">
<p>Deveaud, Romain, Eric SanJuan, and Patrice Bellot. 2014. “Accurate and Effective Latent Concept Modeling for Ad Hoc Information Retrieval.” <em>Document Numerique</em> 17 (1). Lavoisier: 61–84.</p>
</div>
<div id="ref-shiny2017">
<p>Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2017. <em>Shiny: Web Application Framework for R</em>. <a href="https://CRAN.R-project.org/package=shiny" class="uri">https://CRAN.R-project.org/package=shiny</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><a href="http://www.uspto.gov" class="uri">http://www.uspto.gov</a><a href="methods.html#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://www.notsoboringlife.com/list-of-hobbies/,http://www.notsoboringlife.com/list-of-hobbies/" class="uri">http://www.notsoboringlife.com/list-of-hobbies/,http://www.notsoboringlife.com/list-of-hobbies/</a><a href="methods.html#fnref2">↩</a></p></li>
<li id="fn3"><p><a href="http://a-z-animals.com/animals/" class="uri">http://a-z-animals.com/animals/</a><a href="methods.html#fnref3">↩</a></p></li>
<li id="fn4"><p><a href="http://www.medicinenet.com/diseases\_and\_conditions/alpha\_a.htm" class="uri">http://www.medicinenet.com/diseases\_and\_conditions/alpha\_a.htm</a>, <a href="http://www.cdc.gov/DiseasesConditions/az/a.html" class="uri">http://www.cdc.gov/DiseasesConditions/az/a.html</a><a href="methods.html#fnref4">↩</a></p></li>
<li id="fn5"><p><a href="http://www.errequadrosrl.com/" class="uri">http://www.errequadrosrl.com/</a><a href="methods.html#fnref5">↩</a></p></li>
<li id="fn6"><p>The most precise couple of words is <strong>advantage</strong> and <strong>disadvantage</strong>, but the reading is facilitated by using two very different words, therefore we decided to adopt the couple <strong>advantage</strong> and <strong>drawback</strong>.<a href="methods.html#fnref6">↩</a></p></li>
<li id="fn7"><p><a href="http://www.freepatentsonline.com/" class="uri">http://www.freepatentsonline.com/</a><a href="methods.html#fnref7">↩</a></p></li>
<li id="fn8"><p>www.dictionary.com<a href="methods.html#fnref8">↩</a></p></li>
<li id="fn9"><p><a href="https://twitter.com" class="uri">https://twitter.com</a><a href="methods.html#fnref9">↩</a></p></li>
<li id="fn10"><p><a href="https://dev.twitter.com/streaming/public" class="uri">https://dev.twitter.com/streaming/public</a><a href="methods.html#fnref10">↩</a></p></li>
<li id="fn11"><p><a href="http://www.trade.gov/green/sm-101-module.asp" class="uri">http://www.trade.gov/green/sm-101-module.asp</a><a href="methods.html#fnref11">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sota.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="future-developments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["thesis_source.pdf", "thesis_source.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
