# Glossary {-}

algorithm: In mathematics and computer science, an algorithm is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing and automated reasoning tasks.

artificial intelligence: Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals.  In computer science  AI research is defined as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term "artificial intelligence" is applied when a machine mimics "cognitive" functions that humans associate with other human minds, such as "learning" and "problem solving".

bias: Bias is disproportionate weight in favor of or against one thing, person, or group compared with another, usually in a way considered to be unfair.

bibliometrics: Bibliometrics is statistical analysis of written publications, such as books or articles.  Bibliometric methods are frequently used in the field of library and information science, including scientometrics. For instance, bibliometrics are used to provide quantitative analysis of academic literature or for evaluating budgetary spending. Citation analysis is a commonly used bibliometric method which is based on constructing the citation graph, a network or graph representation of the citations between documents. Many research fields use bibliometric methods to explore the impact of their field,4 the impact of a set of researchers, the impact of a particular paper, or to identify particularly impactful papers within a specific field of research.5 Bibliometrics also has a wide range of other applications, such as in descriptive linguistics, the development of thesauri, and evaluation of reader usage.Contents1 Usage History See also4 ReferencesUsage

blockchain: A blockchain,originally block chain is a growing list of records, called blocks, which are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a merkle tree root hash).

business intelligence: Business intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information. BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.

business process management: Business process management (BPM) is a discipline in operations management in which people use various methods to discover, model, analyze, measure, improve, optimize, and automate business processes. BPM focuses on improving corporate performance by managing business processes. Any combination of methods used to manage a company's business processes is BPM. Processes can be structured and repeatable or unstructured and variable. Though not required, enabling technologies are often used with BPM.

citation analysis: Citation analysis is the examination of the frequency, patterns, and graphs of citations in documents. It uses the pattern of citations, links from one document to another document, to reveal properties of the documents.  A typical aim would be to identify the most important documents in a collection.  A classic example is that of the citations between academic articles and books. The judgements produced by judges of law to support their decisions refer back to judgements made in earlier cases so citation analysis in a legal context is important. Another example is provided by patents which contain prior art, citation of earlier patents relevant to the current claim.

cloud computing: Cloud computing is shared pools of configurable computer system resources and higher-level services that can be rapidly provisioned with minimal management effort, often over the Internet.  Cloud computing relies on sharing of resources to achieve coherence and economies of scale, similar to a public utility.

cognitive computing: Cognitive computing (CC) describes technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing.  These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), humanâ€“computer interaction, dialog and narrative generation, among other technologies.Contents1 Definition Use cases Cognitive analytics4 See also5 References6 Further readingDefinition

communication: Communication (meaning "to share") is the act of conveying meanings from one entity or group to another through the use of mutually understood signs and semiotic rules.

computer science: Computer science is the study of the theory, experimentation, and engineering that form the basis for the design and use of computers. It is the scientific and practical approach to computation and its applications and the systematic study of the feasibility, structure, expression, and mechanization of the methodical procedures (or algorithms) that underlie the acquisition, representation, processing, storage, communication of, and access to, information. An alternative, more succinct definition of computer science is the study of automating algorithmic processes that scale. A computer scientist specializes in the theory of computation and the design of computational systems.  See glossary of computer science.

confidence: Confidence has a common meaning of a certainty about handling something, such as work, family, social events, or relationships. Some have ascribed confidence as a state of being certain either that a hypothesis or prediction is correct or that a chosen course of action is the best or most effective. Self-confidence is having confidence in one's self. Arrogance or hubris in this comparison is having unmerited confidence&#160â€“ believing something or someone is capable or correct when they are not. Overconfidence or presumptuousness is excessive belief in someone (or something) succeeding, without any regard for failure. Confidence can be a self-fulfilling prophecy as those without it may fail or not try because they lack it and those with it may succeed because they have it rather than because of an innate ability. Contents1 Extent Core confidence Lack of self-confidence4 In relation to others5 Possible explanation6 See also7 ReferencesExtent

cosine similarity: Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0 is 1, and it is less than 1 for any angle in the interval (0,Ï€] radians. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors oriented at 90Â° relative to each other have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in. The name derives from the term "direction cosine": in this case, unit vectors are maximally "similar" if they're parallel and maximally "dissimilar" if they're orthogonal (perpendicular). This is analogous to the cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular.

data: Data is a set of values of qualitative or quantitative variables.

data science: Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining.

design: Design is the creation of a plan or convention for the construction of an object, system or measurable human interaction. Design has different connotations in different fields (see design disciplines below). In some cases, the direct construction of an object (as in pottery, engineering, management, coding, and graphic design) is also considered to use design thinking.

design methods: Design methods is a broad area that focuses on:Divergence â€“ Exploring possibilities and constraints of inherited situations by applying critical thinking through qualitative and quantitative research methods to create new understanding (problem space) toward better design solutionsTransformation â€“ Redefining specifications of design solutions which can lead to better guidelines for traditional and contemporary design activities (architecture, graphic, industrial, information, interaction, et al.) and/or multidisciplinary  responseConvergence â€“ Prototyping possible scenarios for better design solutions that incrementally or significantly improve the originally inherited situationSustainability â€“ Managing the process of exploring, redefining and prototyping of design solutions continually over timeArticulation - the visual relationship between the parts and the whole.

design science: Design Science was introduced in 1957 by R. Buckminster Fuller who defined it as a systematic form of designing. He expanded on this concept in his World Design Science Decade proposal to the International Union of Architects in 1961. The term  was later used in S. A. Gregory's 1966 book of the 1965 Design Methods Conference 4 where he drew the distinction between scientific method and design method. Gregory was clear in his view that design was not a science and that design science referred to the scientific study of design. Herbert Simon in his 1968 Karl Taylor Compton lectures 5 used and popularized these terms in his argument for the scientific study of the artificial (as opposed to the natural). Over the intervening period the two uses of the term (systematic designing and study of designing) have co-mingled to the point where design science has come to have both meanings.Contents1 Science of design Design as science Design as science in information systems4 See also5 ReferencesScience of design

document: A document is a written, drawn, presented, or memorialized representation of thought. The word originates from the Latin documentum, which denotes a "teaching" or "lesson": the verb doceÅ denotes "to teach". In the past, the word was usually used to denote a written proof useful as evidence of a truth or fact. In the computer age, "document" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, "document" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. "Documentation" is distinct because it has more denotations than "document". Documents are also distinguished from "realia", which are three-dimensional objects that would otherwise satisfy the definition of "document" because they memorialize or represent thought documents are considered more as  dimensional representations. While documents are able to have large varieties of customization, all documents are able to be shared freely, and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinion, etc. all can be expressed in documents. Contents1 Abstract definitions Kinds Drafting4 History5 In law6 See also7 References8 Further readingAbstract definitions

efficacy: Efficacy is the ability to get a job done satisfactorily. The word comes from the same roots as effectiveness, and it has often been used synonymously, although in pharmacology a distinction is now often made between efficacy and effectiveness. The word efficacy is used in pharmacology and medicine to refer both to the maximum response achievable from a pharmaceutical drug in research settings, and to the capacity for sufficient therapeutic effect or beneficial change in clinical settings.Contents1 Pharmacology Medicine See also4 ReferencesPharmacologyMain article: Intrinsic activity

efficency: Efficiency is the (often measurable) ability to avoid wasting materials, energy, efforts, money, and time in doing something or in producing a desired result. In a more general sense, it is the ability to do things well, successfully, and without waste.[1][][][4][5] In more mathematical or scientific terms, it is a measure of the extent to which input is well used for an intended task or function (output). It often specifically comprises the capability of a specific application of effort to produce a specific outcome with a minimum amount or quantity of waste, expense, or unnecessary effort. Efficiency refers to very different inputs and outputs in different fields and industries.

entrepreneurship: Entrepreneurship is the process of designing, launching and running a new business, which is often initially a small business. The people who create these businesses are called entrepreneurs.need quotation to verify

expert system: In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as ifâ€“then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.45678

feature selection: In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for four reasons:simplification of models to make them easier to interpret by researchers/users,shorter training times,to avoid the curse of dimensionality,enhanced generalization by reducing overfitting (formally, reduction of variance)

forecasting: Forecasting is the process of making predictions of the future based on past and present data and most commonly by analysis of trends. A commonplace example might be estimation of some variable of interest at some specified future date.  Prediction is a similar, but more general term. Both might refer to formal statistical methods employing time series, cross-sectional or longitudinal data, or alternatively to less formal judgmental methods.  Usage can differ between areas of application: for example, in hydrology the terms "forecast" and "forecasting" are sometimes reserved for estimates of values at certain specific future times, while the term "prediction" is used for more general estimates, such as the number of times floods will occur over a long period.

grammar: In linguistics, grammar is the set of structural rules governing the composition of clauses, phrases, and words in any given natural language. The term refers also to the study of such rules, and this field includes phonology, morphology, and syntax, often complemented by phonetics, semantics, and pragmatics.

heuristic: A heuristic technique often called simply a heuristic, is any approach to problem solving, learning, or discovery that employs a practical method, not guaranteed to be optimal, perfect, logical, or rational, but instead sufficient for reaching an immediate goal. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision. Examples that employ heuristics include using a rule of thumb, an educated guess, an intuitive judgment, a guesstimate, stereotyping, profiling, or common sense.

industrial engineering:  Industrial engineering  is a branch of engineering which deals with the optimization of complex processes, systems, or organizations. Industrial engineers work to eliminate waste of time, money, materials, person-hours, machine time, energy and other resources that do not generate value. According to the Institute of Industrial and Systems Engineers, they create engineering processes and systems that improve quality and productivity.

information: Information is any entity or form that provides the answer to a question of some kind or resolves uncertainty. It is thus related to data and knowledge, as data represents values attributed to parameters, and knowledge signifies understanding of real things or abstract concepts. As it regards data, the information's existence is not necessarily coupled to an observer (it exists beyond an event horizon, for example), while in the case of knowledge, the information requires a cognitive observer.

information technology: Information technology (IT) is the use of computers to store, retrieve, transmit, and manipulate data, or information, often in the context of a business or other enterprise. IT is considered to be a subset of information and communications technology (ICT).

knowledge: Knowledge is a familiarity, awareness, or understanding of someone or something, such as facts, information, descriptions, or skills, which is acquired through experience or education by perceiving, discovering, or learning.

knowledge base: A knowledge base (KB) is a technology used to store complex structured and unstructured information used by a computer system. The initial use of the term was in connection with expert systems which were the first knowledge-based systems.Contents1 Original usage of the term Knowledge base properties Internet as a knowledge base4 See also5 NotesOriginal usage of the term

latent dirichlet allocation: In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model.

lexicon: A lexicon, word-hoard, wordbook, or word-stock is the vocabulary of a person, language, or branch of knowledge (such as nautical or medical).  In linguistics, a lexicon is a language's inventory of lexemes.

literature review: A literature review or narrative review is a type of review article.  A literature review is a scholarly paper, which includes the current knowledge including substantive findings, as well as theoretical and methodological contributions to a particular topic. Literature reviews are secondary sources, and do not report new or original experimental work.  Most often associated with academic-oriented literature, such reviews are found in academic journals, and are not to be confused with book reviews that may also appear in the same publication.  Literature reviews are a basis for research in nearly every academic field. A narrow-scope literature review may be included as part of a peer-reviewed journal article presenting new research, serving to situate the current study within the body of the relevant literature and to provide context for the reader. In such a case, the review usually precedes the methodology and results sections of the work.

machine learning: Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to "learn" (e.g., progressively improve performance on a specific task) from data, without being explicitly programmed.

morphology: the study of the way words are built up from smaller meaning-bearing units called morphemes.

named-entity recognition: Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.

natural language processing: Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

neural network: The term neural network was traditionally used to refer to a network or circuit of neurons. The modern usage of the term often refers to artificial neural networks, which are composed of artificial neurons or nodes. Thus the term may refer to either biological neural networks, made up of real biological neurons, or artificial neural networks, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be âˆ’1 and 1.

patent: A patent is a form of intellectual property.  A patent gives its owner the right to exclude others from making, using, selling, and importing an invention for a limited period of time, usually twenty years. The patent rights are granted in exchange for an enabling public disclosure of the invention. People who are employed to do research are often obligated by their employment contracts to assign inventions to their employer. In most countries patent rights fall under civil law and the patent holder needs to sue someone infringing the patent in order to enforce their rights. In some industries patents are an essential form of competitive advantage in others they are irrelevant.

policy: A policy is a deliberate system of principles to guide decisions and achieve rational outcomes. A policy is a statement of intent, and is implemented as a procedure or protocol. Policies are generally adopted by a governance body within an organization. Policies can assist in both subjective and objective decision making. Policies to assist in subjective decision making usually assist senior management with decisions that must be based on the relative merits of a number of factors, and as a result are often hard to test objectively, e.g. work-life balance policy. In contrast policies to assist in objective decision making are usually operational in nature and can be objectively tested, e.g. password policy.

precision agriculture: Precision agriculture (PA), satellite farming or site specific crop management (SSCM) is a farming management concept based on observing, measuring and responding to inter and intra-field variability in crops. The goal of precision agriculture research is to define a decision support system (DSS) for whole farm management with the goal of optimizing returns on inputs while preserving resources.

programming language: A programming language is a formal language, which comprises a set of instructions used to produce various kinds of output. Programming languages are used to create programs that implement specific algorithms.

radio-frequency identification: Radio-frequency identification (RFID) uses electromagnetic fields to automatically identify and track tags attached to objects. The tags contain electronically-stored information. Passive tags collect energy from a nearby RFID reader's interrogating radio waves. Active tags have a local power source (such as a battery) and may operate hundreds of meters from the RFID reader. Unlike a barcode, the tag need not be within the line of sight of the reader, so it may be embedded in the tracked object. RFID is one method for Automatic Identification and Data Capture (AIDC).

regular expression: A regular expression, regex or regexp (sometimes called a rational expression) is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern. Usually this pattern is then used by string searching algorithms for "find" or "find and replace" operations on strings, or for input validation.

research excellence framework: The Research Excellence Framework is the successor to the Research Assessment Exercise. It is an impact evaluation which assesses the research of British higher education institutions. It was first used in 014 to assess the period 008â€“01. REF is undertaken by the four UK higher education funding bodies: Research England, the Scottish Funding Council (SFC), the Higher Education Funding Council for Wales (HEFCW), and the Department for the Economy, Northern Ireland (DfE). 

scientific literature: Scientific literature comprises scholarly publications that report original empirical and theoretical work in the natural and social sciences, and within an academic field, often abbreviated as the literature.  Academic publishing is the process of contributing the results of one's research into the literature, which often requires a peer-review process. Original scientific research published for the first time in scientific journals is called the primary literature. Patents and technical reports, for minor research results and engineering and design work (including computer software), can also be considered primary literature. Secondary sources include review articles (which summarize the findings of published studies to highlight advances and new lines of research) and books (for large projects or broad arguments, including compilations of articles). Tertiary sources might include encyclopedias and similar works intended for broad public consumption.

semantic field: In linguistics, a semantic field is a set of words grouped semantically (by meaning) that refers to a specific subject. The term is also used in anthropology, computational semiotics,4 and technical exegesis.5Contents1 Definition and usage History Semantic shifts4 Anthropological discourse5 See also6 ReferencesDefinition and usage

social media: Social media are interactive computer-mediated technologies that facilitate the creation and sharing of information, ideas, career interests and other forms of expression via virtual communities and networks. The variety of stand-alone and built-in social media services currently available introduces challenges of definition however, there are some common features:Social media are interactive Web .0 Internet-based applications.User-generated content, such as text posts or comments, digital photos or videos, and data generated through all online interactions, is the lifeblood of social media.Users create service-specific profiles for the website or app that are designed and maintained by the social media organization.4Social media facilitate the development of online social networks by connecting a user's profile with those of other individuals or groups.

social science: Social science is a category of academic disciplines, concerned with society and the relationships among individuals within a society. Social science as a whole has many branches, each of which is considered a social science. The social sciences include, but are not limited to: anthropology, archaeology, communication studies, economics, history, human geography, jurisprudence, linguistics, political science, psychology, public health, and sociology. The term is also sometimes used to refer specifically to the field of sociology, the original "science of society", established in the 19th century. For a more detailed list of sub-disciplines within the social sciences see: Outline of social science.

statistical model: A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of some sample data and similar data from a larger population. A statistical model represents, often in considerably idealized form, the data-generating process.

strategic management: In the field of management, strategic management involves the formulation and implementation of the major goals and initiatives taken by an organization's top  management on behalf of owners, based on consideration of resources and an assessment of the internal and external environments in which the organization operates.need quotation to verify

taxonomy: Taxonomy (general) is the practice and science of classification of things or concepts, including the principles that underlie such classification.

technology intelligence: Technology Intelligence (TI) is an activity that enables companies to identify the technological opportunities and threats that could affect the future growth and survival of their business. It aims to capture and disseminate the technological information needed for strategic planning and decision making. As technology life cycles shorten and business become more globalized having effective TI capabilities is becoming increasingly important.

topic model: In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.

trademark: A trademark, trade mark, or trade-mark is a recognizable sign, design, or expression which identifies products or services of a particular source from those of others, although trademarks used to identify services are usually called service marks.45 The trademark owner can be an individual, business organization, or any legal entity. A trademark may be located on a package, a label, a voucher, or on the product itself. For the sake of corporate identity, trademarks are often displayed on company buildings.

trait theory: In psychology, trait theory (also called dispositional theory) is an approach to the study of human personality. Trait theorists are primarily interested in the measurement of traits, which can be defined as habitual patterns of behavior, thought, and emotion. According to this perspective, traits are aspects of personality that are relatively stable over time, differ across individuals (e.g. some people are outgoing whereas others are not), are relatively consistent over situations, and influence behavior. Traits are in contrast to states, which are more transitory dispositions.

word-sense disambiguation: In computational linguistics, word-sense disambiguation (WSD) is an open problem of natural language processing and ontology. WSD is identifying which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings. The solution to this problem impacts other computer-related writing, such as discourse, improving relevance of search engines, anaphora resolution, coherence, inference, et cetera.

wordvec: Wordvec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Wordvec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.

