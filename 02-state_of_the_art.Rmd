# State of the Art{#sota}

The analysis of technical documents require the design of processes that rely both on programming and Natural Language Processing techniques and on the undestanding and knowldege of field experts. While the first techniques are codified and explicit, the second are sometimes implicit and always harder to systematize. In this section i treat these two groups of techniques in the same way to give to the reader a sistematic litterature review on these topics. For this reason the sections of this chapter has the sequent structure: 

- At a first level we have two sections \@ref(sotatools) and \@ref(sotadocuments), reviewing respectivelly the processes of _programming and Natural Language Processing_ and of _undestanding and knowldege of field  experts application_;
- Section \@ref(sotatools) has a subsection for each of the _phases_ showed in figure \@ref(fig:mainworkflow). These subsections goes from \@ref(sotatoolsprogram) to \@ref(sotatoolscomunicate);
- Each subsection from \@ref(sotatoolsprogram) to \@ref(sotatoolscomunicate) contains the relative Natural Language Processing _task_ that are relevant for the analysis of technical documents, for example Document Retrieval \@ref(sotatoolsimportretrieval), Part-Of-Speech-Tagging; \@ref(sotatoolstransformpos) or Named Entity Recognition \@ref(sotatoolsmodelner).
- Each task subsection describes the relevant _techniques_ to perform that task. I use the word techniques to include mainly algorythms and procedures but also more generic methods or frameworks;
- Since the second section \@ref(sotadocuments) describes less systematics phases, task and techniques this section opens with a first subsection \@ref(sotadocumentsunderstand) that focuses on the studies of the problems of using expert knowledge in an analytical process and which are the techniques to convert this knowledge in a format that is usable in a Natural Language Processing workflow. 
- Finally, always section \@ref(sotadocuments) has a subsection for each of the technical _documents_ I analyzed (aggiungi gancio con introduzione).  These subsections goes from \@ref(sotadocumentspatents) to \@ref(sotadocumentsjobs).

## Phases, Tasks, and Techniques {#sotatools}

In this section I make a review of the most important techniques for Natural Language Processing in the context of technical documents analysis. The techniques (mainly algorythms) are grouped in phases (Import, Tidy, Transform, Model, Visualize, Communicate) showed in figure \@ref(fig:mainworkflow) and each phases is dived in the NLP tasks that are the most important for the analysis of technical documents. 
The algorythms i reviewed in this section are summmarised in table tot, where the reader can see the relationship between tasks and techniques. 

### Program {#sotatoolsprogram}

Programming is a key activity to perform in order to effectively and efficiently perform text mining. It is not a phases per se because each phase is implemented trough programming. It is critical that an analysts has in mind the need of maximizing the probability that their analysis is reproducible, accurate, and collaborative. This goals can be reached only trough programming. 
The most used programming languages for text mining and natural language processing are R [@r2008] and Python [@py95]. R and Python are both open-source programming languages with a large community of developers, and new libraries or tools are added continuously to their respective catalog. R is mainly used for statistical analysis and data science while Python is a more general purpose programming language. 
R has been developed by Academics and statisticians over two decades. R has now one of the richest ecosystems to perform data analysis and there are around 12000 packages available in CRAN (open-source repository of R).  The rich variety of libraries makes R the first choice for statistical analysis. Another cutting-edge difference between R and the other statistical products is R-studio. RStudio is a free and open-source integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. Finally, it is widelly recognized the great performances that R has for data visualisation and comunication. 
Python can pretty much do the same tasks as R: data wrangling, engineering, feature selection web scrapping, app and so on. Anyway, Python has great performances in the deployement and implementation of machine learning at a large-scale. Furthermore, Python codes are easier to maintain and more robust than R.

### Import {#sotatoolsimport}

- I tipi di codifica di testo
- Pachetti per import 

#### Document Retrieval {#sotatoolsimportretrieval}

- Letteratura query

### Tidy {#sotatoolstidy}

- Hadley

weight, tfids

DTM

problems such as sparsity

### Transform {#sotatoolstransform}

Transforming in the context of Natural Language Processing  is what in computational linguistic is called text normalization. Normalizing text means converting it to a more convenient, standard form. Most of the task of technical document analysis in fact relies on first separating out or tokenizing sentences and words, strip suffixes from the end of the word, determining the root of a word or transform the text using regular expressions. 

### Sentence Splitting

The analysis of technical documents require as first process, that the input text is segmented in sentences. Since documents do not encode this information in a non ambiguous manner (using dots) due to common abbreviations (e.g.: “Mr., Dr.”), a sentence splitting process that does not rely only on a trivial _dot based_ rule is required. This issue in the technical documents domain is even more problematic due to the presence of formulas, numbers, chemical entity names and bibliographic references. Furthermore, since sentece splitting is one of the first processes of an NLP pipeline, errors in this early stage are propagated in the following steps causing a strong decrease for what concerns their accuracy. One of the most advanced techniques are machine learning techniques: given a training corpus of properly segmented sentences and a learning algorithm, a statistical model is built. By reusing the statistical model, the sentence splitter is able to split sentences on texts not used in the training phase. ItalianNLP lab systems uses this approach [@dell2009ensemble;@attardi2009reverse;@attardi2009accurate]. For this reason this algorythm is used for the most of the application presented in this Thesis.

### Tokenization

Since documents are unstructured information, these has to be divided into linguistic units. The definition of linguistic units is non-trivial, and more advanced techniques can be used (such as n-gram extraction) but most of the times these are words, punctuation and numbers. English words are often separated from each other
by whitespace, but whitespace is not always sufficient. Solving this problems and splitting words in well-defined tokensis defined as tokenization. In most of the application described in the present Thesis, the tokenizer developed by the ItalianNLP lab was integrated [@dell2009ensemble;@attardi2009reverse;@attardi2009accurate]. This tokenizer is regular expression based: each token must match one of the regular expression defined in a configuration file. Among the others, rules are defined to tokenize words, acronyms, numbers, dates and equations.

#### Stemming {#sotatoolstransformstemming}

Stemming is a simpler but cruder methodology for chopping off of affixes. The goal of stemming is reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem of a word and its morphological root do not need to be identical; it is sufficient that related words map to the same stem, even if this stem is not a valid root. One of the most widely used stemming is the simple and efficient Porter algorithm [@porter1980algorithm].

#### Lemmatisation {#sotatoolstransformlemmatisation}

Lemmatization is the task of determining the root of a words. The output allow to find that two words have the same root, despite their surface differences. For example, the verbs _am_, _are_, and _is_ have the shared lemma _be_; the nouns _cat_ and _cats_ both have the lemma _cat_. Representing a word by its lemma is important for many natural language processing tasks. Lemmatisation in fact diminish the problem of sparsity of document-word matrix. Futhermore lemmatisaion is important for document retrieval \@ref(sotatoolsimportretrieval) web search, since we want to find documents mentioning motors if we search for motor. The most recent methods for lemmatization involve complete morphological parsing of the word [@hankamer1989morphological]. 

#### Part-of-Speech Tagging {#sotatoolstransformpos}

The part of speech plays an central role in technical document analysis since it provides very useful information concerning the morphological role of a word and its morphosyntactic context: for example, if a token is a determiner, the next token is a noun or an adjective with very high confidence. 
Part of speech tags are used for many information extraction tools such as named entity taggers (see section \@ref(sotatoolsmodelner)) in order to identify named entities. In typical named entity task these are people and locations since tokens representing named entities follow common morphological patterns (e.g. they start with a capital letter). For the application to technical documents, technical entities (like the possibile failures of a manufact) becomes more relevant. In this context a correct part-of-speech tagger becomes even more important since we can not rely on morphosyntactical rules. In addition part of speech tags can be used to mitigate problems related to polysemy since words often have different meaning with respect to their part of speech (e.g. “track”, “guide”). This information is extremelly valuable in patent analysis, and some patent tailored part-of-speech tagger has been designed (see section \@ref(sotadocumentspatents)).
The litterature on pos-tagger is huge, and goes behoind the scope of the present thesis to make a complete review. In most of the application presentend in this work, was employed the ILC postagger [@attardi2006experiments]. This postagger uses a supervised training algorithm: given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus. 


#### Regular Expressions {#sotatoolstransformregex}

Regular expression (regex) is a language for specifying text search strings, an algebraic notation for characterizing a set of strings. This language whidelly used in modern word processor and text processing tools.. They are particularly useful for searching in texts, when we have a pattern to search for. 


A pattern could be at  A regular expression search function will search through the
corpus, returning all texts that match the pattern. The corpus can be a single document
or a collection. For example, the Unix command-line tool grep takes a regular
expression and returns every line of the input document that matches the expression.
A search can be designed to return every match on a line, if there are more than
one, or just the first match. In the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the first
match. We’ll show regular expressions delimited by slashes but note that slashes are
not part of the regular expressions.

### Model {#sotatoolsmodel}

Classi di modelli. Pedro Domingos

[@james2013introduction]

#### N-Grams {#sotatoolstransformngrams}

An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like “credit card”, “3d printing”, or ”printing machine”, and a 3-gram (or trigram) is a three-word sequence of words like “3d printing machine”. Statistical model can be used to extract the n-grams contained in a document. 
A first approach has the of predicting the next item in a sequence in the form of a (n − 1)–order Markov model[@lafferty2001document]. The algorythm begin with the task of computing P(w|h), the probability of a word w given a word h.The way to extimate this probability is using relative frequency counts. To do that the algorythms count the number of times h is followed by the w. With a large enough corpus it is possibile to build valuable models, able to extract n-grams [@bellegarda2004statistical].
While this method of estimating probabilities directly from counts works for many natural language applications, in many cases a huge dimension of the corpus does make the model useful, and this is particularly true for technical documents [@brants2012large]. This is because technical language has a strong ratio of evolution; as new artifcat are invented, new chunks are created all the time, and has no sense to continuolly count every word co-occurrence to update our model[@gibson1994tools].
A more usefull method for chunk extraction fro technical document uses part-of-speech-tagging and regular expression. Once a document is pos-taggerd each word is associated whit a particular part of speech: each sentence is rapresented as a sequence of part-of-spech. Once we have this rappresentation, it is possible to etract only certain sequences of part-of-speeches, the ones that whit an high level of confidence are n-grams.

[TROVA LAVORI SU QUESTO ARGOMENTO]

#### Document Classification {#sotatoolsmodeldocclass}

Classification is a general process that has the goal of taking an object, extract features, and assign to the observation one of a set of discrete classes. This process is largerly used for documents [@borko1963automatic] and there exist many methods for document classification [@aggarwal2012survey]. 

Regardless of technological sector, most organizations today are facing the problem of overload of information. When it comes to classify huge ammount of documents or to separate the useful documents from the irrelevant, document classification techniques can reduce the proecess cost and time.

The simplest method for classifying text is to use expert defined rules. These systems are called expert systems or knowledge engineering approach. Expert rule-based systems are programs that consist of rules in the IF form condition THEN action (if condition, then action). Given a series of facts, expert systems, thanks to the rules they are made of, manage to deduce new facts. 
The expert systems therefore differ from other similar programs, since, by referring to technologies developed according to artificial intelligence, they are always able to exhibit the logical steps that underlie their decisions: a purpose that, for example, is not feasible from the human mind or black box-systems. There are many type of documents for which expert based classifiers constitute a stateof-the-art system, or at least part of it. Anyway, rules can be useless in situations such as:
- data change over time
- the rules are too many and interrelated

Most systems of documents classification are instead done via supervised learning: we have a data set of input observations, each associated with some correct output  (training set). The goal of the algorithm is to build a statical model able to learn how to map from a new observation (test set) to a correct output. The advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains.

In the supervised document classification process, we have a training set of N documents that have each been tipically hand-labeled with a class: (d1, c1),....,(dN, cN). I say tipically, because other less expensive methods could be designed, as we will show for the task of Named Entity Recongition (another supervised learning task, that classifies words intstead of documents \@ref(sotatoolsmodelner)). The goal of the supervised document classification task is to learn a statistical model capable of assign a new document d to its correct class c ∈ C. There exist a class of these classifier, probabilistic classifiers, that additionally will tell us the probability of the observation being in the class. 

Many kinds of machine learning algorithms are used to build classifiers [@aggarwal2012survey], such as:

- _Decision Tree Classifiers_: Decision tree documents classifier are systems that has as output a classification tree [@sebastiani2002machine]. In this tree internal nodes are terms contained in the corpus under analysis, branches departing are labeled by the weight (see section \@ref(sotatoolstidy)) that the term has in the test document, and leafs are labeled by categories. There exists many methods to automatically learn trees from data. A tree can be build by splitting the data source into subsets based on an test feature. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions.

- _Rule Based Classifiers_: Rule-based classifiers are systems in wich the patterns which are most likely to be related to the different classes are extracted from a set of test documents. The set of rules corresponds to the lefthand side to a word pattern, and the right-hand side to a class label. These rules are used for the purposes of classification. In its most general form, the left hand side of the rule is a boolean condition, which is expressed in Disjunctive Normal Form (DNF). However, in most cases, the condition on the left hand side is much simpler and represents a set of terms, all of which must be present in the document for the condition to be satisfied [@yang2004building].

- _Support Vector Machines (SVM) Classifiers_: SVM Classifiers attempt to partition the data
space with the use of linear or non-linear delineations between the different classes. The main principle of SVM algorythm is to determine separators in the feature space which can best separate the different classes [@joachims1998text,@manevitz2001one].

- _Baeysian Classifiers_: Bayesian classifiers build a probabilistic classifier based on modeling the underlying word features in different classes. The idea is then to classify documents using  the posterior probability of the documents belonging to the different classes on the basis of the word presence in the documents [@pop2006approach].

- _Neural Netword Classifiers_: The basic unit in a neural network is a neuron. Each neuron receives a set of inputs, which are denoted by the vector _Xi_, which are the values of the feature vector for a certain instance. Each neuron is also associated with a set of weights, which are used in order to compute a function of its inputs. Neural Networks Classifier are able, thank to a process called learning phase, to ajust their weights in such a way that the function is abble to effectively classify new instances. Neural networks are nowdays one of the best method for documents classification, and are used in a wide variety of appliations [@manevitz2007one]. Great performances has also been reached by deep neural networks, which are neural networks whit a large number o neurons arranged in multiple layers [@lai2015recurrent. @kim2014convolutional]. 


#### Sentiment Analsysis {#sotatoolsmodelsentanal}

Sentiment analysis techniques are algorythms able to measure from text, people’s opinions and emotions toward events, topics, products and their attributes [@pang2008opinion].  For example, businesses (particularly marketeers) are intrested in finding costumers opinions about their products and services.

Thanks to the growth of social media (forums, blogs and social networks), individuals and organizations are producing a huge quantity of their written opinion. This has make it possible to scholars to study this phenomena and to develop many different and effective sentiment analysis techniques [@liu2012survey]. In the past decade, a considerable amount of research has been done by scholars and there are also numerous commercial companies that provide opinion mining services. However, measuring sentiment in documents and distilling the information contained in them remains a challenging task because of the diversity of documents from which is possibile to extract sentiment. 

The approaches to perform sentiment analysis are many. Among all, the most intresting for tehcnical documents analysis are:


- _Dictionary Base Approaches_ : This approach has the aim of collecting words that are clues for positive or negative sentiment. In litterature these words are called opinion words, opinion-bearing words or sentiment words. Examples of positive opinion words are: good, nice and amazing. Examples of negative opinion words are bad, poor, and terrible. Collectively, they are called the opinion lexicon. The most simple and widely used techniques to produce a dictionary of opinion words is based on bootstrapping using a small set of seed opinion words and an online dictionary such as WordNet [@miller1995wordnet].
The works that used this approach [@hu2004mining, @kim2004determining], adopts a process that consist in two phases: first collect set of opinion words manually, then grow this set by searching in the WordNet for their synonyms and antonyms. The process stops when no more new words are found. 
After that a manual inspection can be carried out to remove and/or correct errors. Scholars has developed several opinion lexicons [@ding2008holistic, @baccianella2010sentiwordnet, @hu2004mining, @philip1966general, @wiebe1999development] The lexicon based approach has the characteristic of beeing strongly context specific. This is an advantage when the goal is to design a method able to extract sentiment in a specific context [@chiarello2017product], but is a major  shortcoming if the goal is to design a general purpose method.

- _Supervised Learning Approaches_: Sentiment analysis can be formulated as a document classification problem with three classes: positive, negative and neutral[@mullen2004sentiment]. Treining and test sets of documents are tipically collected from product reviews, movies revies or are created by scratch using manuall annotation. Any learning algotyrhm can be applied to sentiment classification (naive Bayesian classification, and support vector machines [@prabowo2009sentiment]). The crucial phase for Supervised Learning sentiment analysis is the features rapresentaion of the data. It was shown [@pang2002thumbs] that using unigrams (a bag of individual words) as features in classification performed well with either naive Bayesian or SVM. Subsequent research used many more features and techniques in learning [@pang2008opinion]. 


#### Text Clustering {#sotatoolsmodelnetanal}

The goal of clustering methods is to find groups of similar objects in the data thanks to the measure of a similarity function [@jain1988algorithms, @kaufman2009finding]. Clustering techniques has been widelly applied in the text domain, where the objects of the clustering can be documents (at different level of granulairyt) or terms. In the context of technical documents analysis Clustering is especially useful documents retrieval [@anick1997exploiting, @cutting1993constant].
Clustering problems has been and are studied widely outside the text domain. Methods for clustering have been developed focusing on quantitative/non-textual data [@guha1998cure, @han2001spatial, @zhang1996birch]. 

In the context of text analysis, the problem of clustering finds applicability for a number of tasks, such as Document Organization and Browsing [@cutting2017scatter], Corpus Summarization using documents maps [@schutze1997projections] or word clusters [@baker1998distributional, @bekkerman2001feature]. It is usefull also to use a Soft clustering approach, that associates each document with multiple clusters with a given probability. 

However, standard techniques for cluster analysis (k-means or hierachical clusterin) do not typically work well for clustering textual data in general or more specific technical documents. This is because of the unique characteristics of textual data which implies the design of specialized algorithms for the task. 

The distinguishing characteristics of the text representation are the following [@aggarwal2012survey]:

- There is a problem of course of dimensionality. The dimensionality of the bug-of-words representation is very large and the underlying data is sparse. In other words, the lexicon from which the documents are drawn may be of the order of milions, but a given document may contain only a few hundred words.This problem is even more serious for technical documents in which the lexicon is even more large. 

- The words are correlated with one another and thus the number of concepts (or principal components) in the data is much smaller than the feature space. This necessitates the careful design of algorithms which can account for word correlations in the clustering process.

- The number of words (or non-zero entries) in the different documents may vary widely. Therefore, it is important to normalize the document representations appropriately during the clustering task.

The problems of sparsity and high dimensionality necessitate the design of specific algorithms text processing. The topic has been heavily studied in the information retrieval literature where many techniques have been proposed [@ricardo2011modern]. 

#### Named Entity Recognition {#sotatoolsmodelner}

Named Entity Recognition is the task of identifying entity names like people, organizations, places, temporal expressions or numerical expressions. An example of an annotated sentence for a NER extraction system tailored for user entity extraction from patents, is the following:

_Traditionally, < user > guitar players < user/ > or < user > players < user/ > of other stringed instruments may perform in any of a number of various positions, from seated, with the stringed instru- ment supported on the leg of the<user>performer<user/>, to standing or walking, with the stringed instrument suspended from a strap._

Methods and algorithms to deal with the entity extraction task are different, but the most effective are the ones based on supervised methods. Supervised methods tackle this task by extracting relevant statistics from an annotated corpus. These statistics are collected from the computation of features values, which are strong indicators for the identification of entities in the analyzed text. Features used in NLP for NER purposes are divided in two main categories:
- Linguistically motivated features, such as n-gram of words (se- quences of n words), lemma and part of speech 
- External resources features as, for example, external lists of entities that are candidates to be classified in the extraction process.

The annotation methods of a training corpus can be of two different kinds: human based, which is time expensive, but usually effective in the classification phase; automatically based, which can lead to annotation errors due to language ambiguity. For instance driver can be classified both as a user (the operator of a motor vehicle), or not a user (a program that determines how a computer will communicate with a peripheral device). Different training algorithms, such as Hidden Markov Models [@eddy1996hidden], Conditional Random Fields (CRF) [@lafferty2001conditional] Support Vector Machines (SVM) [@hearst1998support], or Bidirectional Long Short Term Mermory-CRF Neural Networks [@lample2016neural, @misawa2017character ] are used to build a sta- tistical model based on features that are extracted from the analyzed documents in the training phase.

#### Topic Modelling {#sotatoolsmodeltopicmodel}

Topic modeling is a form of dimension reduction that uses probabilistic models to find the co-occurrence patterns of terms that correspond to semantic topics in a collection of documents [@crain2012dimensionality]. To understand topic modelling it is usefull to understand its differences with clustering \ref(@sotatoolsmodelnetanal) and the problem they both solves: the course of dimensionality. Both these techniques has in fact the goal of representing documents in such a way that they reveals their internal structure and interrelations. 
Clustering measures the similarity (or dissimilarity) between documents to place documents into groups. Rapresenting each document by considering the belonging to a group, clustering induces a low-dimensional representation for documents. However, it is often difficult to characterize a cluster in terms of meaningful features because the clustering is independent of the document representation, given the computed similarity.
Topic modeling integrates soft clustering (assigning each element to a cluster with a given probability and not with a boolean variable) with dimension reduction. Each document is associated with a number of latent topics: a topic can be seed as both document clusters and compact group of words identified from a corpus. Each document is assigned to the topics with different weights: this feature can be seen both as the degree of membership in the clusters, as well as the coordinates of the document in the reduced dimension space. The result is an understandable representation of documents that is useful for analyzing the themes in documents.
Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model [@blei2003latent]. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language. 

### Visualize {#sotatoolsvisualize}

Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests. This problem influences also text mining[@parker2017opinionated]. The process of developing the output of the process (the paper, the report, the dashboard, or other deliverable) is less frequently anlyzed. 

### Comunicate {#sotatoolscomunicate}

## Documents {#sotadocuments}

### Understand {#sotadocumentsunderstand}

Expertise (collins)

Sheela Jasanow

Taleb?

#### The problem of byases {#sotadocumentsunderstandbyas}

Each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs. The average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them. Moreover, it is also known that human analysis of text information is subject to con- siderable biases, e.g., people often pay greater attention to opinions that are consistent with their own preferences. People also have difficulty, owing to their mental and physical limitations, producing consistent results when the amount of information to be processed is large. Automated opinion mining and summarization systems are thus needed, as subjective biases and mental limitations can be overcome with an objective sentiment analysis system.

#### The Importance of Lexicons for Technical Documents Analysis  {#sotadocumentsunderstandlexicons}

#### Expert Systems

### Patents {#sotadocumentspatents}

### Papers {#sotadocumentspapers}

- Parte Barilari.Keyword base, defini i confini di area tecnologica. Hot-topics su paper (guaiè)
- Biblio 

### Projects {#sotadocumentsprojects}


### Wikipedia {#sotadocumentswiki}

### Twitter {#sotadocumentstwitter}

### Job Profiles {#sotadocumentsjobs}
