# (PART) State of the Art {-}

The analysis of technical documents require the design of processes that rely both on Natural Language Processing techniques and on technical field expertise. While the first techniques are codified and explicit, the second are sometimes implicit and always hard to systematize. In this section i treat these two groups of techniques in the same way to give to the reader a systematic literature review on these topics. This chapter can be seen as a background chapter as it illustrates the information that the reader should know to understand the rest of the dissertation. Furthermore it is important to define the value in terms of new knowledge that documents can bring to companies. For this reason the chapter of this part has the subsequent structure: 

- At a first level there are two sections \@ref(sotatools) and \@ref(sotadocuments), reviewing respectively the processes of _programming and Natural Language Processing_ and the _documents_;

- Section \@ref(sotatools) has a subsection for each of the _phases_ showed in figure \@ref(fig:mainworkflow). These subsections goes from \@ref(sotatoolsprogram) to \@ref(sotatoolscomunicate);

- Each subsection from \@ref(sotatoolsprogram) to \@ref(sotatoolscomunicate) contains the relative Natural Language Processing _task_ that are relevant for the analysis of technical documents, for example Document Retrieval \@ref(sotatoolsimportretrieval), Part-Of-Speech-Tagging; \@ref(sotatoolstransformpos) or Named Entity Recognition \@ref(sotatoolsmodelner).

- Each task subsection describes the relevant _techniques_ to perform that task. I use the word techniques to include mainly algorithms and procedures but also more generic methods or frameworks;

- Finally,  section \@ref(sotadocuments) has a subsection for each of the analyzed technical _documents_. These subsections goes from \@ref(sotadocumentspatents) to \@ref(sotadocumentstwitter).

The goal of this state of the art is to demonstrate that a large literature on Techniques able to solve the problems described in section \@ref(introproblems) exists, but week or no effort exist in the literature in the systemic integration of these tools.

# Phases, Tasks, and Techniques {#sotatools}

In this section I make a review of the most important techniques for Natural Language Processing in the context of technical documents analysis. The techniques (mainly algorithms) are grouped in phases (Import, Tidy, Transform, Model, Visualize, Communicate) showed in figure \@ref(fig:mainworkflow) and each phases is dived in the NLP tasks that are the most important for the analysis of technical documents. This standard process has been disclosed in the framework of the tidyverse [@wickham2016r].

## Program {#sotatoolsprogram}

Programming is a key activity to perform in order to effectively and efficiently perform text mining. It is not a phases per se because each phase is implemented trough programming. It is critical that an analysts has in mind the need of maximizing the probability that their analysis is reproducible, accurate, and collaborative. This goals can be reached only trough programming. 
The most used programming languages for text mining and natural language processing are R [@r2008] and Python [@py95]. R and Python are both open-source programming languages with a large community of developers, and new libraries or tools are added continuously to their respective catalog. R is mainly used for statistical analysis and data science while Python is a more general purpose programming language. 

R has been developed by Academics and statisticians over two decades. R has now one of the richest ecosystems to perform data analysis and there are around 12000 packages available in CRAN (open-source repository of R).  The rich variety of libraries makes R the first choice for statistical analysis. Another cutting-edge difference between R and the other statistical products is R-studio. RStudio is a free and open-source integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. Finally, it is widely recognized the great performances that R has for data visualisation and communication. 

Python can pretty much do the same tasks as R: data wrangling, engineering, feature selection web scrapping, app and so on. Anyway, Python has great performances in the deployment and implementation of machine learning at a large-scale. Furthermore, Python codes are easier to maintain and more robust than R.

## Import {#sotatoolsimport}

The first activities to perform in a text mining pipeline is to find all the documents that contains useful information for the analysis and then import the corpus (the set of documents) in to the computer program. The present section is thus focused on techniques for document retrieval \@ref(sotatoolsimportretrieval) and on the most popular documents digital formats \@ref(sotatoolsimportformat).

### Document Retrieval {#sotatoolsimportretrieval}

Document retrieval is the process of matching a user query against a set of documents. A document retrieval system has two main tasks:

1- Find the documents that are relevant with respect to the user queries
2- Measure the relevance of the matching results

Building a query means to use field specific knowledge and logical rules to write a text string that is the composition of keywords and Boolean operators. The set of keywords (single words or phrases) is chosen in such a way that these are likely to be contained in the searched documents. Boolean operators can also be used to increment the performance of the query. The AND operator, for example is used to retrieve all the document that contains both of the terms at the left and the right of it, OR for document that contains at least one of the two words. 
Another important tool for making a good query are regular expressions. Regular expression (regexp) is a language for specifying text search strings, an algebraic notation for characterizing a set of strings. This language widely used in modern word processor and text processing tools. A regular expression search function will search through the corpus, returning all texts that match the pattern. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression.
To evaluate the performance of a query is useful to understand the concepts of precision and recall. 

Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned. Due to the ambiguities of natural language, full-text-search systems typically includes options like stop words to increase precision. Stop-words are words that filter all the document which contains them. On the other side, stemming to increase recall \@ref(sotatoolstransformstemming). The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision [@yuwono1996search]. Usually when a user performs a query, the main problem are false positives (the results that are returned by the systems but are not relevant to the user). False positives has a negative impact on the precision of the query. The retrieval of irrelevant documents is particularly strong for technical documents due to the inherent ambiguity of technical language. For this reason to understand and to use the rules of query building are fundamental to the technical document analysis, since without a good query is rare to have a good set of documents to analyze. 

### Documents Format {#sotatoolsimportformat}

For the purpose of the present thesis documents are considered in a digital format, and there is no need to read it from a analogical source. From the computer science point of view, text is a human-readable sequence of characters and the words they form that can be encoded into computer-readable formats. There is no standard definition of a text file, though there are several common formats. The most common types of encoding are:

- ASCII, UTF-8: plain text formats

- .doc for Microsoft Word: Structural binary format developed by Microsoft (specifications available since 2008 under the Open Specification Promise)

- HTML (.html, .htm): open standard, ISO from 2000

- Office Open XML .docx: XML-based standard for office documents

- OpenDocument .odt: XML-based standard for office documents

- PDF: Open standard for document exchange. ISO standards include PDF/X (eXchange), PDF/A (Archive), PDF/E (Engineering), ISO 32000 (PDF), PDF/UA (Accessibility) and PDF/VT (Variable data and transactional printing). PDF is readable on almost every platform with free or open source readers. Open source PDF creators are also available.

- Scalable Vector Graphics (SVG): Graphics format primarily for vector-based images.

- TeX: Popular open-source typesetting program and format. First successful mathematical notation language.

For the R software there exist many packages that helps to import documents in several formats [@readr2017r]. 

## Tidy {#sotatoolstidy}

After that data are imported they have to be processed in such a way that it would be possible to perform the main task of data analysis (transformation, modelling and visualisation). This task of tidying data (usually referred to as data pre-processing) can be very time expensive, so it is important to have clear methods and techniques to perform this task. 

Tidy data sets have structure and working with them is easy; they’re easy to manipulate, model and visualize [@wickham2014tidy]. Tidy data sets main concept is to arrange data in a way that each variable is a column and each observation (or case) is a row. The characteristics of tidy data can be thus summarised as the points [@leek2015elements]:

- Each variable you measure should be in one column
- Each different observation of that variable should be in a different row
- If you have multiple tables, they should include a column in the table that allows them to be linked

The main advantages of structuring the data in this way is that a consistent data structure make it easier to use the tools (programs) that work with it because they have an underlying uniformity. This lead to an advantage in reproducibility of code.

In the context of the present Thesys, the Tidy task mean applying this process to text. It is clear that to tidy unstructured information is harder then to tidy structured data [@silge2016tidytext]. On the other side, using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use. Treating text as data frames of individual words allows us to manipulate, summarize, and visualize the characteristics of text easily and integrate natural language processing into effective workflows already used.

Tidy text format is designed as being a table with one-token-per-row. A token  unit of text that is meaningful for the analysis to be performed (for example letters, words, n-gram, sentences, or paragraphs ). Tokenization is the process of splitting text into tokens. This one-token-per-row structure is in different from the ways documents are often stored in current analyses, mainly strings or document-term matrix. The term document matrix has each corpus word represented as a row with documents as columns. The document term matrix is the transposition of the TDM so each document is a row and each word is a column. The term document matrix or document term matrix is the foundation of bag of words text mining. The bag-of-words model is a simplifying representation of documents: a text is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity [@mctear2016conversational]. 

## Transform {#sotatoolstransform}

Transforming in the context of Natural Language Processing  is what in computational linguistic is called text normalization. Normalizing text means converting it to a more convenient, standard form. Most of the task of technical document analysis in fact relies on first separating out or tokenizing sentences and words, strip suffixes from the end of the word, determining the root of a word or transform the text using regular expressions. 

### Sentence Splitting {#sotatoolstransformsentencesplit}

The analysis of technical documents require as first process, that the input text is segmented in sentences. Since documents do not encode this information in a non ambiguous manner (using dots) due to common abbreviations (e.g.: “Mr., Dr.”), a sentence splitting process that does not rely only on a trivial _dot based_ rule is required. This issue in the technical documents domain is even more problematic due to the presence of formulas, numbers, chemical entity names and bibliographic references. Furthermore, since sentence splitting is one of the first processes of an NLP pipeline, errors in this early stage are propagated in the following steps causing a strong decrease for what concerns their accuracy. One of the most advanced techniques are machine learning techniques: given a training corpus of properly segmented sentences and a learning algorithm, a statistical model is built. By reusing the statistical model, the sentence splitter is able to split sentences on texts not used in the training phase. ItalianNLP lab systems uses this approach [@dell2009ensemble; @attardi2009reverse; @attardi2009accurate]. For this reason this algorithm is used for the most of the application presented in this Thesis.

### Tokenization

Since documents are unstructured information, these has to be divided into linguistic units. The definition of linguistic units is non-trivial, and more advanced techniques can be used (such as n-gram extraction) but most of the times these are words, punctuation and numbers. English words are often separated from each other
by white space, but white space is not always sufficient. Solving this problems and splitting words in well-defined tokens defined as tokenization. In most of the application described in the present Thesis, the tokenizer developed by the ItalianNLP lab was integrated [@dell2009ensemble; @attardi2009reverse; @attardi2009accurate]. This tokenizer is regular expression based: each token must match one of the regular expression defined in a configuration file. Among the others, rules are defined to tokenize words, acronyms, numbers, dates and equations.

### Stemming {#sotatoolstransformstemming}

Stemming is a simpler but cruder methodology for chopping off of affixes. The goal of stemming is reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem of a word and its morphological root do not need to be identical; it is sufficient that related words map to the same stem, even if this stem is not a valid root. One of the most widely used stemming is the simple and efficient Porter algorithm [@porter1980algorithm].

### Lemmatisation {#sotatoolstransformlemmatisation}

Lemmatization is the task of determining the root of a words. The output allow to find that two words have the same root, despite their surface differences. For example, the verbs _am_, _are_, and _is_ have the shared lemma _be_; the nouns _cat_ and _cats_ both have the lemma _cat_. Representing a word by its lemma is important for many natural language processing tasks. Lemmatisation in fact diminish the problem of sparsity of document-word matrix. Furthermore lemmatisaion is important for document retrieval \@ref(sotatoolsimportretrieval) web search, since the goal is to find documents mentioning motors if the search is for motor. The most recent methods for lemmatization involve complete morphological parsing of the word [@hankamer1989morphological]. 


### Words importance metrics  {#sotatoolstransformwi}

Once that a document has been tokenized and the tokens has been transformed, an analyst usually wants to measure how important a word is to a document in a collection or corpus. Some of the metrics adopted are:

- _Term Frequency_: the number of times that a term occurs in document. 
- _Boolean frequency_: 1 if the term occurs in the document and 0 otherwise;
- _Term frequency adjusted for document length_: is raw count normalized for the number of words contained in the document
- _Logarithmically scaled frequency_: is raw count normalized for the natural logarithm of one plus the number of words contained in the document
- _Inverse document frequency_: is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient. It is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. 
- _Term frequency–Inverse document frequency_: the product between _term frequency and inverse document frequency_.  A high weight in tf–idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms.


### Part-of-Speech Tagging {#sotatoolstransformpos}

The part of speech plays an central role in technical document analysis since it provides very useful information concerning the morphological role of a word and its morphosyntactic context: for example, if a token is a determiner, the next token is a noun or an adjective with very high confidence. 
Part of speech tags are used for many information extraction tools such as named entity taggers (see section \@ref(sotatoolsmodelner)) in order to identify named entities. In typical named entity task these are people and locations since tokens representing named entities follow common morphological patterns (e.g. they start with a capital letter). For the application to technical documents, technical entities (like the possible failures of a manufact) becomes more relevant. In this context a correct part-of-speech tagger becomes even more important since morphosyntactical rules can not be used. In addition part of speech tags can be used to mitigate problems related to polysemy since words often have different meaning with respect to their part of speech (e.g. “track”, “guide”). This information is extremely valuable in patent analysis, and some patent tailored part-of-speech tagger has been designed (see section \@ref(sotadocumentspatents)).
The literature on pos-tagger is huge, and goes behind the scope of the present thesis to make a complete review. In most of the application presented in this work, was employed the ILC postagger [@attardi2006experiments]. This postagger uses a supervised training algorithm: given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus. 

## Model {#sotatoolsmodel}

The goal of a model is to provide a simple low-dimensional summary of a dataset [@wickham2016r]. Ideally, the model will capture patterns generated by the phenomenon of interest (true signals), and ignore random variations (noise). A good model at the same time is able to capture the weak signals that cab be easily confounded with noise. These information is particularly valuable in the context of technical document analysis, where great technical insight could come weak quasi-invisible signals. [@james2013introduction]

Probabilistic models are widely used in text mining nowadays, and applications range from topic modeling, language modeling, document classification and clustering to information extraction. The present section contains a review of the most used methods used to model textual information.

### N-Grams {#sotatoolstransformngrams}

An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like “credit card”, “3d printing”, or ”printing machine”, and a 3-gram (or trigram) is a three-word sequence of words like “3d printing machine”. Statistical model can be used to extract the n-grams contained in a document. 
A first approach has the of predicting the next item in a sequence in the form of a (n − 1)–order Markov model[@lafferty2001document]. The algorithm begin with the task of computing P(w|h), the probability of a word w given a word h.The way to estimate this probability is using relative frequency counts. To do that the algorithms count the number of times h is followed by the w. With a large enough corpus it is possible to build valuable models, able to extract n-grams [@bellegarda2004statistical].
While this method of estimating probabilities directly from counts works for many natural language applications, in many cases a huge dimension of the corpus does make the model useful, and this is particularly true for technical documents [@brants2012large]. This is because technical language has a strong ratio of evolution; as new artifact are invented, new chunks are created all the time, and has no sense to continuously count every word co-occurrence to update our model[@gibson1994tools].
A more useful method for chunk extraction fro technical document uses part-of-speech-tagging and regular expression. Once a document is pos-taggerd each word is associated whit a particular part of speech: each sentence is represented as a sequence of part-of-spech. Once this representation is ready, it is possible to extract only certain sequences of part-of-speeches, the ones that whit an high level of confidence are n-grams.

### Document Classification {#sotatoolsmodeldocclass}

Classification is a general process that has the goal of taking an object, extract features, and assign to the observation one of a set of discrete classes. This process is largely used for documents [@borko1963automatic] and there exist many methods for document classification [@aggarwal2012survey]. 

Regardless of technological sector, most organizations today are facing the problem of overload of information. When it comes to classify huge amount of documents or to separate the useful documents from the irrelevant, document classification techniques can reduce the process cost and time.

The simplest method for classifying text is to use expert defined rules. These systems are called expert systems or knowledge engineering approach. Expert rule-based systems are programs that consist of rules in the IF form condition THEN action (if condition, then action). Given a series of facts, expert systems, thanks to the rules they are made of, manage to deduce new facts. 
The expert systems therefore differ from other similar programs, since, by referring to technologies developed according to artificial intelligence, they are always able to exhibit the logical steps that underlie their decisions: a purpose that, for example, is not feasible from the human mind or black box-systems. There are many type of documents for which expert based classifiers constitute a state-of-the-art system, or at least part of it. Anyway, rules can be useless in situations such as:
- data change over time
- the rules are too many and interrelated

Most systems of documents classification are instead done via supervised learning: a data set of input observations is available and each observation is associated with some correct output (training set). The goal of the algorithm is to build a static model able to learn how to map from a new observation (test set) to a correct output. The advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains.

In the supervised document classification process, is used a training set of N documents that have each been typically hand-labeled with a class: (d1, c1),....,(dN, cN). I say typically, because other less expensive methods could be designed, as it will be shown for the task of Named Entity Recognition (another supervised learning task, that classifies words instead of documents \@ref(sotatoolsmodelner)). The goal of the supervised document classification task is to learn a statistical model capable of assign a new document d to its correct class c ∈ C. There exist a class of these classifier, probabilistic classifiers, that additionally will tell us the probability of the observation being in the class. 

Many kinds of machine learning algorithms are used to build classifiers [@aggarwal2012survey], such as:

- _Decision Tree Classifiers_: Decision tree documents classifier are systems that has as output a classification tree [@sebastiani2002machine]. In this tree internal nodes are terms contained in the corpus under analysis, branches departing are labeled by the weight (see section \@ref(sotatoolstidy)) that the term has in the test document, and leafs are labeled by categories. There exists many methods to automatically learn trees from data. A tree can be build by splitting the data source into subsets based on an test feature. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions.

- _Rule Based Classifiers_: Rule-based classifiers are systems in which the patterns which are most likely to be related to the different classes are extracted from a set of test documents. The set of rules corresponds to the left hand side to a word pattern, and the right-hand side to a class label. These rules are used for the purposes of classification. In its most general form, the left hand side of the rule is a Boolean condition, which is expressed in Disjunctive Normal Form (DNF). However, in most cases, the condition on the left hand side is much simpler and represents a set of terms, all of which must be present in the document for the condition to be satisfied [@yang2004building].

- _Support Vector Machines (SVM) Classifiers_: SVM Classifiers attempt to partition the data
space with the use of linear or non-linear delineations between the different classes. The main principle of SVM algorithm is to determine separators in the feature space which can best separate the different classes [@joachims1998text; @manevitz2001one].

- _Baeysian Classifiers_: Bayesian classifiers build a probabilistic classifier based on modeling the underlying word features in different classes. The idea is then to classify documents using  the posterior probability of the documents belonging to the different classes on the basis of the word presence in the documents [@pop2006approach].

- _Neural Network Classifiers_: The basic unit in a neural network is a neuron. Each neuron receives a set of inputs, which are denoted by the vector _Xi_, which are the values of the feature vector for a certain instance. Each neuron is also associated with a set of weights, which are used in order to compute a function of its inputs. Neural Networks Classifier are able, thank to a process called learning phase, to adjust their weights in such a way that the function is able to effectively classify new instances. Neural networks are nowadays one of the best method for documents classification, and are used in a wide variety of applications [@manevitz2007one]. Great performances has also been reached by deep neural networks, which are neural networks whit a large number o neurons arranged in multiple layers [@lai2015recurrent; @kim2014convolutional]. 


### Sentiment Analysis {#sotatoolsmodelsentanal}

Sentiment analysis techniques are algorithms able to measure from text, people’s opinions and emotions toward events, topics, products and their attributes [@pang2008opinion].  For example, businesses (particularly marketeers) are interested in finding costumers opinions about their products and services.

Thanks to the growth of social media (forums, blogs and social networks), individuals and organizations are producing a huge quantity of their written opinion. This has make it possible to scholars to study this phenomena and to develop many different and effective sentiment analysis techniques [@liu2012survey]. In the past decade, a considerable amount of research has been done by scholars and there are also numerous commercial companies that provide opinion mining services. However, measuring sentiment in documents and distilling the information contained in them remains a challenging task because of the diversity of documents from which is possible to extract sentiment. 

The approaches to perform sentiment analysis are many. Among all, the most interesting for technical documents analysis are:


- _Dictionary Base Approaches_ : This approach has the aim of collecting words that are clues for positive or negative sentiment. In literature these words are called opinion words, opinion-bearing words or sentiment words. Examples of positive opinion words are: good, nice and amazing. Examples of negative opinion words are bad, poor, and terrible. Collectively, they are called the opinion lexicon. The most simple and widely used techniques to produce a dictionary of opinion words is based on bootstrapping using a small set of seed opinion words and an online dictionary such as WordNet [@miller1995wordnet].
The works that used this approach [@hu2004mining; @kim2004determining], adopts a process that consist in two phases: first collect set of opinion words manually, then grow this set by searching in the WordNet for their synonyms and antonyms. The process stops when no more new words are found. 
After that a manual inspection can be carried out to remove and/or correct errors. Scholars has developed several opinion lexicons [@ding2008holistic; @baccianella2010sentiwordnet; @hu2004mining; @philip1966general; @wiebe1999development] The lexicon based approach has the characteristic of being strongly context specific. This is an advantage when the goal is to design a method able to extract sentiment in a specific context [@chiarello2017product], but is a major  shortcoming if the goal is to design a general purpose method.

- _Supervised Learning Approaches_: Sentiment analysis can be formulated as a document classification problem with three classes: positive, negative and neutral[@mullen2004sentiment]. Training and test sets of documents are typically collected from product reviews, movies reviews or are created by scratch using manual annotation. Any learning algorithm can be applied to sentiment classification (naive Bayesian classification, and support vector machines [@prabowo2009sentiment]). The crucial phase for Supervised Learning sentiment analysis is the features presentation of the data. It was shown [@pang2002thumbs] that using uni-grams (a bag of individual words) as features in classification performed well with either naive Bayesian or SVM. Subsequent research used many more features and techniques in learning [@pang2008opinion]. 


### Text Clustering {#sotatoolsmodelnetanal}

The goal of clustering methods is to find groups of similar objects in the data thanks to the measure of a similarity function [@jain1988algorithms; @kaufman2009finding]. Clustering techniques are widely applied in the text domain, where the objects of the clustering can be documents (at different level of granularity) or terms. In the context of technical documents analysis Clustering is especially useful documents retrieval [@anick1997exploiting; @cutting1993constant].
Clustering problems are studied widely outside the text domain. Methods for clustering have been developed focusing on quantitative/non-textual data [@guha1998cure; @han2001spatial; @zhang1996birch]. 

In the context of text analysis, the problem of clustering finds applicability for a number of tasks, such as Document Organization and Browsing [@cutting2017scatter], Corpus Stigmatization using documents maps [@schutze1997projections] or word clusters [@baker1998distributional; @bekkerman2001feature]. It is useful also to use a Soft clustering approach, that associates each document with multiple clusters with a given probability. 

However, standard techniques for cluster analysis (k-means or hierarchical clustering) do not typically work well for clustering textual data in general or more specific technical documents. This is because of the unique characteristics of textual data which implies the design of specialized algorithms for the task. 

The distinguishing characteristics of the text representation are the following [@aggarwal2012survey]:

- There is a problem of course of dimensionality. The dimensionality of the bug-of-words representation is very large and the underlying data is sparse. In other words, the lexicon from which the documents are drawn may be of the order of millions, but a given document may contain only a few hundred words.This problem is even more serious for technical documents in which the lexicon is even more large. 

- The words are correlated with one another and thus the number of concepts (or principal components) in the data is much smaller than the feature space. This necessitates the careful design of algorithms which can account for word correlations in the clustering process.

- The number of words (or non-zero entries) in the different documents may vary widely. Therefore, it is important to normalize the document representations appropriately during the clustering task.

The problems of sparsity and high dimensionality necessitate the design of specific algorithms text processing. The topic has been heavily studied in the information retrieval literature where many techniques have been proposed [@ricardo2011modern]. 

### Named Entity Recognition {#sotatoolsmodelner}

Named Entity Recognition is the task of identifying entity names like people, organizations, places, temporal expressions or numerical expressions. An example of an annotated sentence for a NER extraction system tailored for user entity extraction from patents, is the following:

_Traditionally, <user> guitar players <user/> or <user> players <user/> of other stringed instruments may perform in any of a number of various positions, from seated, with the stringed instrument supported on the leg of the <user>performer<user/>, to standing or walking, with the stringed instrument suspended from a strap._

Methods and algorithms to deal with the entity extraction task are different, but the most effective are the ones based on supervised methods. Supervised methods tackle this task by extracting relevant statistics from an annotated corpus. These statistics are collected from the computation of features values, which are strong indicators for the identification of entities in the analyzed text. Features used in NLP for NER purposes are divided in two main categories:
- Linguistically motivated features, such as n-gram of words (sequences of n words), lemma and part of speech 
- External resources features as, for example, external lists of entities that are candidates to be classified in the extraction process.

The annotation methods of a training corpus can be of two different kinds: human based, which is time expensive, but usually effective in the classification phase; automatically based, which can lead to annotation errors due to language ambiguity. For instance driver can be classified both as a user (the operator of a motor vehicle), or not a user (a program that determines how a computer will communicate with a peripheral device). Different training algorithms, such as Hidden Markov Models [@eddy1996hidden], Conditional Random Fields (CRF) [@lafferty2001conditional] Support Vector Machines (SVM) [@hearst1998support], or Bidirectional Long Short Term Mermory-CRF Neural Networks [@lample2016neural; @misawa2017character] are used to build a statistical model based on features that are extracted from the analyzed documents in the training phase.

### Topic Modelling {#sotatoolsmodeltopicmodel}

Topic modeling is a form of dimension reduction that uses probabilistic models to find the co-occurrence patterns of terms that correspond to semantic topics in a collection of documents [@crain2012dimensionality]. To understand topic modelling it is useful to understand its differences with clustering \@ref(sotatoolsmodelnetanal) and the problem they both solves: the course of dimensionality. Both these techniques has in fact the goal of representing documents in such a way that they reveals their internal structure and interrelations. 

Clustering measures the similarity (or dissimilarity) between documents to place documents into groups. Representing each document by considering the belonging to a group, clustering induces a low-dimensional representation for documents. However, it is often difficult to characterize a cluster in terms of meaningful features because the clustering is independent of the document representation, given the computed similarity.
Topic modeling integrates soft clustering (assigning each element to a cluster with a given probability and not with a Boolean variable) with dimension reduction. Each document is associated with a number of latent topics: a topic can be seed as both document clusters and compact group of words identified from a corpus. Each document is assigned to the topics with different weights: this feature can be seen both as the degree of membership in the clusters, as well as the coordinates of the document in the reduced dimension space. The result is an understandable representation of documents that is useful for analyzing the themes in documents.
Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model [@blei2003latent]. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language. 

## Visualize {#sotatoolsvisualize}

Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests: the process of developing the outputs (the paper, the report, the dashboard, or other deliverable) is less frequently analyzed. This problem influences also text mining [@parker2017opinionated]. One of the most studied problems of output production is data visualisation.
Data visualisation involves the creation and study of the visual representation of data [@friendly2001milestones]. Data visualization uses statistical graphics, plots, information graphics and other tools to communicate information in a clear and efficient way. The main process of data visualisation is the visual encoding of numbers. Numerical data may be encoded in many ways, using a wide range of shapes: the main used are dots, lines, and bars [@wickham2016ggplot2]. The main goal of visualizations is to help users (students, researchers, companies and many others) analyze and reason about evidences hidden in data. It is possible thanks to the ability of visualisation to make complex data more accessible, understandable and usable. Tables are generally used where users will look up a specific measurement, while charts of various types are used to show patterns or relationships in the data for one or more variables.

Data visualisation has become in the last year a well established discipline thanks to the increased amounts of data created by Internet activity and an expanding number of sensors in the environment are referred to as "big data" or Internet of things [@ertug2018editors]. It is important to underline how the way this data is communicated present ethical and analytical challenges for data visualization practitioner [@bikakis2018big]. The field of data science and practitioners called data scientists help address this challenge [@loukides2011data].

Users of information displays are executing (consciously or not) particular analytical tasks such as making comparisons or determining causality [@tufte1990envisioning]. The design principle of the information graphic should thus support the analytical task, showing the comparison or causality [@tufte2006beautiful].

Graphical displays and principles for effective graphical display is defined as the ability to communicate complex statistical and quantitative ideas with clarity, precision and efficiency [@mulrow2002visual]. For this reason graphical displays should:

- show the data
- induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production or something else
- avoid distorting what the data has to say
- present many numbers in a small space
- make large data sets coherent
- encourage the eye to compare different pieces of data
- reveal the data at several levels of detail, from a broad overview to the fine structure
- serve a reasonably clear purpose: description, exploration, tabulation or decoration
- be closely integrated with the statistical and verbal descriptions of a data set


In literature are identified eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message [@few2012show]:

- Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.
- Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.
- Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.
- Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.
- Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis. A boxplot helps visualize key statistics about the distribution, such as median, quartiles, outliers, etc.
- Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.
- Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.
- Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.

Data visualisation practitioners has to consider whether some or all of the messages and graphic types above are applicable to their task and audience. The process of trial and error to identify meaningful relationships and messages in the data is part of exploratory data analysis.

### The Grammar of Graphics

Even if trial and error is and will remain an important part of data visualisation, some works has tried to give to data visualisation practitioners a well structured framework able to guide the process of data visualisation. Among the many frameworks, the most used is the _Grammar of Graphics_ [@wilkinson2006grammar] and its implementation [@wickham2008ggplot2]. The grammar of graphics is a coherent system for describing and building graphs. Like other kind of grammars, it describes to basic rules to use the element of data visualization with the goal of communicating some content. The main concept in the grammar of graphics is that graphs are made by multiple layers. Layers are responsible for creating the objects that we perceive on the plot. A layer is composed of four parts:

- _Data and aesthetic mapping_: Data are independent from the other components: we can construct a graphic that can be applied to multiple datasets. Along with the data, we need a specification of which variables are mapped to which aesthetics.
- _Statistical transformation_: A statistical transformation transforms the data, typically by summarizing them in some manner. 
- _Geometric object_: Geometric objects control the type of plot that is created. For example, using a point geom will create a scatterplot, whereas using a line geom will create a line plot. Geometric objects can be classified by their dimensionality.
- _Position adjustment_: Sometimes there exist the need to tweak the position of the geometric elements on the plot, when otherwise they would obscure each other. This is most common in bar plots, where we stack or dodge (place side-by-side) the bars to avoid overlaps.

Multiple layers together are used to create complex plots.

Together with the layer the designer can control the _scale_. A scale controls the mapping from data to aesthetic attributes, and one scale for each aesthetic property used in a layer is needed. Scales are common across layers to ensure a consistent mapping from data to aesthetics. 

After the decision of the scale, the designer has to decide the _coordinate system_ for the layer. A coordinate system maps the position of objects onto the plane of the plot. Position is often specified by two coordinates (x, y), but could be any number of coordinates. The Cartesian coordinate system is the most common coordinate system for two dimensions, whereas polar coordinates and various map projections are used less frequently. For higher dimensions, we have parallel coordinates (a projective geometry), mosaic plots (a hierarchical coordinate system), and linear projections onto the plane. Coordinate systems affect all position variables simultaneously and differ from scales in that they also change the appearance of the geometric objects. 

Finally, the last element of the grammar are _facets_. Faceting makes it easy to create small multiples of different subsets of an entire dataset. This is a powerful tool when investigating whether patterns are the same or different across conditions. The faceting specification describes which variables should be used to split up the data, and how they should be arranged.

## Comunicate {#sotatoolscomunicate}

The last task to perform in the process of knowledge extraction from technical documents is communications. If it means to communicate the results of an analysis inside a team or to the world, it does not matter how great an analysis is unless it is impossible to  explain it to others [@wickham2016r]. For the purposes of the present thesis, the focus is on the review of technical mechanics of communication especially in the R [@R-base] environment.
One of the most important innovation for the task of communication in data science is R Markdown [@R-rmarkdown].  R Markdown provides an unified authoring framework for data science, combining code, results, and comments. R Markdown documents are fully reproducible and support dozens of output formats, like PDFs, Word files, slideshows, and more.

R Markdown files are designed to be used in three ways:

- For communicating to decision makers, who want to focus on the conclusions, not the code behind the analysis.

- For collaborating with other data scientists (including future you!), who are interested in both your conclusions, and how you reached them ( i.e. the code).

- As an environment in which to do data science, as a modern day lab notebook where you can capture not only what you did, but also what you were thinking.

Together with reports (and usually contained in them) there are visualisation. Making graphics for communication follow all the rules and framework previously revised in section \@ref(sotatoolsvisualize), but when a graph has to be used to communicate to a wide audience there are some more rules to follow. The reason why this happen is that the audience likely do not share the background knowledge of the analysis and do not be deeply invested in the data. To help others quickly build up a good mental model of the data, the analyst need to invest considerable effort in making plots as self-explanatory as possible. For this reason has been developed many tools to help data scientist to make effective communication graphs[@wickham2016ggplot2; @shiny2017; @plotly2017; @ggprah2018; @ICWSM09154] . 

## Understand {#sotadocumentsunderstand}

The harder challenge in technology intelligence is not how to detect the large trends- they are visible anyway; it is, rather, how to detect weak signals, or information that initially appears with low frequency, in unrelated or unexpected regions of the technology landscape, and associated with large noise [@apreda2016functional]. These signals escape from traditional statistical detection techniques, exactly because it is difficult to distinguish them from pure statistical noise. It is hard to distinguish weak-signals from noise only using algorithms: understanding the problem to be solved and having domain expertise is essential. For this reason _understand_ is not only part of the process of analysis, but is maybe the most important task with respect to the present thesis. 

Unfortunately is difficult to design a process of analysis in such a way that the source of domain knowledge and they way they impact the analysis is clear. According to the survey carried out by Popper [@popper2008foresight], expert panels are the second most used methodology, after the survey of the literature (see section \@ref(sotadocumentspapers) for automatic approach to this task). It is non-sense to gain technical knowledge that is valuable to carry on a text mining analysis by relying exclusively on algorithms, without the support of human experts that are acknowledgeable about the knowledge field, technologies, their inner functioning, and the range of practical problems they are intended to solve. Due to the ill-structured nature of knowledge field expertise systematisation in data science, it requires an extensive problem-formulation phase, which cannot be done by others than human experts [@bracken2013making]. 

It is usually necessary to involve experts into panels or focus groups, in which they have face-to-face interaction (FTF). It is well known that experts have a preference for methods involving personal communication, such as FTF, but this method delivers the least reliable results, as compared to other methods such as prediction markets, nominal groups and, above all, the Delphi technique [@woudenberg1991evaluation; @graefe2011comparing].

Yet the use of experts creates a large number of problems. Even when experts are trained in data science, professionally engaged and personally committed to high standards of ethical integrity, they may introduce cognitive and motivational biases in the process[@kahneman2011thinking]. A crucial point in the literature is that not only lay people but also experts are subject to _cognitive biases_, and that cognitive abilities may moderate but not eliminate these effects [@stanovich2008relative]. Therefore the issue cannot be solved by selecting better experts [@taleb2007black].

In addition, experts often work in groups or panels, rather than in isolation or following the Delphi procedure, in which the revision of estimates is done separately and under the protection of anonymity [@meijering2016effect; @makkonen2016policy]. The group decision making process may add other peculiar distortions to the knowledge extraction exercise, as shown by the large literature on groupthink [@janis1972victims; @esser1998alive]. These biases are difficult to overcome and require a dedicated attention. As the Expert Group of the European Commission has recommended: “in order to minimize cognitive bias in analysis, selection and interpretation of information, explicit tools and methodologies should be used” [@tuomi2013next]. 

The issue of biases in data science technological analysis has been raised only recently in a few seminal papers, but it has not yet created a consistent literature, comparable to the large and robust literature in the field of forecasting and in many other areas of social sciences. 

### The problem of biases {#sotadocumentsunderstandbyas}

The literature in the heuristic and biases tradition is now in the order of several thousand articles. A deliberately crude search for “heuristic and biases” delivers more than 208,000 items in Google Scholar, a more sober 5,298 items in JSTOR, and 2,463 items in Scopus (accessed January 19, 2018). Even looking at specialized branches of the field we find references in the hundreds: in the EBSCO Business Source database the items are 453, in Pub Med they are 281. There is not a unique agreement on the terminology and the number. To make an example, Arnott [@arnott1998decision] mentions 37 heuristics and biases; a quite different list of 13 heuristics is discussed by Mousavi and Gigerenzer [@mousavi2014risk]. There is a minority opinion that has contested the overall Kahneman-Tversky [@kahneman2011thinking] paradigm, apparently without substituting it [@gigerenzer1991make]. There are several meta-analyses and systematic reviews that examine specific biases, which will be included in the reference list and in the discussion below. 

What follows is a selection of topics that might be important in data science, and in particular for knowledge extraction from technical documents.  In the following we start from the biases suggested by Goodwin [@goodwin2015history] and we add a few ones, which we believe are relevant for the field. The list is not necessarily exhaustive but is focused on those biases that are more important for anticipating technological trends. We first describe the biases in general terms.

#### Framing effect {-}

According to a famous experiment by Tversky and Kahneman [@tversky1981framing] people react differently to a decision problem according to the way it is “framed” in the task description, ignoring the content. In particular, framing a problem in terms of losses induces more risk-taking than framing in terms of gains. Risk or loss aversion occurs when people prefer a small reward with greater certainty than a large reward with less certainty. Framing a problem in terms of gains, or improvement, activates a different response [@tversky1981framing]. This effect has been confirmed in a large number of studies. The literature review by Levin, Schneider and Gaeth [@levin1998all] suggests to distinguish the effects of framing on risky choices, on the evaluation of attributes of alternatives, and of goal-related attributes. Experts are subject to framing effects not differently than novices [@loke1992effects] while in high level cognitive tasks the differences are remarkable [@larkin1980expert].

Several studies suggest that the framing can be mitigated or reduced to zero by warning on the effect [@cheng2010debiasing], using Decision Support Systems [@bhandari2008debiasing], creating heterogeneous groups [@yaniv2011group], exposing people to dissenting minority views [@nemeth1988modelling], or using deliberately devil’s advocate or dialectical inquiry methods [@lord1984considering]. However, other studies underline the importance of real conflict, or minority views, as opposed to artificially created situations (such as the devil’s advocate method, based on role playing) [@goodwin2010limits]. Exposure to dissent and disagreement reduces the confidence in one’s own views, but also reduces the level of conformity and improves the accuracy [@nemeth2001devil]. At the same time, it is critical that the dissent is centered around factual data, so that the discussion does not degenerate into issues of personalities [@keay2012authorising]. It is also critical that the judgment is repeated, so that people come to weight information according to its accuracy, considering discordant opinions and not discounting them [@harries2004combining].

According to Burgman [@burgman2015trusting], “experts groups should be as diverse as possible, and systems for engagement should encourage people to listen and integrate information and reasoning from as many sources as possible, and to explore competing explanations”. 

Different expert orientation than a framing in which gains and losses are compared and weighted.
A related problem is that the framing of the problem may hinder the ability to identify rare events, or low predictability phenomena. A combination between quantitative modelling and scenario analysis is suggested to avoid blindness [@makridakis2009forecasting].

#### Anchoring {-}

In producing numeric estimates of unknown quantities, people are heavily influenced by the information provided in the task description, even when it is not consistent or realistic [@gigerenzer2015calculated]. In other words, according to Kahneman and Tversky [@tversky1974judgment], people “anchor” their estimate to the number provided in the description and incorporate new information by “adjusting” the estimate above and below the anchor. In this way people do not make efficient use of information available and may be distorted in their judgment [@thorsteinson2008anchoring]. Furnham and Boo [@furnham2011literature] offers a literature review. The effect seems to be originated from the confirmatory testing of hypothesis that selectively activates anchor-consistent information in memory [@block1991overconfidence; @chapman1999anchoring]. 

In a classical experiment [@slovic2000violence] the authors asked thousands of forensic experts, trained formally in psychology and psychiatry, to estimate the probability that a given person with mental disorder would reiterate an offense after release from hospital. Two random assignments were used, one in which there was a suggestion to use a 1-40 scale to estimate probability, the other to use a 1-100 scale. Those using the 1-100 scale estimated that the probability to re-offend was approximately double than the one estimated by the other group. Another striking example was found by Ariely et al. [@ariely2003coherent]: the willingness to pay for consumption goods is influenced by an arbitrary anchor created by asking the subjects to remember the last two digits of their Social Security Number.

The magnitude of the bias is a function of personality and motivational orientation [@eroglu2010biases] and of the social origin of the anchor value [@meub2015anchoring], but not of cognitive abilities, so that experts are also subject to its effect [@mussweiler2000numeric; @oechssler2009cognitive; @bergman2010anchoring]. However, the level of knowledge of experts moderates the intensity of the anchoring effect [@wilson1996new; @smith2013knowledge].

The extent to which the anchoring bias can be mitigated is the object of large research. George, Duffy and Auja [@george2000countering] and Legerstee and Franses [@legerstee2014experts] report that after a training on the feedback from automated statistical program the accuracy of experts improves substantially, while Mussweiler et al. [@mussweiler2002malleability] and Furnham and Boo [@furnham2011literature] suggest that using compensating strategies, such as consider-the-opposite strategy, may reduce the anchoring bias (see also Mussweiler and Strack [@mussweiler2001semantics]). Providing experts with a feedback that uses metric knowledge (but not mapping knowledge) reduces the anchoring bias [@smith2015resisting]. Experts may receive incentives for the accuracy of their predictions, so that a loss function is defined and implemented [@lawrence2005judgmental]. This does not eliminate biases in the adjustment: Franses, Legerstee and Paap [@franses2011estimating] show that the loss functions of experts is typically asymmetric, so that underprediction is penalized more. 

#### Overconfidence {-}

A pervasive effect observed in many settings is the inability of individuals to evaluate correctly the degree of knowledge they have about issues or facts on which their judgment is requested [@shanteau1992competence]. Moore and Healey [@moore2008trouble] review a large literature, starting from a pioneering observation by Oskamp [@oskamp1965overconfidence]. The dominant effect is that people overestimate their own ability to perform accurate judgments (precision or accuracy), that is, they exhibit overconfidence in the form of overprecision, or unwarranted certainty [@koriat1980reasons]. They also overestimate their absolute ability or performance (overestimation) and their relative comparison within a group (overplacement) [@kruger1999unskilled].

According to Plous “no problem in judgment and decision making is more prevalent and more potentially catastrophic than overconfidence” [@plous1993psychology]. In technical terms, people are not good in defining appropriately the confidence interval they assign to their own estimates [@lichtenstein1977calibration]. If confidence is defined at 90%, it means that about 90% of the probability intervals provided by experts will include the observed (true) values. In a classical experiment, on the contrary, Russo and Schoemaker [@russo1992managing] found that only betwen 40% an 60% of the intervals suggested by managers included the true value of several economic variables, while respondents claimed they were 90% confident about the estimate. The overconfidence leads to illusion of competence [@castel2007illusions].

Very importantly for our discussion, experts produce on average better estimates than non experts, but they are subject to overconfidence in the same way: “Experts are good at reporting relatively narrow intervals centered on true values, but they are no better than novices at reporting well calibrated, high-confidence interval” [@mckenzie2008overconfidence].

It has also been suggested that the willingness of people to revise their judgment in the light of feedback depends on their power and egocentrism. People who self-perceive themselves as powerful tend to be more self-confident and do not adjust their own judgments following external feedback [@bonaccio2006advice].

#### Hindsight bias {-}

Another distortion that makes it difficult to learn from past events and external knowledge is the hindsight bias. The hindsight bias, initially discussed by Fischhoff [@fischhoff2003hindsight], is the unjustified increase in the perceived probability of an event due to the knowledge of how the event actually took place, or outcome knowledge [@hawkins1990hindsight]. People who know the outcome of an event find it difficult to report objectively the predictions they made before the knowledge of the outcome. 

For example, after the outcome of a disease, physicians may become unable to retrieve the reasons for their diagnosis, which at the time of execution was based on a prediction judgment [@arkes1980effect; @arkes1981hindsight; @arkes1984demonstration]. As another example, Herrmann and Choi [@herrmann2007prediction] examined the judgments of senior experts in foreign policy in Korea after the international political events were manifest and asked them to reconstruct their predictions, finding that “experts tend to forget what they used to believe and conclude in hindsight that they understand the causal forces driving developments” [@herrmann2007prediction]. Christensen-Szahansky and Willham [@christensen1991hindsight] offers a meta-analysis of 128 studies on the effect and Guilbault et al [@guilbault2004meta] update it.

This effect is also labeled “curse of knowledge”, or “curse of expertise”, that is, the tendency to be biased by one’s current knowledge in the evaluation of the knowledge of others (that is, the same person in an earlier perspective, or someone’s else) or the difficulty in discounting one’s privileged knowledge in judging what other people know or should know [@koriat2006illusions].

In other words, outcome knowledge compromises the ability of respondents to appreciate one’s own prior knowledge and another person’s naïve knowledge [@bernstein2012auditory] and therefore produces an exaggerated perception of the a priori predictability, or obviousness, of outcomes after they become known [@fessel2009hindsight; @lassiter2010videotaped], leading to the inability to learn from the past and the underestimation of the informativeness of facts [@mazursky1996knew]. 

Interestingly, expertise exacerbates the hindsight bias [@knoll2017effects]. Experts incorporate more easily new knowledge but tend to “go beyond the information given”. This means that they erroneously infer information that was not actually presented or consider old the newly presented information [@arkes1984demonstration]. They tend to take for granted elements that are in their knowledge, failing to examine whether the same level of knowledge is available to others. As shown by Tetlock and Lebow [@tetlock2001poking], experts that have a preference for explanatory closure and are theory-driven, tend to exaggerate the degree to which “they knew it all along”.

#### Desirability bias {-}

One of the fundamental assumptions of rational decision theory is that people are able to separate the judgment from the motivation, that is, the estimation of the frequency or probability of an event, from the desirability of the event. Beliefs and desires should be a priori independent. A large and consistent literature shows that this is not the case: people systematically inflate the judged probability of desirable events, and diminish that of undesirable events. This is an example of motivational bias, or the influence of motivational factors, such as preferences or desires, on cognitive activities. This distortion is, in principle, of great importance if experts are asked to judge the future developments of techologies which they consider, for any possible reason, desirable. This seems to be plausible in data science, in which experts come from industry, academia, or consulting.

Initially discovered in the ‘50s, the desirability bias has been the object of an extensive empirical research. It is considered a case of motivated reasoning [@kunda1990case], or interdependence between cognitive and motivational factors, and is also labelled wishful thinking. People distort their evaluation of probability in the direction of their preferred decision alternative [@dekay2009distortion]. It has been found not only in lay people [@lench2012automatic], but in experts in a variety of fields, from professional investors to price forecasters , from political election voters , to sport fans . Interestingly, people exhibit the desirability bias not only when they have preferences before the choice (disposition), but also when the preferences are acquired during the decision process [@russo1996distortion].

The explanations for this effect are multiple: people distort their judgment because the future prospects have an impact on their personal and affective future [@ayton1989psychological; @wright1996role; @lench2008automatic], or because there is high uncertainty and large amount of guessing on future prospects [@windschitl2010desirability], or because the prospect horizon is very far [@vosgerau2010prevalent]. A related distortion, called advocacy bias, refers to the conscious over-estimation of the prospects for technologies of products, when there is competition for resources. Here experts may inflate the probability of success in order to actively champion some technologies.

Of particular interest is the situation in which overoptimism, induced by the desirability bias, is associated to overconfidence, or the misplaced belief in the accuracy of one’s own judgments. Several studies show that overconfidence is frequently found in people with top managerial responsibilities in companies [@hribar2016ceo]. It is not known to what extent people involved in data science are also subject to this effect, but this is clearly an area for further research.


# Documents {#sotadocuments}

In this section contains a review of the main classes of technical documents analyzed in the present work. The documents are Patents, Papers, Wikipedia and Social Media.

## Patents {#sotadocumentspatents}

Nowadays patent data can be used for planning technological strategy [@azoulay2009impact;@ernst2003patent]. The focus on the technological usefulness of patent data is certainly a great advantage, but this huge research area could hide other useful application for patents. 
For example, in [@jin2015technology] the authors consider on one side patents as a source to collect information about technologies and products, and on the other side manuals, handbooks and market reports to collect market information.
Since patents are only technological documents many potential patent reader (e.g. designers, marketers) could be taken aside. Despite this problem, some researchers [@bonino2010review] affirm that there is an increasing variety of readers: not only technician and researchers but also marketers and designers who have grown an interest in patent analysis. Nevertheless, to our knowledge there are no researches that aim at facilitating information extraction for non-technological focused patent readers.

The bias that patent are only tech-oriented documents is due to two main reasons:

- Patents are produced to disclose and protect an invention, their content is mainly technical and legal. 
- 80% of technical information is not available elsewhere [@terragno1979], so patents are one of the most comprehensive resources for technical analysis.

Focusing on the second point, our hypothesis is that also a fraction of all the other kinds of information (e.g. marketing and sociological information ) is not contained elsewhere and it will appear in public documents (e.g. manual handbooks and market reports) in 6-18 months [@golzio2012]. 


The main difference between typical and non-tech patent readers is the information they focus on. _Patent attorneys_ and _Intellectual Property (IP) managers_ are interested in reading patents for legal reasons to orient the IP direction. Analyzing patents is the core of their work, so they are experts in finding the information they need. Furthermore, they can spend most of their work-time on the activity. On the other hand, usually _marketers and designers_ (taken as example of non-tech oriented readers) search users' behavioral changes and needs, market trends, designers' vision, R&D trends and competitors' strategies.  In addition, they rarely work with patents, so they do not know what and how to search. Lastly, they have short time to spend on the activity, and they waste most of this time understanding the legal and technical jargon used in patents.

Due to the large amount of information contained in patents and the growing interest to exploit this information, huge efforts have been devoted to the development of systems source to automatically extract different kind of information from such an enormous and valuable data.

Many techniques introduced in order to extract textual information from patents come from extensive research advances in the Natural Language Processing field (NLP). NLP is an area of research and artificial intelligence which aims at teaching computers to understand and manipulate natural language text in order to performs different tasks such as information extraction, machine translation and sentiment analysis.

The field of technology intelligence has become so large in recent years that several efforts to review and summarize the various approaches have been undertaken [@abbas2014literature]. There are several possible ways to classify the approaches. For example a used classification distinguish between Visualization techniques (Patent networks, Clustering) and Text mining (NLP-based, Property-function, Rule-based, Semantic analysis, Neural networks). Another possible classification is:


- _Metadata approaches_: methods that uses sources of information embedded in patents, such as claims structure or bibliographic information

- _Keyword approaches_: methods able to produce vector representations of the analyzed documents. Computed vectors can be used for many applications such as patent retrieval by keyword, or patent similarity matching. Even though this approach can be used for several tasks, it is not suitable to catch semantic relationships between entities in sentences. Furthermore, these methods use a blacklist to remove noisy words [@blanchard2007understanding] or use predefined lexicons [@chiarello2017product]. The right design of such list dramatically impacts the final output of the analysis [@Lee2009; @Lee2015; @Montecchi2013]

- _Natural Language Processing approaches_: methods based on grammatical and syntactical structures extracted by natural language processing tools, such as Part-of-Speech taggers and syntactical parsers. Unlike the keyword based approach, these methods are able to capture the relationships between the entities mentioned in sentences [@Park20131; @Park2011a; @Park20133].

Each approach allows to capture different types of information from patents and build a knowledge base which can be exploited by patent analysis tools. For this reason the right approach to be chosen to develop a patent analysis system depends on the task to be solved, on the information to be analyzed and on the computational resources involved to solve the task. Choosing a good trade-off between these factors is a strict requirement in particular when analyzing big patent sets.

### Metadata Approaches

Patent documents has the following metadata:

-	Patent office
-	Inventors
-	Affiliation of the inventors
- Filling date
- Publication date
-	Address of the affiliation of the inventor
-	Patent classifications
-	References
-	Assignee
-	Affiliation of the assignee
-	Address of the affiliation of the assignee

Furthermore they contain text content:

-	Title
-	Abstract
-	Keyword
- Summary page
- Drawing set
- Background of the invention
- Brief summary of the invention
- Brief description of the drawings
- Detailed description of the invention
- Claim set

This does not mean that metadata allow a unique identification. Disambiguation of metadata remains a challenge in most cases. Only recently documents have started to include a unique identifier, following the cooperation among the main producers and users. The unique identifiers refer to the publication (DOI, Digital Object Identifier) or the author (author ID).

However, metadata are written and stored in a standardized way, so that it is possible to categorize them. Issues of disambiguation refer mainly to the identity of individual entities (e.g. distinguish between two authors with exactly the same name and surname) but not of categories (e.g. distinguish between the name of an author and the name of a university).

Metadata approaches for patent analysis exploit three types of information: 

- bibliometric information
- patent structure information
- patent review process information.

In this approach are usually considered both patent and non-patent literature: for example patents with a high number of citations in papers usually indicate a strong correlation with the foundation of a technology. 

On of the main problems addressed using metadata approaches is measure of the technology life cycle stage. The information about at which stage of maturity a technology is, is an important aspect taken into account by who decides to invest. Since the life cycle of a product is clearly related by patent grants evolution  [@andersen1999hunt], this lead research to investigate on patent indices that can be considered as appropriate life cycle stage indicators. The main effort has been directed in the identification of the three different technology life cycle stages: introduction, growth and maturity [@haupt2007patent]. In this work, the author took into account that several studies have shown that a S-shape evolution of the number of patent applications or even a double-S-shape is typical. Consequently, the author defined the concept of patent activity index as an appropriate life cycle indicator only if its mean value differs significantly between the life cycle stages. The results of the work can be summarised as follow:

1. Backward literature citations increase significantly only at the transition from introduction to growth;
2. Backward patent citations increase significantly at both stage transitions;
3. The number of forward citations decreases significantly at the transition from introduction to growth;
4. The number of dependent claims is significantly higher at later technology life cycle stages than in earlier ones;
5. The number of priorities referred to in a patent application is significantly higher at later technology life cycle stages than in earlier ones;
6. Examination processes take longer in the phases of introduction and maturity than at the growth stage.

The main limit of these methods is the need of assumptions for what concerns the shapes of the stages curves. 

For this reason further works introduced an unsupervised method able to automatically detect the number of life cycle stages and the transition times of the technology of interest [@lee2016stochastic]. Here, seven time series patent indicator were taken into account: 

- patent activity which allows to model the evolution of a pattern. In particular increasing and decreasing patterns are considered a change for what concerns the research and development activity; 
- the number of technology developers in the analyzed temporal series. It has been shown that a great number of competitor enters in the initial stages of a technology’s life cycle, but this number lowers in the maturity stage
- the number of different patent application areas in the considered temporal series. This is an important indicator since it has been shown that the number of technology application areas are small in the first stages of their life cycles and in- creases in the later life cycle stages
- the number of backward citations. It has been shown that patents with an high number of backward citations have less relevance with respect to the patents with a lower number of citations
- the number of forward citations which expresses the technological value of a patent in the analyzed temporal period
- the duration of examination processes as the average time between the filing and granting dates. 
- the number of claims belonging to the patent. The more the number of claims reported by the patents, the higher the correlation with novelty and the financial value is.

Another widely adressed problem using metadata is citation analysis. Publications, patents, technical standards or clinical guidelines include a section in which other documents are cited. Citation analysis argues that including the reference to another document is the result of an intentional act, whose meaning may differ according to the type of document, but is nevertheless always worth of consideration [@moed2006citation]. The analysis of citations, initially developed in scientometrics and bibliometrics, has migrated to technology intelligence, following the initial concept of patent bibliometrics [@narin1994patent].
Patent (or firms, or inventors) that cite the same prior art are clustered together. Patent citation networks are then generated [@karki1997patent; @erdi2013prediction]. In fact, citations form a network structure, whose graph-theoretic properties can be interpreted in technology intelligence exercises [@lee2017knowledge]. Patent citation networks have properties of small world [@cowan2004network] and their degree follows a power law distribution [@chen2004tracing]. Patent citation analysis can be used to identify trajectory patterns and technology structure and paths, that is, knowledge flows among firms and among subsectors of an industry.

In standard citation analysis all citations are considered equal. This counting approach can be criticized because “it relies on the assumption that patents are equally significant” [@gerken2012new], which is in contrast with the empirical evidence on the large differences in patent value. This assumption is therefore removed in more advanced techniques in which the structure of citations from patents gets a qualification. 
It may be possible, however, that citations to other patents are strategically made by applicants, for example by citing their own patents or hiding other relevant citations). Backward citations may not be associated to technological novelty if they deliberately point only to the state of the art [@rost2011strength]. Citations introduced by examiners are also another potential source of bias [@alcacer2006patent].
Finally, it has been reported that the total number of citations increased over time, leading to “citation inflation” and the loss of value [@hall2001nber].

Derived from citation analysis, co-citation analysis argues that documents that are cited by the same documents should be considered part of the same cluster [@small2006tracking; @small1985clustering]. A variant of this technique, called author co-citation analysis [@white1981author], cluster documents that are cited by the same authors.
Co-citation analysis can also be used to create classification systems of patents [@lai2005using].

### Keywords Approaches

In the keyword based approach each patent is represented as a vector where each component measure the importance of a specific keyword, like explained in section \@ref(sotatoolstransformwi). The keywords to be taken into account depend on the patent set under analysis and on the goal of the task. Keywords can be extracted automatically using a text mining module, manually by experts or with hybrid methods where domain experts judge the relevance and the quality of the extracted keywords in order to limit the results to the most important keywords. Once keyword vectors are obtained, tasks such as patent similarity can be easily computed by using standard distance measures like cosine similarity. In addition, the keyword extraction allows to define more complex patent similarities measures [@moehrle2010measures] that can be exploited for the development of patent analysis tools [@lee2009approach; @lee2015novelty] such as mappers or patent search engines.  The main goal of these works is to developed systems for building keyword-based patent maps to be used for technology innovation activities. The system is composed of a text mining module, a patent mapping module and a patent vacancies identification module. Once a specific technology field is taken into account for analysis and a related patent set is extracted, the modules of the system are sequentially executed.

The text mining module automatically identifies relevant keywords in each patent of the considered patent set. Once all the keywords are extracted, only the ones with the highest relevance are selected for a further screening by domain experts. The final set of keywords resulting from the screening process is then considered for building the patent keyword vectors on the considered patent set. Specifically each component of the patent vector holds the frequency the corresponding keyword in the considered patent.
Once all the keyword vectors are computed, the patent mapping module is executed to generate the patent map. The mapping is calculated by executing the Principal Component Analysis (PCA) algorithm on all the vectors. The PCA method allows to map n-dimensional vectors on a rectangular planar surface in order to generate the patent map. Intuitively this method allows to find the most meaningful 2 dimensional projection that filters out the correlated components of a n-dimensional vectors. The result of applying this method over the patent keyword vectors is a meaningful patent mapping, in which each patent is mapped over a 2-dimensional surface.

Once the patent map is computed, a vacancy detection module is executed on the patent map. The vacancy detection module identifies sparse areas which can be considered good candidates for a research investigation. For each interesting vacancy, a list of related patents is obtained by selecting the ones which are located on the region boundaries. On the calculated list, a set of information for each patent is computed. This information is used to capture the importance of a patent in this patent list. Features considered strong indicators of the relevance of each patent are the number of citations [38] and the number of average citations by patents in the patent list. Finally, emerging and declining keywords are computed by taking into account the time series analysis of the considered keywords in the patent list. This allows to identify promising technology trends that can considered for further investigation.

The metadata and keyword approaches has a long tradition but suffers from several limitations. First, the initial query based on keywords is usually produced by human experts, either on an individual basis or organized as a panel. In practice, one of the best skills of research centers or consultancies specialised in technology intelligence has been, in the past, the ability to mobilize high level experts on an international basis in order to produce well crafted query lists. Unfortunately these lists, even if they are produced following elicitation procedures that respect state of the art recommendations in social sciences, are inevitably biased. Experts are extremely good in their field, but are not better that others if they have to evaluate matters that are outside their domain [@burgman2015trusting]. To the extent that emerging technologies are complex and fast evolving technologies, it is likely that experts have a narrow, or biased, perception of the dynamics. Experts tend to keep their existing R&D areas in mind, have personal and organizational inclinations, are subject to halo effects in favor of well known institutions or solutions, and may follow different criteria for selecting promising technologies [@kim2017novel].

It has been shown that little differences in the wording of queries, or on the time window, may end up in completely different sets of documents, leading the analysis in different directions [@bassecoulard2007mapping]. In addition to these authors, several studies in recent years have called the attention to the risk that initial differences in the delineation process generate non-comparable descriptions of technologies [@mogoutov2007data; @youtie2008nanotechnology; @ghazinoory2013application]. Following this line of concern,methodologies to update the keyword structure in an iterative, or evolutionary way has been proposed [@mogoutov2007data].
Second, it has been shown that when experts are asked to decide on relatedness measures (e.g. synonyms, hypernims or hyponims), they do not apply systematic rules [@tseng2007text; @noh2015keyword].

Third, the query list is static. Once defined, it is used to extract documents from large corpora, which are then processed. In dynamic technologies, it is likely that the pace of technological changes exceeds the speed of updating of the query lists. It is difficult to convene panels of experts repeatedly, also because of the large costs incurred in expert selection and management [@tseng2007text]. As an example, with the advent of nanotechnology it was felt the need to introduce a new patent sub-class. The sub-class B82B was introduced in year 2000, but it did not incorporate the previous patents, so that a comparison across time is not feasible. A new sub-class, B82Y, was introduced in 2011 [@kreuchauff2017patent].

### Natural Language Processing approaches

The impressive advancements of computational linguistics in the last two decades have made it possible to carry out analysis on the full content, not only the metadata, of large collections of texts. In text mining patterns are extracted from unstructured collection of documents, while in the metadata approach the patterns are extracted from structured documents or databases.

This has opened the way to the “full text based scientometrics” [@boyack2013improving] and has created the conditions for the convergence between the citationist approach illustrated above, and the lexical approach. 

Text mining techniques have then been applied to the corpus of patent texts, with a number of extremely powerful results [@tseng2007text; @joung2017monitoring; @kreuchauff2017patent; @ozcan2017patent; @yoon2012detecting]. 

In turn, text mining can be applied for the search of specific words (or combination thereof) or in the search for patterns that are not defined ex ante. In the former case the most used techniques are combination of keywords, correspondence analysis or category specific terms. These approaches expand the search over the full text of patents but preserve the limitations of keyword-based search. On the contrary, the search for patterns is the object of the most largely used technique, namely topic modelling. Pattern recognition in patent texts is “still in its infancy” [@madani2016evolution] but its applications are growing rapidly.

A useful review of NLP techniques in patent analysis [@madani2016evolution] identifies:

- the statistical approach that uses the Term Frequency-Inverted Document Frequency (TF-IDF) method to detect regularities
- the semantic approach uses SAO (Subject-Action-Object) and property-function structures in order to attribute meaning to the texts
- the corpus approach adopts ontology-based techniques. 

In turn, all these three information retrieval approaches can be extended by using pattern recognition techniques, that are keyword-, patent- or concept-based.

Text mining has several limitations : it cannot consider synonyms and the co-occurrence of keywords, while the inclusion of compound words and n-gram expressions requires large computational power. In addition, in the case of patents, claims are written in “arcane legalese” in order to hide critical elements and confound potential competitors.

The challenge here is how to maximize the substantive knowledge that can be generated by automatic processing of the full text. 

It has been remarked since long time that a promising direction for research into technology intelligence and data science lies in the combination of methods. This recommendation requires the combination between domain-knowledge and powerful computational approaches. It is this combination that holds the best promise to generate methods for the identification of emerging technologies, and more generally, for technology intelligence, that are able to identify high-granularity information producing weak signals, that is, to distinguish accurately the signal from the noise in turbulent and dynamic technological landscapes.


By exploiting the information obtained by these steps, several information extraction tasks can be solved by other NLP tools such as:

- Term extraction: the task of automatically extract relevant terms from a given corpus. Part of Speech tags are typically used by term extractors to narrow the terms search to a predefined term structure;

- Named entity recognition: the task of automatically identify and classify named entities in text such as persons, organizations and locations. Named entity recognizers usually use Part of Speech tags in order to disambiguate the morphosyntactic role of tokens in a phrase, improving the performance of the extraction;

- Relation extraction: the task of automatically build relations among entities in the analyzed text. In this context entities can be named entities or extracted terms. In addition, the syntactic role of the entities can be exploited to better categorize the relation type (e.g.: subject, object).

Technical domain language, as other linguistic domains, suffers from linguistic ambiguities. For instance the word “support” can have two totally different meanings when used as a noun or as a verb. By using part of speech taggers which are able to disambiguate the morphological role of each word in a sentence, more precise information extractions are possible and can be used in several applications (e.g. patent search engines). In addition part of speech taggers allow to perform textual lemmatization, which can further improve the performances of automatic patent analysis tools. Another key NLP tool used by several automatic patent analysis systems are syntactic parsers: by identifying the syntactic role of each word in document sentences, several patent analysis applications are possible. 

The most well established system for patent analysis using NLP techniques is the extraction of the Subject-Action-Object (SAO) structures, which is also a common use of syntactic parsers in automatic patent analysis tools.  Each SAO structure represents the subject (S), the action (A) and the object (O) in a patent sentence [@yoon2011identifying]. By automatically extracting SAO structures from patents, relationships between key technological components can be easily represented [@yoon2013identifying; @choi2011sao; @park2011identifying].

Another techniques that is growing in patent literature analysis is Named Entity Recognition (for further details see section \@ref(sotatoolsmodelner)). The Named Entity Recognition (NER) is the task of identifying entity names like people, organizations, places, temporal expressions or numerical expressions. 

Entity extraction tools used in patent analysis are largely based on NLP tools which can be applied to the analyzed text to extract entities that are important for the extraction purpose.
For example,  in the chemical field relevant entities are chemical components, proteins or product names. For the latter cases, adaptations of Named Entity Recognizers (NER) are commonly used for this task.

Methods and algorithms to deal with the entity extraction task are different, but the most effective are based on supervised methods. Supervised methods tackle this task by extracting relevant statistics from an annotated corpus. These statistics are collected from the computation of features values, which are strong indicators of the identification of entities in the analyzed text. Features used in NLP based entity recognition systems, are divided in two main categories:


- linguistically motivated features, such as n-grams of words, lemma and part of speech;
-  external resources features as, for example, external lists of entities that are candidates to be classified in the extraction process.


The annotation methods of a training corpus can be of two different kinds: (a) human based, which is time expensive, but usually effective in the classification phase; (b) automatically based, which can lead to annotation errors due to language ambiguity. As an example _crack_ can be classified both as a drawback (a fracture), or not drawback (short for crack cocaine).
Different training algorithms, such as Hidden Markov Models [@hmm], Neural Networks [@nnet], Conditional Random Fields [@crf] or Support Vector Machines [@svm], are used to build a statistical model based on the features that are extracted from the analyzed documents in the training phase. The same statistical model is later used in classification of unseen documents.

For what concerns the extraction of specific entities in patents, a major interest both in academia and commercial organizations has raised in the latest years, with the main aim of improving the accuracy of domain specific patent retrieval systems [@chemdner]. In [@lee-ner13] the authors proposed a machine learning based patent NER system that identifies key terms in patent documents and recognizes products, services and technology names in patent summaries and claims. In this work a study was conducted to identify the most relevant features for this classification task and by using lexical features like word uni-grams, word bi-grams and word trig-rams, their NER system reached an F1 score (the harmonic mean of precision and recall) of 65.4\%. The authors compared their NER tagging system resulting from the optimal feature selection method, with the human tagged corpus, showing that the kappa coefficient was 0.67. This result was better than the kappa coefficient between two human taggers (0.60).

Other entity extraction systems for the patent domain were proposed for the CHEMDNER  (chemical compounds and drug names recognition) community challenge [@chemdner]. The main aim of the organizers was to promote the development of novel, competitive and accessible chemical text mining systems. The best results were obtained by the _tmChem_ system [@leaman2015], achieving a 0.8739 f-measure score. The authors proposed an ensemble system composed of two Conditional Random Fields based classifiers, each one using hard feature engineering such as lemmatization, stemming, lexical and morphological features. In addition, external lists of entities were exploited to recognize whether a token matched the name of a chemical symbol or element, each one used to compute features to be added in the final statistical model.

The described entity tagging systems have very good performances mainly for two reasons: firstly, chemical entity names (such as molecular formulas) have very common orthographic patterns; secondly, these entities surrounding contexts are very similar. In more generic cases, these two features can not be exploited for entity extraction from patents, since different words  have totally different surrounding contexts. Another important key factor concerning the high performances of the described systems is that many external resources, such as lists of chemicals or product names, are available: this external knowledge can not be fully exploited in generic system.

## Papers {#sotadocumentspapers}

Researchers are starting to increasingly design and rely on automated literature analysis using text mining techniques [@nuzzo2010text;@azoulay2006publicationharvester]. This task is particularly valuable in the starting stage of any study, when gathering knowledge about a scientific question or a problem is crucial in formulation of  hypotheses and resaearch planning. Furthermore, even if research communities and publishing houses are putting increasing efforts in delivering scientific articles as structured texts [@poelmans2012text], nowadays a considerable part of digital scientific literature is available in layout-oriented data formats, like PDF, lacking any explicit structural or semantic information. As a consequence the bootstrap of textual analysis of scientific papers is often a time-consuming activity. 

To understand how text minign techniques can help to extract knowledge from scientific papers, we can analyze the phases in wich the literature review process (the proces trough which researchers extract knowledge from a corpus of documents) consists of. The task are:

- _locate_ the best-available empirical evidence to answer specific research questions
- _appraise_ which documents are valuable for the research
- _synthesize_ the information in such a way that the results of the analysis are clear and can be comunicated to the scientific community.

An ideal literature search would retrieve all relevant papers for inclusion (precision) and exclude all irrelevant papers (recall). However, researchers have proved that a lot of studies are not fully indexed, as well as a number of scientific papers are indexed incorrectly [@cooper1988structure; @okoli2010guide; @edinger2013large].

In general an automatic systems able to extract knowledge from scientific papers and a process of literature review are structured in a similar way. The steps for the first type of process (with a particular focus on paper classification, see section \@ref(sotatoolsmodeldocclass)) can be divied in  paper collection, text processing, data division, feature extraction, feature selection, data representation, classifier training, applying a classification model, and performance evaluation [@khorsheed2013comparative;@weiss2010text;@carlos2015text] (which are a subset of the generical tasks of natural language processing described in chapter \@ref(sotatools)).

Papers Collection has the aim of selecting all the papers that belong to a limited scientific domain, e.g., “industry 4.0” [@weiss2015fundamentals;@chiarello2018extracting]. This task is not trivial, and domain experts has to work with computer scientists with the goal of building a query able to filter for all the relevant documents.  Each sample text must then be labeled with one or more tags indicating a label to a certain class.

_Text preprocessing_ has been discussed in section \@ref(sotatoolstransform).  Actually preprocessing is in this context a way to improve text classification by removing useless information [@weiss2010text, @meyer2008text].

In _data division_ papers are divided into two parts, training data and testing data. Based on training data, the classification statistical model will be trained. The testing data will be used to asses the performance of the resulting classification model. There is no ideal ratio of training data to testing data, but usually a split of 25% for training and 75% for testing is used.

Then the process of _feature extraction_ is performed. Considering single words, the simplest of lexical features, as a representation feature in text classification has proven effective for a number of applications. The result of this step is a list of features and their corresponding frequency in the training data set [@weiss2010text].

The result of the previous step is a  collection of features. Clearly not all of these features are good for classification. The main reasons are:

- some classification algorithms are negatively affected when using many features due to what is called “curse of dimensionality”

- the over-fitting problem may occur when the classification algorithm is trained in all features 

- some other features are common in most of the classes and thus do not add information to the classifier .

To solve these problems, there is the need of a _feature selection_.  Many methods were proposed to select the most representative features for each class in the training data set [@zhao2012r; @weiss2010text; @weiss2015fundamentals].


The features obtained from the previous step gives the possibility to _rapresent the data_ in matrix format, so that these can be used by the classification algorithm. Usually, the data are in the document-term matrix format with n rows and m columns wherein the columns correspond to the selected feature, and the rows correspond to the texts in the training data. Weighting methods, such as term frequency inverse document frequency (TFIDF) and term frequency (TF) are used to compute the value of each cell in this matrix, which represents the weight of the feature in the text [@meyer2008text].

At this point of the process the _classifier can be trained_. The classification algorithm is trained using the document-term matrix. Many classical machine learning algorithms have been used in paper classification [@hotho2005brief;@aphinyanaphongs2003text;@zhao2012r]. The result is a classification model to be tested by means of the testing data. 

Finally,  _Classification model evaluation_ techniques are assessed to estimate future performance by measures such as accuracy, recall, precision, and f-measure, and to maximize empirical results [@weiss2015fundamentals].

Researchers has also used unsupervised techniques to extract knowledge from papers corpora. In particular the interest have been increased about the manner in which important research topics evolve over time within the corpus of scientific literature [@zhou2017topic]. One of the main method used in this context is topic modelling (see section \@ref(sotatoolsmodeltopicmodel)). Topic models identify and measure latent topics within a corpus of papers documents [@seaghdha2014unsupervised] in such a way that classes of topics has not to be decided apriori by researchers [@gargiulo2017deep; @yang2016discriminative; @paraschiv2015semantic]. 


## Wikipedia {#sotadocumentswiki}

The use of Wikipedia as source of knowledge started more than a decade ago and has been validated repeatedly in a variety of text mining applications (text annotation, categorization, indexing, clustering, searching [@milne2008learning]). In addition to the large and growing size in terms of number of articles, the structure of Wikipedia has a number of useful features that make it a good candidate for text mining applications.

First, Wikipedia pages are considered reliable in many knowledge fields, including the ones more interesting for technical analysis, e.g. engineering and computer science [@xu2015improving]. The pages are regularly and systematically updated by a large global community of contributors, which includes many scientific and industrial authorities in the field. The use of Wikipedia as knowledge source for computerized text mining tools is established in the literature [@ferragina2012fast]. In addition, it is powerful in disambiguation of terms, particularly through the use of redirect pages and disambiguation pages. This means that it can be used for detection and disambiguation of named entities [@bunescu2006using].

Second, the pages include links to other pages motivated by clear reasons on content. There are many links between Wikipedia pages, which are clues for semantic relations. This makes Wikipedia a densely connected structure, creating a classical small world effect: according to an often cited estimate, it takes on average 4.5 clicks to reach an article from any other article [@dolan2008six]. Unfortunately it is not possible to disentangle the kind of semantic relation, introducing a distinction between equivalent relations (synonymy), hierarchical relations (hyponymy/ hyperonimy) and associative relations, but this limitation is not relevant for our applications.

Third, it has the ability to evolve quickly [@lih2004wikipedia], particularly after the development of systems such as Wikify [@mihalcea2007wikify; @cheng2013relational]. Wikipedia has by design a dynamic structure, since it is constantly growing in the number of entries and changing in their content, when this is needed due to the advancements of knowledge [@ponzetto2007knowledge]. Furthermore the new terms that appear on Wikipedia thanks to comprehensive contributions by volunteers around the world, cannot be found in other linguistic corpora, such as WordNet Miller, 1995. Indeed, Wikipedia is the expression of a large international community, that is, of a “real community agreement” [@bizer2009dbpedia] or “community consensus” [@hepp2007harvesting], guaranteed by permanent collective monitoring of the quality and rigor of the entries [@bryant2005becoming].

Finally, Wikipedia is free-content and multilingual. This make it possible to freely collect the information contained in the web pages and allows the possibility for future developments of the dictionary in other languages. In our opinion multilanguage is an interesting feature for the dictionary, due to the fact that Industry 4.0 is a worldwide phenomena.

These properties make Wikipedia the ideal candidate for the goal of extracting technical knowledge from texts. Technical fields are in fact comprehensive, dynamically updated, and, as far as possible, expert-independent.

In particular, Wikipedia entries allow an endogenous measurement of semantic relatedness. This is an exceedingly important property for technical analysis: technologies can be mapped and can be defined as included in the perimeter of a knowledge field if and only it exhibits relatedness with other technologies already included in the perimeter. The inclusion of new technologies is therefore not dependent on experts’ subjective views, but is endogenously generated by the technological community that writes the articles for the encyclopedia and includes hyperlinks in the text of newly added pages.

## Social Media {#sotadocumentstwitter}

Nowadays, more than ever before, companies, governments, and researchers can gather and access data about people on a massive scale. Monitoring public opinion is increasingly made possible thanks to the rise of Social Media. These ones are computer-mediated technologies that facilitate the creation and sharing of information, ideas, career interests and other forms of expression with friends, families, co-workers, and other users, via virtual communities and networks. There are many different Social Media platforms, each of which targets a different aspect of what users want or need: e.g., LinkedIn targets professional networking activities, Facebook provides a mean of connecting friends and family, and Twitter provides a platform from which to quickly broadcast thoughts and ideas. These platforms are incredibly popular: as of February 2017, Facebook sees an average of 1,871 billion active users, with 76% of them that logging in every day [@tuten2017social].

Being so widely used, Social Media platforms generate huge amount of data. In 2013 users were posting an average of over 500 million tweets every day [@krikorian2013new]. Social Media are not constrained by national, cultural, and linguistic boundaries differently from traditional data sources and records of human activities, such as newspapers and broadcast media. Moreover, traditional media requires time to compile relevant information for publication, while Social Media data is generated in real-time as events take place. 

Virtually anyone who wishes to use all this information could collect and mine it. In 2009, the United States Geological Survey (USGS) began investigating the possibility of using SM data to detect earthquakes in real time [@ellis2015usgs]. Information about an earthquake spreads faster on Social Media than the earthquake itself can spread through the crust of the Earth [@konkel2013tweets]. Similarly, interesting work in Social Media forecasting also exists: EMBERS is a currently deployed system for monitoring civil unrest and forecasting events such as riots and protests [@ramakrishnan2014beating]. Using a combination of Social Media and publicly-available, non-SM, researchers are able to predict not just when and where a protest will take place, but also why a protest may occur.
These encouraging results have stocked the interest of researchers toward the possibilities opened by Social Media data, although some unanswered questions remain. If Social Media is useful for detecting real-time events, can it be used to make predictions about the future? What limitations does forecasting with Social Media data face? What methods lead researchers to positive results with Social Media data?
However, some researchers are pessimist about Social Media analysis. According to [@ruths2014social; @weller2015accepting], Social Media is noisy, and the data derived from it are of mixed quality: for every relevant post there may be millions that should be ignored. Learning with Social Media data sometimes requires robust statistical models.
Nevertheless, researchers continue to investigate how best to make use of Social Media data. First studies show positive findings.

Social Media users not only react to and talk about events in real time, but also talk about and react to events that will happen in the future. This fact fuels the interesting possibility that Social Media data might be useful for forecasting events: making predictions about future events. Not only have researchers begun to investigate this line of questioning, earlier review articles on Social Media forecasting showcase early positive examples of predictive success [@kalampokis2013understanding; @o2015twitter; @schoen2013power]. A lot of studies show that Social Media could be used to predict the future. At the same time, some works have been controversial [@schoen2013power]. It’s clear that this domain of research is in its infancy, methodologies are different, common best practices are difficult to determine, and true replication of studies is near-impossible due to data sharing concerns [@weller2015accepting].
The use of data from Social Media for modelling real-world events and behavior has seen a growing interest since his first appearance in academic world around 2008. This increasing popularity is proportional to the leaps ahead made in computational social science. In the past, many sociological theories were hard to prove for the difficulties encountered in gathering indispensable data. Today, Social Media can record so many sides of human relationships on the web from millions of people all around the world. On the other hand, Social Media data cannot always provide a complete picture of what researchers might hope to see. The use of Social Media varies depending on age, culture, social background, gender and ethnicity. However, positive findings and the interest in fundamental dynamics of Social Media platforms explain the exponential growth in popularity of this field of research.

Social Media data has a huge potential but understanding if its application can be useful is not a trivial task. Forecasting models (data- or theory-driven) are important in many fields but Social Media data challenges researchers to find new ways to apply them. In natural sciences, aggregating techniques of data coming from network of sensors are important, but Social Media data challenges researchers to find new ways to increase their forecasting power.
Researchers should first identify the methods through which Social Media challenges may be addressed to be able to make valid and reliable predictions. Among these difficulties, there are: noisy data, possible biases, a rapidly shifting Social Media landscape that prevents generalization and a need for domain-specific theory that brings all together.

Furthermore it is important to chose the best text source for Social Media analysis, among the many available. Previous studies found that researchers focused mainly on Twitter data [@ossola2018]. While Facebook is trying to compete, and Snapchat offers a unique perspective on the theme, Twitter remains the best indicator of the wider pulse of the world and what is happening in it. According to Hamad [@ahmed2017using], there are at least six reasons that explain the importance of Twitter for Social Media analysis:
1.	Twitter is a popular platform in terms of the media attention it receives, and it therefore attracts more research due to its cultural status;
2.	Twitter makes it easier to find and follow conversations (e.g., by both its search feature and by tweets appearing in Google search results);
3.	Twitter has hashtag norms which make it easier gathering, sorting, and expanding searches when collecting data;
4.	Twitter data is easy to retrieve as major incidents, news stories and events on Twitter are tending to be centered around a hashtag;
5.	The Twitter API is more open and accessible compared to other Social Media platforms, which makes it more favorable to developers creating tools to access data. This consequently increases the availability of tools to researchers;
6.	Many researchers themselves are using Twitter and because of their favorable personal experiences, they feel more comfortable with researching a familiar platform.
It is probable that a combination of the response from 1 to 6 led to more research on Twitter. However, this raises another distinct but closely related question: when research is focused so heavily on Twitter, what (if any) are the implications of this on methods? As for the methods that are currently used in analysing Twitter data e.g., sentiment analysis, time series analysis (examining peaks in tweets), network analysis etc., can these be applied to other platforms or are different tools, methods and techniques required?

Below has to be considered whether these methods would work for other Social Media platforms [@ahmed2017using]:

1.	Sentiment analysis works well with Twitter data, as tweets are consistent in length  would sentiment analysis work well with, for example Facebook data where posts may be longer?
2.	Time series analysis is normally used when examining tweets overtime to see when a peak of tweets may occur, would examining time stamps in Facebook posts, or Instagram posts, for example, produce the same results? Or is this only a viable method because of the real-time nature of Twitter data?
3.	Network analysis is used to visualize the connections between people and to better understand the structure of the conversation. Would this work as well on other platforms whereby users may not be connected to each other i.e., public Facebook pages?
4.	Machine learning methods may work well with Twitter data due to the length of tweets but would these work for longer posts and for platforms that are not text based, i.e., Instagram?

Maybe at least some of these methods can be applied to other platforms, however they may not be the best methods, and may require the formulation of new methods and tools. In conclusion, Twitter is the best for Social Media analysis for now. Despite its smaller user base compared with Facebook, its responsiveness and openness to researchers’ tool make possible gathering useful data.

Since the usage of social media has a wide impact on a great number of disciplines, here is exposed the main literature in the most technical related fields that are strongly related to social media analysis: economics and marketing.

### Economics

This domain has raised the great interest of researchers. The first studies focused especially on market fluctuation and on aggregated measure, such as Dow Jones Industrial Average (DJIA). Most recent researches have gone further predicting single stock price and yield.

Great interest in Social Media analysis for economics has been on Stock market analysis. 
Stock price forecasting is an important and thriving topic in financial engineering and is considered a very difficult task, even outside Social Media. Many articles in this context present models based on sentiment analysis to make forecasts [@xu2014collective; @kordonis2016stock; @cakra2015stock; @cakra2015stock; @wang2016using; @shen2016using; @brown2012will; @rao2012analyzing], although some researchers realised more detailed models: Crone et al. [@crone2014predicting] implemented neural networks and incorporated non-SM sources, and Shen et al. [@shen2016using] developed a model that studies the connection between consumers’ emotion and commodity prices.

The simplest task for stock market forecasting is predicting whether the following day will see rise or fall in stock prices. Comparison between researches is complicated by the fact that stock market volatility, and so the difficulty of prediction, may vary over time periods. High accuracy on this task was reported by Bollen et al. [@bollen2011twitter], using sentiment analysis to achieve an accuracy of 87,6%. They investigated whether measurements of collective mood states derived from large-scale Twitter feeds are correlated to the value of the Dow Jones Industrial Average (DJIA) over time. They analysed the text content of daily Twitter feeds by two mood tracking tools, namely OpinionFinder, that measures positive vs negative mood, and Google-Profile of Mood States (GPOMS) that measures mood in terms of 6 dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). They find that measures of “calm” on Twitter along with DJIA numbers from the previous three days provide the best up/down predictions. Further adding the emotion “happy” reduces rise/fall accuracy to 80% but does reduce error in terms of forecasting absolute DJIA values. Importantly, they find that positive/negative sentiment analysis through the popular OpinionFinder’s tool leads to no improvement over just using previous DJIA values.
In conclusion, researchers obtained good results forecasting up/down movements in the stock market.

Furthermore the topic of Sales and revenues is of great interest for people working on economics. For example, boosting movie ticket sales is an important task for producers and publishers, and this has been studied specifically on Social Media platforms like Twitter. Asur et al. [@asur2010predicting] showed how social media content can be used to predict movie success. In particular, they used the chatter from Twitter.com to forecast box-office revenues for movies. Specifically, using the rate of chatter from almost 3 million tweets, they constructed a linear regression model for predicting box-office revenues of movies in advance of their release. Then, they showed that the results outperformed in accuracy those of the Hollywood Stock Exchange and that there is a strong correlation between the amount of attention a given topic has (in this case a forthcoming movie) and its ranking in the future. They also analysed the sentiments present in tweets and demonstrated their efficacy at improving predictions after a movie has released.

Cheng et al. [@cheng2013predicting] obtained mixed results developing a model for predicting TV audience rating. They accumulated the broadcasted TV programs’ word-of-mouse on Facebook and apply the Back-propagation Network to predict the latest program audience rating. They also presented the audience rating trend analysis on demo system which is used to describe the relation between predictive audience rating and Nielsen TV rating.

Kim et al. [@kim2014nowplaying] investigated the relationship between music listening behavior in Twitter and the Billboard rankings. They found that the play-counts extracted from tweets have strong relationships with the Billboard rank, whereas, interestingly, the artist popularity extracted from tweets has a weak correlation with future chart rankings. In addition, the number of weeks on chart information alone was insufficient to predict rank alone.

With the features extracted from tweets, They built three regression models to predict the ranking. Among the proposed models, SVR (Support Vector Machine) showed the highest squared correlation coefficient (0.75). Although the combined model with the number of weeks on chart performed the best in rank prediction, the music listening behavior available in Twitter can generate an outstanding predictive model. They also built a hit prediction classifier with the features acquired in tweets and the number of weeks on chart. They classified the hit and non-hit songs in the Billboard Hot 100 and obtained a value of 83.9% accuracy, 83% precision, and 85.3% recall for classifying a hit song over the whole data set. The proposed feature showed a high performance both for rank prediction and hit classification. The previous week’s twitter features and the number of weeks on chart are effective for predicting the Billboard rank of a song.
Ahn et al. [@ahn2014sales] focused on periodic forecasting problems of product sales based on social media analysis and time-series analysis. In particular, they presented a predictive model of monthly automobile sales using sentiment and topical keyword frequencies related to the target brand over time on social media. Their predictive model illustrates how different time scale-based predictors derived from sentiment and topical keyword frequencies can improve the prediction of the future sales.

Tuarob et al. [@tuarob2013fad] proposed a Knowledge Discovery in Databases (KDD) model for predicting product market adoption and longevity using large scale, social media data. In particular, the authors analysed the sentiment in tweets and use the results to predict product sales. The authors presented a mathematical model that can quantify the correlations between social media sentiment and product market adoption in an effort to compute the ability to stay in the market of individual products. The proposed technique involves computing the Subjectivity, Polarity, and Favorability of the product. Finally, the authors utilised Information Retrieval techniques to mine users’ opinions about strong, weak, and controversial features of a given product model. The authors evaluated their approaches using the real-world smartphone data, which are obtained from www.statista.com and www.gsmarena.com.

The findings show that tweets can be used to predict product sales for up to at least 3 months in advance for well-known products such as Apple iPhone 4, Samsung Galaxy S 4G, and Samsung Galaxy S II, thus the predictive ability varies across products. 

### Marketing

Scholars had a great focus in the last years on using Social Media Information for marketing. Chen et al. [@chen2015making] conducted a survey study and a field study to explore the feasibility of using predicted personality traits derived from social media text for the purpose of ad targeting. In the survey study, they measured people's personalities and their responses to an advertisement tweet. They found that people with high openness and low neuroticism responded more favorably to a targeted advertisement, thus demonstrating the effects of the personality traits themselves. In the field study, they sent the advertisement tweets to real-world Twitter users, and found the same effects on users' responses using personality traits derived from users' tweet text. They demonstrate that aiming advertisements at users with particular personality traits improves click and follow rates by 66% and 87% respectively, representing a large increase in value for companies. These results suggest that the derived personality traits had the same effects as the personality traits measured by traditional personality questionnaires and can indeed improve ad targeting in real-world settings. Li et al. [@li2016project] present a solution to the problem of predicting project success in a crowd-funding environment combined with innovative introduction of survival analysis based approaches. They used comprehensive data of 18 thousand Kick-starter (a popular crowd-funding platform) projects and 116 thousand corresponding tweets collected from Twitter. While the day of success is considered to be the time to reach an event, the failed projects are considered to be censored since the day of success is not known.

They performed rigorous analysis of the Kick-starter crowd-funding domain to reveal unique insights about factors that impact the success of projects. Their experimental results show that incorporation of failed projects (censored information) can significantly help in building a robust prediction model. Additionally, they also created several Twitter-based features to study the impact of social network on the crowd-funding domain. Their study shows that these social network-based features can help in improving the prediction performance. They found that the temporal features obtained at the beginning stage (first 3 days) of each project will significantly improve the prediction performance. Even when just using Social Media information from the first three days of the project, they achieve an AUC of 0.90, reflecting very high classification performance.

Also the software engineering field, has helped marketing specialits using social media and other data sources, as, e.g., reviews, to improve products design specificaly in the requirements design phase [@groen2017crowd].
