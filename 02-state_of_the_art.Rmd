# State of the Art{#sota}

The analysis of technical documents require the design of processes that rely both on programming and Natural Language Processing techniques and on the undestanding and knowldege of field experts. While the first techniques are codified and explicit, the second are sometimes implicit and always harder to systematize. In this section i treat these two groups of techniques in the same way to give to the reader a sistematic litterature review on these topics. For this reason the sections of this chapter has the sequent structure: 

- At a first level we have two sections \@ref(sotatools) and \@ref(sotadocuments), reviewing respectivelly the processes of _programming and Natural Language Processing_ and of _undestanding and knowldege of field  experts application_;
- Section \@ref(sotatools) has a subsection for each of the _phases_ showed in figure \@ref(fig:mainworkflow). These subsections goes from \@ref(sotatoolsprogram) to \@ref(sotatoolscomunicate);
- Each subsection from \@ref(sotatoolsprogram) to \@ref(sotatoolscomunicate) contains the relative Natural Language Processing _task_ that are relevant for the analysis of technical documents, for example Document Retrieval \@ref(sotatoolsimportretrieval), Part-Of-Speech-Tagging; \@ref(sotatoolstransformpos) or Named Entity Recognition \@ref(sotatoolsmodelner).
- Each task subsection describes the relevant _techniques_ to perform that task. I use the word techniques to include mainly algorythms and procedures but also more generic methods or frameworks;
- Since the second section \@ref(sotadocuments) describes less systematics phases, task and techniques this section opens with a first subsection \@ref(sotadocumentsunderstand) that focuses on the studies of the problems of using expert knowledge in an analytical process and which are the techniques to convert this knowledge in a format that is usable in a Natural Language Processing workflow. 
- Finally, always section \@ref(sotadocuments) has a subsection for each of the technical _documents_ I analyzed (aggiungi gancio con introduzione).  These subsections goes from \@ref(sotadocumentspatents) to \@ref(sotadocumentsjobs).

## Phases, Tasks, and Techniques {#sotatools}

In this section I make a review of the most important techniques for Natural Language Processing in the context of technical documents analysis. The techniques (mainly algorythms) are grouped in phases (Import, Tidy, Transform, Model, Visualize, Communicate) showed in figure \@ref(fig:mainworkflow) and each phases is dived in the NLP tasks that are the most important for the analysis of technical documents. 
The algorythms i reviewed in this section are summmarised in table tot, where the reader can see the relationship between tasks and techniques. 

### Program {#sotatoolsprogram}

- __Articoli Emily__

### Import {#sotatoolsimport}

- I tipi di codifica di testo
- Pachetti per import 


#### Document Retrieval {#sotatoolsimportretrieval}

- Letteratura query

### Tidy {#sotatoolstidy}

- Hadley

weight, tfids

DTM

problems such as sparsity

### Transform {#sotatoolstransform}

Transforming in the context of Natural Language Processing  is what in computational linguistic is called text normalization. Normalizing text means converting it to a more convenient, standard form. Most of the task of technical document analysis in fact relies on first separating out or tokenizing sentences and words, strip suffixes from the end of the word, determining the root of a word or transform the text using regular expressions. 

### Sentence Splitting

The analysis of technical documents require as first process, that the input text is segmented in sentences. Since documents do not encode this information in a non ambiguous manner (using dots) due to common abbreviations (e.g.: “Mr., Dr.”), a sentence splitting process that does not rely only on a trivial _dot based_ rule is required. This issue in the technical documents domain is even more problematic due to the presence of formulas, numbers, chemical entity names and bibliographic references. Furthermore, since sentece splitting is one of the first processes of an NLP pipeline, errors in this early stage are propagated in the following steps causing a strong decrease for what concerns their accuracy. One of the most advanced techniques are machine learning techniques: given a training corpus of properly segmented sentences and a learning algorithm, a statistical model is built. By reusing the statistical model, the sentence splitter is able to split sentences on texts not used in the training phase. ItalianNLP lab systems uses this approach [@dell2009ensemble;@attardi2009reverse;@attardi2009accurate]. For this reason this algorythm is used for the most of the application presented in this Thesis.

### Tokenization

Since documents are unstructured information, these has to be divided into linguistic units. The definition of linguistic units is non-trivial, and more advanced techniques can be used (such as n-gram extraction) but most of the times these are words, punctuation and numbers. English words are often separated from each other
by whitespace, but whitespace is not always sufficient. Solving this problems and splitting words in well-defined tokensis defined as tokenization. In most of the application described in the present Thesis, the tokenizer developed by the ItalianNLP lab was integrated [@dell2009ensemble;@attardi2009reverse;@attardi2009accurate]. This tokenizer is regular expression based: each token must match one of the regular expression defined in a configuration file. Among the others, rules are defined to tokenize words, acronyms, numbers, dates and equations.

#### Stemming {#sotatoolstransformstemming}

Stemming is a simpler but cruder methodology for chopping off of affixes. The goal of stemming is reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem of a word and its morphological root do not need to be identical; it is sufficient that related words map to the same stem, even if this stem is not a valid root. One of the most widely used stemming is the simple and efficient Porter algorithm [@porter1980algorithm].

#### Lemmatisation {#sotatoolstransformlemmatisation}

Lemmatization is the task of determining the root of a words. The output allow to find that two words have the same root, despite their surface differences. For example, the verbs _am_, _are_, and _is_ have the shared lemma _be_; the nouns _cat_ and _cats_ both have the lemma _cat_. Representing a word by its lemma is important for many natural language processing tasks. Lemmatisation in fact diminish the problem of sparsity of document-word matrix. Futhermore lemmatisaion is important for document retrieval \@ref(sotatoolsimportretrieval) web search, since we want to find documents mentioning motors if we search for motor. The most recent methods for lemmatization involve complete morphological parsing of the word [@hankamer1989morphological]. 

#### Part-of-Speech Tagging {#sotatoolstransformpos}

The part of speech plays an central role in technical document analysis since it provides very useful information concerning the morphological role of a word and its morphosyntactic context: for example, if a token is a determiner, the next token is a noun or an adjective with very high confidence. 
Part of speech tags are used for many information extraction tools such as named entity taggers (see section \@ref(sotatoolsmodelner)) in order to identify named entities. In typical named entity task these are people and locations since tokens representing named entities follow common morphological patterns (e.g. they start with a capital letter). For the application to technical documents, technical entities (like the possibile failures of a manufact) becomes more relevant. In this context a correct part-of-speech tagger becomes even more important since we can not rely on morphosyntactical rules. In addition part of speech tags can be used to mitigate problems related to polysemy since words often have different meaning with respect to their part of speech (e.g. “track”, “guide”). This information is extremelly valuable in patent analysis, and some patent tailored part-of-speech tagger has been designed (see section \@ref(sotadocumentspatents)).
The litterature on pos-tagger is huge, and goes behoind the scope of the present thesis to make a complete review. In most of the application presentend in this work, was employed the ILC postagger [@attardi2006experiments]. This postagger uses a supervised training algorithm: given a set of features and a training corpus, the classifier creates a statistical model using the feature statistics extracted from the training corpus. 


#### Regular Expressions {#sotatoolstransformregex}

Regular expression (regex) is a language for specifying text search strings, an algebraic notation for characterizing a set of strings. This language whidelly used in modern word processor and text processing tools.. They are particularly useful for searching in texts, when we have a pattern to search for. 


A pattern could be at  A regular expression search function will search through the
corpus, returning all texts that match the pattern. The corpus can be a single document
or a collection. For example, the Unix command-line tool grep takes a regular
expression and returns every line of the input document that matches the expression.
A search can be designed to return every match on a line, if there are more than
one, or just the first match. In the following examples we generally underline the
exact part of the pattern that matches the regular expression and show only the first
match. We’ll show regular expressions delimited by slashes but note that slashes are
not part of the regular expressions.

### Model {#sotatoolsmodel}

Classi di modelli. Pedro Domingos

[@james2013introduction]

#### N-Grams {#sotatoolstransformngrams}

An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like “credit card”, “3d printing”, or ”printing machine”, and a 3-gram (or trigram) is a three-word sequence of words like “3d printing machine”. Statistical model can be used to extract the n-grams contained in a document. 
A first approach has the of predicting the next item in a sequence in the form of a (n − 1)–order Markov model[@lafferty2001document]. The algorythm begin with the task of computing P(w|h), the probability of a word w given a word h.The way to extimate this probability is using relative frequency counts. To do that the algorythms count the number of times h is followed by the w. With a large enough corpus it is possibile to build valuable models, able to extract n-grams [@bellegarda2004statistical].
While this method of estimating probabilities directly from counts works for many natural language applications, in many cases a huge dimension of the corpus does make the model useful, and this is particularly true for technical documents [@brants2012large]. This is because technical language has a strong ratio of evolution; as new artifcat are invented, new chunks are created all the time, and has no sense to continuolly count every word co-occurrence to update our model[@gibson1994tools].
A more usefull method for chunk extraction fro technical document uses part-of-speech-tagging and regular expression. Once a document is pos-taggerd each word is associated whit a particular part of speech: each sentence is rapresented as a sequence of part-of-spech. Once we have this rappresentation, it is possible to etract only certain sequences of part-of-speeches, the ones that whit an high level of confidence are n-grams.

[TROVA LAVORI SU QUESTO ARGOMENTO]

#### Document Classification {#sotatoolsmodeldocclass}

Classification is a general process that has the goal of taking an object, extract features, and assign to the observation one of a set of discrete classes. This process is largerly used for documents [@borko1963automatic] and there exist many methods for document classification [@aggarwal2012survey]. 

Regardless of technological sector, most organizations today are facing the problem of overload of information. When it comes to classify huge ammount of documents or to separate the useful documents from the irrelevant, document classification techniques can reduce the proecess cost and time.

The simplest method for classifying text is to use expert defined rules. These systems are called expert systems or knowledge engineering approach. Expert rule-based systems are programs that consist of rules in the IF form condition THEN action (if condition, then action). Given a series of facts, expert systems, thanks to the rules they are made of, manage to deduce new facts. 
The expert systems therefore differ from other similar programs, since, by referring to technologies developed according to artificial intelligence, they are always able to exhibit the logical steps that underlie their decisions: a purpose that, for example, is not feasible from the human mind or black box-systems. There are many type of documents for which expert based classifiers constitute a stateof-the-art system, or at least part of it. Anyway, rules can be useless in situations such as:
- data change over time
- the rules are too many and interrelated

Most systems of documents classification are instead done via supervised learning: we have a data set of input observations, each associated with some correct output  (training set). The goal of the algorithm is to build a statical model able to learn how to map from a new observation (test set) to a correct output. The advantages of this approach over the knowledge engineering approach are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains.

In the supervised document classification process, we have a training set of N documents that have each been tipically hand-labeled with a class: (d1, c1),....,(dN, cN). I say tipically, because other less expensive methods could be designed, as we will show for the task of Named Entity Recongition (another supervised learning task, that classifies words intstead of documents \@ref(sotatoolsmodelner)). The goal of the supervised document classification task is to learn a statistical model capable of assign a new document d to its correct class c ∈ C. There exist a class of these classifier, probabilistic classifiers, that additionally will tell us the probability of the observation being in the class. 

Many kinds of machine learning algorithms are used to build classifiers [@sebastiani2002machine], such as:

- _Decision Tree Classifiers_: A decision tree documents classifier are systems that has as output a classification tree. In this tree internal nodes are terms contained in the corpus under analysis, branches departing are labeled by the weight (see section \@ref(sotatoolstidy)) that the term has in the test document, and leafs are labeled by categories. There exists many methods to automatically learn trees from data. A tree can be build by splitting the data source into subsets based on an test feature. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions.


#### Sentiment Analsysis {#sotatoolsmodelsentanal}

#### Network Analysis {#sotatoolsmodelnetanal}

#### Named Entity Recognition {#sotatoolsmodelner}

#### Vector Semantics {#sotatoolsmodelvec}

#### Topic Modelling {#sotatoolsmodeltopicmodel}


### Visualize {#sotatoolsvisualize}

### Comunicate {#sotatoolscomunicate}





## Documents {#sotadocuments}

### Understand {#sotadocumentsunderstand}


Expertise (collins)

Sheela Jasanow

Taleb?

#### The problem of byases {#sotadocumentsunderstandbyas}

#### The Importance of Lexicons for Technical Documents Analysis  {#sotadocumentsunderstandlexicons}

#### Expert Systems

### Patents {#sotadocumentspatents}

### Papers {#sotadocumentspapers}

- Parte Barilari.Keyword base, defini i confini di area tecnologica. Hot-topics su paper (guaiè)
- Biblio 

### Projects {#sotadocumentsprojects}

### Wikipedia {#sotadocumentswiki}

### Twitter {#sotadocumentstwitter}

### Job Profiles {#sotadocumentsjobs}
