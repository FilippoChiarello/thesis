---
output:
  pdf_document: default
  word_document: default
  html_document: default
---
# (PART) Introduction {-}

The information field human beings are living in has changed dramatically over the last years, effecting economy, technology, culture and society. And yet, its stronger mark has been over business systems [@Arun2006Firm; @jin2015significance; @degryse2016digitalisation; @john2014big; @o2016weapons]: companies have to master methods and tools to solve the information overload issue in order to strive and stay competitive [@levitin2014organized; @feng2015competing].

To understand what companies go through in terms of challenges, we have to wear their shoes. Let’s say that the amount of information (such as tactile, visual and sounds, rumors and conversations, the content of business papers) gathered in the location we live in starts to rise quickly. To survive such an environment, we would need new and more powerful senses; we would probably need outside systems to help us processing information. Considering the mole of digital information produced in the past 10 years, companies have found themselves in our hypothetical scenario: their digital universe is chaotic and constantly expanding. In fact, this very universe will grow from 2005 to 2020, from 130 exabytes to 40,000 exabytes (40 zettabytes) [@gantz2012digital], as shown in figure \@ref(fig:biginfo).

Moreover, new digital technologies have had an impact outside but most importantly inside companies [@lasi2014industry; @brettel2014virtualization; @russmann2015industry]. Thanks to Industry 4.0, it is possible to create a low-budget digital copy of the company; in other terms, it is economically sustainable to extract information from any business process [@davies2015industry].  Even though it seems a great advantage for companies, it leads to an information overload as an internal issue which should be handled as an external one.

Inside this rising pressure, the general goal of the present thesis is to understand how this information can be turned in to technical knowledge by technical experts (e.g. engineers, researchers, managers, marketeers). In order to do that, this work investigates:

- where valuable business information is hidden in certain documents
- which technical knowledge can be extracted from different sources of information
- what methods and software are available in order to reach this goal
- how these tools can be redesigned (if necessary) for being effective in non-standard contexts


```{r  biginfo, echo=FALSE, fig.align='center', fig.asp= 1.00, fig.cap='Hisogram of the eximated Zettabites of information produced from 2005 to 2020.', message=FALSE, warning=FALSE, out.width='60%'}

 

tabletmp <- readxl::read_xlsx("_bookdown_files/tables/data_growth.xlsx") 

 

ggplot2::ggplot(tabletmp,ggplot2::aes(x= Year, y=`Information [ZB]`)) +

  ggplot2::geom_bar(stat="identity", color= "black", fill="grey") +

  ggplot2::theme_bw()

 

```


# Scope of the Thesis

## Sources of Information

Documents written in natural language contains knowledge by design. Their goal in fact is the explication of implicit knowledge [@masters1992knowledge, @graesser1985structures]. With the growth of information of the last years also this kind of documents has grown in number and there exist a huge opportunity for business to manage this knowledge. The problem to address in order to take advantage of this opportunity, is understanding which sources of information are relevant to a certain company [@larose2014discovering;  @chemchem2015data; @kasemsap2015role]. It is considered relevant all the information that helps the company building knowledge as a tool to achieve their goals (both vision and mission). Finding these sources requires a deep understanding of digital technologies and of business acumen , skills that not every company can have [@hecklau2016holistic;  @davenport2012data; @provost2013data; @van2014data]. To understand the practical implications of this issue, it has been estimated that just a small portion of the digital universe has been explored with the purpose of extracting competitive gain [@data2012bigger]. The estimated percentage of untapped data is 25% and it is going to rise to 33% within 2020: it is likely that many of the potential sources of valuable knowledge are not taken in to consideration by companies and that a huge value is still hidden. This untapped value can be potentially found everywhere in the info-verse: users behaviors in social media, correlations among scientific studies, overlaps between medical and sociological studies, beneath complex analysis in legal documents and so on. 

For these reasons an analytically process gains great value thanks to a correct decision of which documents mine to extract knowledge. To maximize this value the problem space has to be searched in term of business relevant knowledge content of documents. Since this problem space is huge (all the documents of which a company is stakeholder) in the present thesis this decision has been taken a-priori ensuring a wide set of document types in terms of their characteristics. These documents are:

1. _Patents_
2. _Scientific Papers_
3. _Wikipedia_
4. _Social Media_

Patents has their core in technical information. Despite this, some researchers [@bonino2010review] affirm that there is an increasing variety of readers of patents: not only technician and researchers but also marketers and designers who have grown an interest in patent analysis. To our knowledge this thesis represent a first step the direction of these potential readers. Papers are similar to patents in terms of technical content but are not affected by the bias of being legal documents. If this makes technical information easier to find, it also brings a problem when there is the need of synthesizing the huge information coming from a corpus of papers. Wikipedia is distant from patents and papers. It (by design) contains encyclopedic and less objective information, which comes in a semi-structured form. Due to the bottom-up approach in which Wikipedia is developed, it has some reliability shortages to take in to account. Finally social media documents are far from the previous ones since they make use of a short, subjective and colloquial lexicon. It is another of the challenges addressed by this thesis to demonstrate that this sources contains exploitable technical information too. 

## Methods and Focus of the Analysis

Thanks to the improvement in the Artificial Intelligence (AI) field, _Natural Language Processing_ (NLP) has dramatically increased its effectiveness in the last years [@russell2016artificial]. Now powerful tools for the analysis of documents are available to every one that has a little understating of programming [@Taylor2017Tidy, @Kenneth2018spacyr, @Wijffels2018udpipe]. For this reason, the processes carried out in this thesis uses state of the art NLP techniques that only in few cases has been deeply modified in the design process. In other words the value of the output is not in the modified NLP algorithm but on the whole process of knowledge extraction that goes from the decision of which documents to analyze to the communication of the results. 

Furthermore, despite the golden ages of AI and NLP we are living in, some problems this research area is trying to address are still having a huge impact on companies [@hand2007principles; @mining2006data]. 

The first one is that the information overload is not manageable by machines alone and thus there is the need of understating which are the best practices in experts-machine interaction. In this context, it is interesting to notice that the way companies incorporate knowledge in their own artificial systems is drastically changed in the last 30 years. In fact, in the 1990s the main approach was knowledge engineering: machine learned how to perform analysis tasks thanks to the rules experts gave them. This idea has failed. In some contexts, it is impossible representing knowledge with explicit expert-driven rules. For this reason on the 2010s inductive revolution developed; machines are used to extract knowledge from huge amounts of data in an inductive way. 

This has led to a second major problem observed by literature: black boxes or systems whose working mechanism are unknown, except input and output. A recent paper [@pedreschi2018open] tries to clarify the issue with black boxes and offers a few solutions. The authors point at a way similar to reversing, in which a program is stimulated with various inputs to study the outputs and understand its logic. In the same way, to study the mechanisms of a machine-learning model, it is possible to modify slightly the input only, and observe the result; or, foresee the right input for the needed output. It is still cutting edge, but it is already discussed and it is opening up to specific research, such as explainable AI. 

Considering these problems and considering the general goal of the present thesis (i.e. to extract and synthesize technical knowledge from unstructured documents) we can state that we can not take aside the design of processes, which can provide correct knowledge exchange between human and machine, leading to:

- incorporate knowledge of the experts inside AI systems

- experts’ ability to use in their process of decision making, inductively generated knowledge of machines

# Approach

## Design Science Paradigm {#introdesres}

Knowledge extraction systems are implemented within an organization for the purpose of improving the probability for that organisation to reach its goals. A wide literature exists on the design of these kind of systems and the main focus comes from Information Systems (IS) discipline. It is incumbent upon researchers in the IS to further knowledge that aids in the productive application of information technology to human organizations and their management [@edit2002info] and to develop and communicate knowledge concerning both the management of information technology and the use of information technology for managerial and organizational purposes [@Zmud1997edit]. Hevner et al. [@bichler2006design] argues that acquiring such knowledge involves two complementary but distinct paradigms: _behavioral science_ and _design science_ [@march1995design]. 

The _behavioral-science paradigm_ has its roots in the research method of natural science [@makadok2018practical; @colquitt2007trends]. Its goal is to develop and justify theories that explain (or predict) organizational phenomena [@zahra2009maximizing] about the analysis, design, implementation, management, and use of information systems. 
Such theories inform researchers and companies of the interactions among humans, machines, and organizations that must be managed if an information system has to be effective. These theories interact with design decisions made with respect to the system development methodology used [@fini2018collaborative].

The _design-science paradigm_ is a problem solving paradigm and has its roots in engineering and the sciences of the artificial [@simon1996sciences]. Its goal is to create innovations: define the ideas, practices, technical capabilities, and products through which the analysis, design, implementation, management, and use of information systems can be effectively and efficiently accomplished [@denning1997new; @tsichritzis1997dynamics]. In this context the researchers switches his goal: the artifacts that are created in the research process are not exempt from natural laws or behavioral theories but relies on existing kernel theories that are applied, tested, modified, and extended through the experience, creativity, intuition, and problem solving capabilities of the researcher [@george2016big;@madhavji2015big; @markus2002design; @walls1992building].

Since Design science creates and evaluates IT artifacts intended to solve identified organizational problems, the present thesis has as main approach this second paradigm. In Design Science, artifacts are represented in a structured form that may vary from software, formal logic, and rigorous mathematics to informal natural language descriptions. We will use all these representations at different level, for the different applications described. 

Furthermore the thesis has been developed following the guidelines of design-science research. Hevner et al. [@bichler2006design] provide a set of seven guidelines which help information systems researchers conduct, evaluate and present design-science research. The guidelines are summarised in table \@ref(tab:introdesignscience).

As previously discussed, design science is a problem solving paradigm. The fundamental principle of design-science research is that is the building and application of an artifact that permits to the researcher to gain new knowledge and understanding of a design problem and its solution. From this fundamental principle the seven guidelines are derived. That is, design-science research must end and must have as goal the _creation of an innovative, purposeful artifact (Guideline 1)_ for a specified _problem domain (Guideline 2)_. Because the artifact has to be purposeful, researcher has to measure its utility for the specified problem: this imply that _evaluation of the artifact is crucial (Guideline 3)_. Novelty is similarly crucial since the artifact must _solve an unsolved problem or solving a known problem in a more effective or efficient manner (Guideline 4)_. In this way, design-science research is differentiated from the practice of design since artifact itself must be _rigorously defined, formally represented, coherent, and internally consistent (Guideline 5)_. The process of design of the artificial and also artifact itself, incorporates a search process whereby _a problem space is constructed and a mechanism posed or enacted to find an effective (not necessarely otpimal) solution (Guideline 6)_. Finally, the results of the _design-science research must be communicated effectively (Guideline 7)_.

The communication phase is crucial, and the results of a design science based research has to be comprehensible and usable to:

- a technical audience (researchers who will extend them and practitioners who will implement them) 
- a managerial audience (researchers who will study them in context and practitioners who will decide if they should be implemented within their organizations).


```{r introdesignscience, echo=FALSE, fig.align='center', fig.asp=.75, message=FALSE, warning=FALSE, out.width='80%'}

library(magrittr)

knitr::kable(
  readxl::read_xlsx("_bookdown_files/tables/introdesignscience.xlsx"), "latex", booktabs = T,caption = "Design-Science Research Guidelines" ) %>%      kableExtra::kable_styling(full_width = F) %>% 
  kableExtra::column_spec(1) %>% 
  kableExtra::column_spec(2, italic=T, width = "20em")
  

```

## Data Science Workflow

Many frameworks and models exists that describes the process of knowledge extraction from data [@kanehisa2013data, @bellinger2004data, @ackoff1989data, @liew2007understanding]. All this models brings contribution to the literature but with a focus on the _behavioral-science paradigm_, thus with the goal to develop theories about the analysis, design, implementation, management, and use of data analysis systems. 

For the present thesis we adopted a more actionable framework popular in the field of data science: the Data Science Workflow [@wickham2016r]. We adopted this model because is a generic workflow that can be used as guideline to design any knowledge extraction system. The workflow is represented in figure \@ref(fig:mainworkflow).

The workflow is a model of the tools needed in a typical data science project. Since data science is a huge field, and there’s no way that a model can master it, the goal of the model is to give a solid foundation in the most important tools. These tools are used in every data science project, but for most projects they are not enough. The tools represented in the model has been chosen following a rough Pareto 80-20 rule [@pareto1971manual].

The workflow is divided in the subsequent tasks:

- _Import:_ take data stored in a file, database, or web API, and load it into a computer. 

- _Tidy:_ Tidying data means storing it in a consistent form that matches the semantics of the data-set with the way it is stored. In brief, when the data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets  focus on questions about the data and not in its shape. 

- _Transform:_ Transformation includes narrowing in on observations of interest, creating new variables that are functions of existing variables, and calculating a set of summary statistics.

- _Visualize:_ Visualisation is a fundamentally human activity. A good visualisation will show unexpected information, or raise new questions about the data. A good visualisation might also hint that the hypothesis is wrong, or there is the need to collect different data. Visualisations can surprise, but do not scale particularly well because they require a human to interpret them.

- _Model:_ Model is complementary to visualize. Once the hypothesis are sufficiently precise, a model can answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains. But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise.

- _Communicate_: The last step is communication, an absolutely critical part of any project. As stated before communication phase is crucial, and the results of a design science based research has to be comprehensible and usable to a technical audience a managerial audience.

- _Program:_ To design knowledge extraction tool programming is nowadays essential. There is no need to be an expert programmer to design effective knowledge extraction tool, but learning more about programming allows  to automate common tasks, and solve new problems with greater ease.

- _Understand:_ The greatest feature of a knowledge extraction system is not to detect the large trends- they are visible anyway; it is, rather,  to detect weak signals, or information that initially appears with low frequency [@apreda2016functional]. These signals escape from traditional statistical detection techniques, exactly because it is difficult to distinguish them from pure statistical noise. It is hard to distinguish weak-signals from noise only using algorithms: understanding the problem to be solved and having domain expertise is essential. For this reason _understand_ is not only part of the process of analysis, but is maybe the most important task with respect to the present thesis. 

A full state of the art on all of these tasks as been conducted in section \@ref(sotatools).

```{r mainworkflow, echo=FALSE, fig.align='center', fig.asp=.75, fig.cap='A general workflow for the process of data analysis. Readapted from Wickham (2016)', message=FALSE, warning=FALSE, out.width='80%'}

knitr::include_graphics("_bookdown_files/figures/main_work_flow.png")

```

# Structure and Rationale {-}

The selected documents to analyze, the exploitation of Natural Language Processing Techniques , the Design Science Approach and the Data Science Workflow helps to explain the structure of this thesis, since the decision of using these paradigm and tools had a direct impact on its form and content. The thesis is in fact structured as follow: 

- _Part Number 2 "State of the Art"_ makes a review of tools and techniques for Natural Language Processing and shows the documents used for knowledge extraction. In particular, the analysis of technical documents require the design of processes that rely both on Natural Language Processing techniques and on technical field experts. While the first techniques are codified and explicit, the second are sometimes implicit and always hard to systematize. In the State of the Art these two groups of techniques are treated in the same way to give to the reader a systematic literature review on these topics. Furthermore it is important to define the value in terms of new knowledge that documents can bring to companies. The goal of this state of the art is to demonstrate that a large literature on Techniques able to solve the problems described in section \@ref(introproblems) exists, but weak or no effort exist in the literature in the systemic integration of these tools and in the design of holistic methods to solve these problems.

- _Part Number 3 "Methods and Results"_ is the core Part of the thesis. This part describes the methods and processes designed for the analysis of technical documents following the guidelines of design research described in section \@ref(introdesres). Since design research is a problem solving paradigm, each chapter (focalised on a different kind of document) starts with a brief description of the field of application of the method and with the framing of the problem to be solved.  Then the methodology to solve the problem is described with the goal of being clear to a technical audience and a managerial audience. Each chapter closes with the results description that proves the utility, quality, and efficacy of the design artifact.

- _Part Number 4 "Applications of the Results"_  is a collection of projects that describes the applications and results of the methods designed in _Part 3 "Methods and Results"_. The goal, again following the  the guidelines of design research [@bichler2006design], is to provide clear and verifiable contributions in the areas of the design artifact, design foundations, and/or design methodologies.

- _Part Number 5 "Conclusions"_ summarises the thesis, and its possible future developments. The thesis ends giving a more descriptive-science view of the results. In this last part we focus on a general method that has proven to be effective in the major part of the designs of the present thesis: __lexicons design__.


