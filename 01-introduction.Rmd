---
output:
  pdf_document: default
  word_document: default
  html_document: default
---
# (PART) Introduction {-}

# Context {-}

The information field human beings are living in has changed dramatically over the last years, effecting economy, technology, culture and society. And yet, its stronger mark has been over business systems [@Arun2006Firm; @jin2015significance; @degryse2016digitalisation; @john2014big; @o2016weapons] that to strive and stay competitive, have to master methods and tools to solve the information overload issue [@levitin2014organized; @feng2015competing].

In order to understand what companies go through in terms of challenges, we have to wear their shoes. Let’s say that the amount of information (such as tactile, visual and sounds, rumors and conversations, the content of business papers) gathered in the location we live in starts to rise quickly. To survive such an environment, we would need new and more powerful senses; we would probably need outside systems to help us processing information. Considering the mole of digital information produced in the past 10 years, companies have found themselves in our hypothetical scenario: their digital universe is chaotic and constantly expanding. In fact, this very universe will grow from 2005 to 2020, from 130 exabytes to 40,000 exabytes (40 zettabytes) [@gantz2012digital], as shown in figure \@ref(fig:biginfo).

Moreover, new digital technologies have had an impact outside but most importantly inside companies [@lasi2014industry; @brettel2014virtualization; @russmann2015industry]. Thanks to Industry 4.0, it is possible to create a low-budget digital copy of the company; in other terms, it is economically sustainable to extract information from any business process [@davies2015industry].  Even though it seems a great advantage for companies, it leads to an information overload as an internal issue which should be handled as an external one.

```{r  biginfo, echo=FALSE, fig.align='center', fig.asp=.75, fig.cap='Hisogram of the eximated Zettabites of information produced from 2005 to 2020.', message=FALSE, warning=FALSE, out.width='60%'}

 

tabletmp <- readxl::read_xlsx("_bookdown_files/tables/data_growth.xlsx") 

 

ggplot2::ggplot(tabletmp,ggplot2::aes(x= Year, y=`Information [ZB]`)) +

  ggplot2::geom_bar(stat="identity", color= "black", fill="grey") +

  ggplot2::theme_bw()

 

```

# Problems and Goals {-}

Inside this rising information pressure, the general goal of the present Thesis is to extract and synthesise knowledge from these information.

The first problem is to understand which pieces of information are relevant to a certain company [@larose2014discovering;  @chemchem2015data; @kasemsap2015role]. It is considered relevant all the information that helps the company building knowledge as a tool to achieve their goals (both vision and mission). This kind of analysis requires a deep understanding of digital technologies and of business acumen: not all companies have those skills [@hecklau2016holistic;  @davenport2012data; @provost2013data; @van2014data], considering that business acumen is strongly influenced by the field of work. It has been estimated that just a small portion of the digital universe has been explored with the purpose of extracting competitive gain [@data2012bigger]. The estimated percentage of untapped data is 25% and it is going to rise to 33% within 2020. This untapped value can be found in hidden pattern in social media, correlations among scientific studies, overlaps between medical and sociological studies, beneath complex analysis in legal documents and so on.

The difficulty of gathering valuable information is related to high-impact information being no longer sectorial (belonging to one knowledge field); instead, this information comes from multi-disciplinary fields, making it even harder to spot. Although those resources are accessible to experts from a certain field they may have troubles comprehending a mixed jargon of overlapping areas far from their own. To show the phenomena, here is a recent update from CPC (Cooperative Patent Classification) in which it has been added the class B33 regarding additive manufacturing technologies [@cpc2014], a technological field with a vast multidisciplinary content (material sciences, mechanic, computer, robotic engineering) [@mueller2012additive]. A new CPC means a great impact on patent office, whose job is to form new examiners for that specific CPC: it means that the previous examiners did not have the needed set of skills which are not acquirable inside a domain in a short period of time. So, if new hybrid technological fields have such a relevant impact on European Patent Office (which has special access to both knowledge and human resources needed to run a certain field) we can imagine how this impact is determinant onto a manufacturing company.

Secondly, knowing the potential sources of information that can bring value, it is essential to understand which tools can be used to extract this value in the form of knowledge [@hand2007principles; @mining2006data]. As previously shown, a company is at risk if the information grows in quantity and directions impossible to process. In other terms, the rise of information shown in picture \@ref(fig:biginfo) does not imply the rise of knowledge; in fact, it is possible that it can prevent it from growing [@allen2003information; @herbig1994effect]. That phenomena is known as the productivity paradox.

Nowadays, in the big data era, companies (and universities too) need new _senses_ and _helpers_ who can assist them in the processes of extraction and grounding of knowledge, considering the growing amount of resources and information available. For these reason the emerging field of Data Science had a huge success in the last decade with a great focus on the study of artificial intellingece and machine learning. 

It is clear that data science is not manageable by machines alone. A system capable of extracting knowledge from large amounts of information, which companies have to deal with, needs both humans and machines.

Human-computer interaction, HCI, also called human-machine interaction (wide spectrum connotation, mostly used in the literature in Informatics), is the study of the interaction between people and computers to design and develop interactive systems to be usable, trustworthy and able to support and facilitate human activity.

The in-depth study of the interaction covers informatics, psychology, cognitive sciences, ergonomics, design, information science, artificial intelligence, natural language and others.

The main goal of this discipline is the usability. The usability, as stated in the norm ISO 9241, is the measure in which a product can be used by specific users, to achieve specific goals with efficacy, efficiency and satisfaction in a specific context of use. The reasons why usability is important are:  

- increase efficiency for users (more productivity)

- decrease in errors (increase in security)

- decrease the need for training (lower costs)

- decrease the need of support for users

- increase sales

 There must exist an interaction: in such a strategic environment with a great amount of domain knowledge, it is impossible to substitute experts with artificial intelligence.

It is interesting to notice that the way companies incorporate knowledge in their own systems is drastically changed. In fact, in the 1990s the main approach was knowledge engineering: machine learned how to perform analysis tasks thanks to the rules experts gave them. This idea has failed, too. In some contexts, it is impossible representing knowledge with rules.

For that reason, lately, inductive revolution developed; machines are used to extract the rules that code knowledge in the form of huge amounts of data representing reality leading to a new problem observed by literature: black boxes. This expression is commonly used to indicate those cases when algorithms and computational methods, whose working mechanism are unknown, except input and output, are needed. 

A recent paper [@pedreschi2018open] tries to clarify the issue with black box and offers a few solutions. The authors point at a way similar to reversing, in which a program is stimulated with various inputs to study the outputs and understand its logic. In the same way, to study the mechanisms of a machine-learning model, it is possible to modify slightly the input only, and observe the result; or, foresee the right input for the needed output. It is still cutting edge, but it is already discussed and it is opening up to specific research, such as explainable AI.

To conclude, considereing the general goal of the present thesis (e.g. to extract and synthesise knowledge from information) we can state that the path to reach this objective pass trough the design of analysis algorithms, which can provide correct knowledge exchange between human and machine, leading to:

- incorporate knowledge of the experts inside machine-learning systems

- experts’ ability to use in their process of decision making, inductively generated knowledge of machines

# Solution {-}

Many frameworks and models exists that describes the process of knowledge extraction from data [@kanehisa2013data, @bellinger2004data, @ackoff1989data, @liew2007understanding]. All this models brings contribution to the literature but with a focus on the _behavioral-science paradigm_, thus with the goal to develop theories about the analysis, design, implementation, management, and use of data analysis systems. 

For the present thesis we adopted a more actionable framework popular in the field of data science: the Data Science Workflow described by Wickham [@wickham2016r]. We adopted this model because is a generic workflow that can be used as guideline to design any knowledge extraction system. The workflow is represented in figure \@ref(fig:mainworkflow).

The workflow is a model of the tools needed in a typical data science project. Since data science is a huge field, and there’s no way that a model can master it, the goal of the model is to give a solid foundation in the most important tools.These tools are used in every data science project, but for most projects they are not enough. The tools represented in the model has been chosen following a rough Pareto 80-20 rule [@pareto1971manual].

The workflow is divided in the sequent tasks:

- _Import:_ take data stored in a file, database, or web API, and load it into a computer. 

- _Tidy:_ Tidying data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when the data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets  focus on questions about the data and not in its shape. 

- _Transform:_ Transformation includes narrowing in on observations of interest, creating new variables that are functions of existing variables, and calculating a set of summary statistics.

- _Visualize:_ Visualisation is a fundamentally human activity. A good visualisation will show unexpected information, or raise new questions about the data. A good visualisation might also hint that the hypothesis is wrong, or there is the need to collect different data. Visualisations can surprise, but do not scale particularly well because they require a human to interpret them.

- _Model:_ Model is complementary to visualize. Once the hypothesis are sufficiently precise, a model can answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains. But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise.

- _Communicate_: The last step is communication, an absolutely critical part of any project. As stated before communication phase is crucial, and the results of a design science based research has to be comprehensible and usable to a technical audience a managerial audience.

- _Program:_ To design knowledge extraction tool programming is nowadays essential. There is no need to be an expert programmer to design effective knowledge extraction tool, but learning more about programming allows  to automate common tasks, and solve new problems with greater ease.

- _Understand:_ The greatest feature of a knowledge extraction system is not to detect the large trends- they are visible anyway; it is, rather,  to detect weak signals, or information that initially appears with low frequency [@apreda2016functional]. These signals escape from traditional statistical detection techniques, exactly because it is difficult to distinguish them from pure statistical noise. It is hard to distinguish weak-signals from noise only using algorithms: understanding the problem to be solved and having domain expertise is essential. For this reason _understand_ is not only part of the process of analysis, but is maybe the most important task with respect to the present thesis. 

A full state of the art on all of these tasks as been conducted in section \@ref(sotatools).


```{r mainworkflow, echo=FALSE, fig.align='center', fig.asp=.75, fig.cap='A general workflow for the process of data analysis. Readapted from Wickham (2016)', message=FALSE, warning=FALSE, out.width='80%'}

knitr::include_graphics("_bookdown_files/figures/main_work_flow.png")

```

As stated before, the focus of the present thesis is on the interaction between men and machines in the _extraction of knowledge from technical documents_. This focus has two main reasons:

1. Documents written in natural language contains knowledge by design. Their goal in fact is the explication of implicit knowledge [@masters1992knowledge, @graesser1985structures]. With the growth of information of the last years also this kind of documents has grown in number and there exist a huge opportunity for business to manage this knowledge.

2. Thank to the improvement in the Artificial Intelligence field, Natural Language Processing has dramatically increased its effectiveness in the last years [@russell2016artificial]. Now powerful tools for the analysis of documents are available to every one that has a little understating of programming [@Taylor2017Tidy, @Kenneth2018spacyr, @Wijffels2018udpipe]. 

The design processes carried out in this thesis uses state of the art Natural Language Processing techniques or algorithms that only in few cases has been deeply modified in the design process. In other words the value of the output is not in the modified natural language algorithm but on the whole process of knowledge extraction that goes from the decision of which documents to analyze to the communication of the results. In fact, great value is added to the design process by the decision of which documents mine to extract knowledge. To maximize this value the problem space has to be searched in term of _business relevant knowledge content of documents_. Since this problem space is huge (all the documents of which a company is stakeholder) the decision can be considered as a feature of the method, as valuable as (or even more in this case) the algorithm of natural language processing. 

To decide the documents on which the thesis has the focus, section \@ref(sotadocuments) contains a review of the characteristics and on the state of the art of the analysis on four different document types: patents, papers, Wikipedia and social media. 

# Stakeholders of this Thesis {-}

# Structure and Rationale {-}

The Design Science Approach, the Data Science Workflow and the exploitation of Natural Language Processing Techniques helps to explain the structure of this thesis, since the decision of using these paradigm and tools had a direct impact on the form and content of it. 

- _Part Number 2 "State of the Art"_ makes a review of tools and techniques for Natural Language Processing and shows the documents used for knowledge extraction. In particular, the analysis of technical documents require the design of processes that rely both on Natural Language Processing techniques and on technical field experts. While the first techniques are codified and explicit, the second are sometimes implicit and always hard to systematize. In the State of the Art these two groups of techniques are treated in the same way to give to the reader a systematic literature review on these topics. Furthermore it is important to define the value in terms of new knowledge that documents can bring to companies. The goal of this state of the art is to demonstrate that a large literature on Techniques able to solve the problems described in section \@ref(introproblems) exists, but weak or no effort exist in the literature in the systemic integration of these tools and in the design of holistic methods to solve these problems.

- _Part Number 3 "Methods and Results"_ is the core Part of the thesis. This part describes the methods and processes designed for the analysis of technical documents following the guidelines of design research described in section \@ref(introdesres). Since design research is a problem solving paradigm, each chapter (focalised on a different kind of document) starts with a brief description of the field of application of the method and with the framing of the problem to be solved.  Then the methodology to solve the problem is described with the goal of being clear to a technical audience and a managerial audience. Each chapter closes with the results description that proves the utility, quality, and efficacy of the design artifact.

- _Part Number 4 "Applications of the Results"_  is a collection of projects that describes the applications and results of the methods designed in _Part 3 "Methods and Results"_. The goal, again following the  the guidelines of design research [@bichler2006design], is to provide clear and verifiable contributions in the areas of the design artifact, design foundations, and/or design methodologies.

- _Part Number 5 "Conclusions"_ summarises the thesis, and its possible future developments. The thesis ends giving a more descriptive-science view of the results. In this last part we focus on a general method that has proven to be effective in the major part of the designs of the present thesis: __lexicons design__.


